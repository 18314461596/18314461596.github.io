<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="初级以内我无敌，中级以上我一换一">
<meta property="og:type" content="website">
<meta property="og:title" content="第五门徒">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="第五门徒">
<meta property="og:description" content="初级以内我无敌，中级以上我一换一">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张宴银">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>第五门徒</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="第五门徒" type="application/rss+xml">
</head>




<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">第五门徒</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="heartbeat fa-fw"></i>公益 404</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/28/spark-core-rdd-practice/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/28/spark-core-rdd-practice/" class="post-title-link" itemprop="url">Spark RDD算子实战</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-08-28 12:49:44 / 修改时间：13:05:51" itemprop="dateCreated datePublished" datetime="2023-08-28T12:49:44+08:00">2023-08-28</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="人有逆天之时，天无绝人之路"><a href="#人有逆天之时，天无绝人之路" class="headerlink" title="人有逆天之时，天无绝人之路"></a>人有逆天之时，天无绝人之路</h1><h1 id="创建运行环境"><a href="#创建运行环境" class="headerlink" title="创建运行环境"></a>创建运行环境</h1><pre><code>object Spark_Core_Practice &#123;
    def main(args: Array[String]): Unit = &#123;
        val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;Operator&quot;)
         val sc = new SparkContext(sparkConf)  

        //RDD算子代码块

        sc.close()

    &#125;
&#125;  
</code></pre>
<h2 id="1-创建一个1-10的数组RDD，将所有元素-2形成新的RDD"><a href="#1-创建一个1-10的数组RDD，将所有元素-2形成新的RDD" class="headerlink" title="1.创建一个1-10的数组RDD，将所有元素*2形成新的RDD"></a>1.创建一个1-10的数组RDD，将所有元素*2形成新的RDD</h2><pre><code>val inputRDD = sc.parallelize(1 to 10)
val newRDD = inputRDD.map(_ * 2)
newRDD.foreach(println(_))  
</code></pre>
<h2 id="2-创建一个10-20数组的RDD-使用mapPartitions将所有元素-2形成新的RDD"><a href="#2-创建一个10-20数组的RDD-使用mapPartitions将所有元素-2形成新的RDD" class="headerlink" title="2.创建一个10-20数组的RDD,使用mapPartitions将所有元素*2形成新的RDD"></a>2.创建一个10-20数组的RDD,使用mapPartitions将所有元素*2形成新的RDD</h2><p>注意，mapPartitions和map的区别在于，mapPartitions是一批数据一次性处理，所以是迭代器操作，返回一个迭代器  </p>
<pre><code> val inputRdd = sc.parallelize(10 to 20)
 //makeRDD底层调用的就是parallelize
    val inputRDD = sc.makeRDD(10 to 20)
    val newRDD = inputRDD.mapPartitions(iter =&gt; &#123;
      iter.map(_ * 2)
    &#125;)
    newRDD.foreach(println(_))  
</code></pre>
<h2 id="3-创建一个元素为1-5的RDD，运用flatMap创建一个新的RDD，新的RDD为原RDD每个元素的平方和三次方来组成"><a href="#3-创建一个元素为1-5的RDD，运用flatMap创建一个新的RDD，新的RDD为原RDD每个元素的平方和三次方来组成" class="headerlink" title="3.创建一个元素为1-5的RDD，运用flatMap创建一个新的RDD，新的RDD为原RDD每个元素的平方和三次方来组成"></a>3.创建一个元素为1-5的RDD，运用flatMap创建一个新的RDD，新的RDD为原RDD每个元素的平方和三次方来组成</h2><pre><code>val inputRDD = sc.makeRDD(1 to 5)
val newRDD = inputRDD.flatMap(n =&gt; &#123;
      List(Math.pow(n, 2).toInt, Math.pow(n, 3).toInt)
    &#125;)
newRDD.foreach(println(_))  
</code></pre>
<h2 id="4-创建一个-4-个分区的-RDD数据为Array-10-20-30-40-50-60-，使用glom将每个分区的数据放到一个数组"><a href="#4-创建一个-4-个分区的-RDD数据为Array-10-20-30-40-50-60-，使用glom将每个分区的数据放到一个数组" class="headerlink" title="4.创建一个 4 个分区的 RDD数据为Array(10,20,30,40,50,60)，使用glom将每个分区的数据放到一个数组"></a>4.创建一个 4 个分区的 RDD数据为Array(10,20,30,40,50,60)，使用glom将每个分区的数据放到一个数组</h2><pre><code>val inputRDD:RDD[Int] = sc.makeRDD(Array(10, 20, 30, 40, 50, 60), 4)
//glom之后的结果集是array类型，不能直接打印。但是可以将array数组的数据通过mkString（&quot;&quot;）的方式拼接为一个字符串
//字符串是可以foreach单独打印出来的
    val newRDD:RDD[Array[Int]] = inputRDD.glom()
    newRDD.foreach(
      x =&gt; println(x.mkString(&quot; &quot;))
    )  
</code></pre>
<h2 id="5-创建一个-RDD数据为Array-1-3-4-20-4-5-8-，按照元素的奇偶性进行分组"><a href="#5-创建一个-RDD数据为Array-1-3-4-20-4-5-8-，按照元素的奇偶性进行分组" class="headerlink" title="5. 创建一个 RDD数据为Array(1, 3, 4, 20, 4, 5, 8)，按照元素的奇偶性进行分组"></a>5. 创建一个 RDD数据为Array(1, 3, 4, 20, 4, 5, 8)，按照元素的奇偶性进行分组</h2><pre><code>val inputRDD:RDD[Int] = sc.makeRDD(Array(1, 3, 4, 20, 4, 5, 8))
val newRDD:RDD[(Boolean,Iterable[Int])] = inputRDD.groupBy(num =&gt; num % 2 == 0)
newRDD.foreach(num =&gt; println(num))  
</code></pre>
<h2 id="6-创建一个-RDD（由字符串组成）Array-“xiaoli”-“laoli”-“laowang”-“xiaocang”-“xiaojing”-“xiaokong”-过滤出一个新-RDD（包含“xiao”子串）"><a href="#6-创建一个-RDD（由字符串组成）Array-“xiaoli”-“laoli”-“laowang”-“xiaocang”-“xiaojing”-“xiaokong”-过滤出一个新-RDD（包含“xiao”子串）" class="headerlink" title="6.创建一个 RDD（由字符串组成）Array(“xiaoli”, “laoli”, “laowang”, “xiaocang”, “xiaojing”, “xiaokong”) 过滤出一个新 RDD（包含“xiao”子串）"></a>6.创建一个 RDD（由字符串组成）Array(“xiaoli”, “laoli”, “laowang”, “xiaocang”, “xiaojing”, “xiaokong”) 过滤出一个新 RDD（包含“xiao”子串）</h2><pre><code>val inputRDD:RDD[String] = sc.makeRDD(Array(&quot;xiaoli&quot;, &quot;laoli&quot;, &quot;laowang&quot;, &quot;xiaocang&quot;, &quot;xiaojing&quot;, &quot;xiaokong&quot;))
val newRDD:RDD[String] = inputRDD.filter(x =&gt; x.contains(&quot;xiao&quot;))
newRDD.foreach(x =&gt; &#123;
      println(x)
    &#125;)  
</code></pre>
<h2 id="7、创建一个-RDD数据为1-to-10，请使用sample不放回抽样"><a href="#7、创建一个-RDD数据为1-to-10，请使用sample不放回抽样" class="headerlink" title="7、创建一个 RDD数据为1 to 10，请使用sample不放回抽样"></a>7、创建一个 RDD数据为1 to 10，请使用sample不放回抽样</h2><pre><code>val inputRDD:RDD[Int] = sc.makeRDD(1 to 10)
val newRDD:RDD[Int] = inputRDD.sample(false, 0.5, 1)
newRDD.foreach(x =&gt; println(x))  
</code></pre>
<h2 id="8、创建一个-RDD数据为1-to-10，请使用sample放回抽样"><a href="#8、创建一个-RDD数据为1-to-10，请使用sample放回抽样" class="headerlink" title="8、创建一个 RDD数据为1 to 10，请使用sample放回抽样"></a>8、创建一个 RDD数据为1 to 10，请使用sample放回抽样</h2><pre><code>val inputRDD:RDD[Int] = sc.makeRDD(1 to 10)
val newRDD:RDD[Int] = inputRDD.sample(true, 0.5, 1)
newRDD.foreach(x =&gt; println(x))    
</code></pre>
<h2 id="9、创建一个-RDD数据为Array-10-10-2-5-3-5-3-6-9-1-对-RDD-中元素执行去重操作"><a href="#9、创建一个-RDD数据为Array-10-10-2-5-3-5-3-6-9-1-对-RDD-中元素执行去重操作" class="headerlink" title="9、创建一个 RDD数据为Array(10,10,2,5,3,5,3,6,9,1),对 RDD 中元素执行去重操作"></a>9、创建一个 RDD数据为Array(10,10,2,5,3,5,3,6,9,1),对 RDD 中元素执行去重操作</h2><pre><code>val inputRDD:RDD[Int] = sc.makeRDD(Array(10, 10, 2, 5, 3, 5, 3, 6, 9, 1))
val newRDD:RDD[Int] = inputRDD.distinct()
newRDD.foreach(x =&gt; println(x))  
</code></pre>
<h2 id="10、创建一个分区数为5的-RDD，数据为0-to-100，之后使用coalesce再重新减少分区的数量至-2"><a href="#10、创建一个分区数为5的-RDD，数据为0-to-100，之后使用coalesce再重新减少分区的数量至-2" class="headerlink" title="10、创建一个分区数为5的 RDD，数据为0 to 100，之后使用coalesce再重新减少分区的数量至 2"></a>10、创建一个分区数为5的 RDD，数据为0 to 100，之后使用coalesce再重新减少分区的数量至 2</h2><pre><code>val inputRDD:RDD[Int] = sc.makeRDD(0 to 100,5)
val newRDD:RDD[Int]= inputRDD.coalesce(2, false)
newRDD.foreach(x =&gt; println(x))  
</code></pre>
<h2 id="11、创建一个分区数为5的-RDD，数据为0-to-100，之后使用repartition再重新减少分区的数量至-3"><a href="#11、创建一个分区数为5的-RDD，数据为0-to-100，之后使用repartition再重新减少分区的数量至-3" class="headerlink" title="11、创建一个分区数为5的 RDD，数据为0 to 100，之后使用repartition再重新减少分区的数量至 3"></a>11、创建一个分区数为5的 RDD，数据为0 to 100，之后使用repartition再重新减少分区的数量至 3</h2><pre><code>val inputRDD:RDD[Int] = sc.makeRDD(0 to 100, 5)
val newRDD:RDD[Int] = inputRDD.repartition(3)
newRDD.foreach(x =&gt; println(x))
</code></pre>
<h2 id="12、创建一个-RDD数据为1-3-4-10-4-6-9-20-30-16-请给RDD进行分别进行升序和降序排列"><a href="#12、创建一个-RDD数据为1-3-4-10-4-6-9-20-30-16-请给RDD进行分别进行升序和降序排列" class="headerlink" title="12、创建一个 RDD数据为1,3,4,10,4,6,9,20,30,16,请给RDD进行分别进行升序和降序排列"></a>12、创建一个 RDD数据为1,3,4,10,4,6,9,20,30,16,请给RDD进行分别进行升序和降序排列</h2><pre><code>val inputRDD = sc.makeRDD(Seq(1,3,4,10,4,6,9,20,30,16),1)
//排序操作算子  sortBy(x =&gt; x, ascending = true / false) true升序，false降序  默认升序true
//val newRDD:RDD[Int] = inputRDD.sortBy(x =&gt; x)   //对RDD数据进行升序排序
val newRDD = inputRDD.sortBy(x =&gt; x, ascending = false)  //对RDD数据进行降序排序
newRDD.foreach(x =&gt; println(x))  
</code></pre>
<h2 id="13、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-6和4-to-10，求交集"><a href="#13、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-6和4-to-10，求交集" class="headerlink" title="13、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 6和4 to 10，求交集"></a>13、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 6和4 to 10，求交集</h2><pre><code>// 求并集算子 intersection
// 求并集时，可以指定分区数
val rdd1:RDD[Int] = sc.makeRDD(1 to 6)
val rdd2:RDD[Int] = sc.makeRDD(4 to 10)
val rdd3:RDD[Int] = rdd1.intersection(rdd2,1)
rdd3.foreach(x =&gt; println(x))  
</code></pre>
<h2 id="14、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-6和4-to-10，计算差集，两个都算"><a href="#14、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-6和4-to-10，计算差集，两个都算" class="headerlink" title="14、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 6和4 to 10，计算差集，两个都算"></a>14、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 6和4 to 10，计算差集，两个都算</h2><pre><code>// 差集的算子 subtract
val rdd1 = sc.makeRDD(1 to 6)
val rdd2 = sc.makeRDD(4 to 10)
val rdd3 = rdd1.subtract(rdd2)
rdd3.foreach(x =&gt; println(x))
rdd1.subtract(rdd2,1).foreach(x =&gt; println(x))
val rdd4 = rdd2.subtract(rdd1)
rdd4.foreach(x =&gt; println(x))
println(&quot;================================================&quot;)
rdd2.subtract(rdd1,1).foreach(x =&gt; println(x))
</code></pre>
<h2 id="15、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-6和4-to-10，计算并集"><a href="#15、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-6和4-to-10，计算并集" class="headerlink" title="15、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 6和4 to 10，计算并集"></a>15、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 6和4 to 10，计算并集</h2><pre><code>// 求并集的算子是union，注意，spark中的union并集，交集部分未去重，可以调用distinct进行去重操作，得到数学上的并集
val rdd1 = sc.makeRDD(1 to 6)
val rdd2 = sc.makeRDD(4 to 10)
val rdd3:RDD[Int] = rdd1.union(rdd2)
rdd3.foreach(x =&gt; println(x))
</code></pre>
<h2 id="16、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-6和4-to-10，计算-2-个-RDD-的笛卡尔积"><a href="#16、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-6和4-to-10，计算-2-个-RDD-的笛卡尔积" class="headerlink" title="16、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 6和4 to 10，计算 2 个 RDD 的笛卡尔积"></a>16、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 6和4 to 10，计算 2 个 RDD 的笛卡尔积</h2><pre><code>// 求笛卡尔积的算子 cartesian
val rdd1 = sc.makeRDD(1 to 6,1)
val rdd2 = sc.makeRDD(4 to 10,1)
rdd1.cartesian(rdd2).foreach(x =&gt; println(x))  
</code></pre>
<h2 id="17、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-5和11-to-15，对两个RDD拉链操作"><a href="#17、创建两个RDD，分别为rdd1和rdd2数据分别为1-to-5和11-to-15，对两个RDD拉链操作" class="headerlink" title="17、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 5和11 to 15，对两个RDD拉链操作"></a>17、创建两个RDD，分别为rdd1和rdd2数据分别为1 to 5和11 to 15，对两个RDD拉链操作</h2><pre><code>// zip制作RDD拉链表时，要注意RDD中的元素个数要一致
val rdd1 = sc.makeRDD(1 to 5)
val rdd2 = sc.makeRDD(11 to 15)
rdd1.zip(rdd2).foreach(x =&gt; println(x))    
</code></pre>
<h2 id="18、创建一个RDD数据为List-“female”-1-“male”-5-“female”-5-“male”-2-，请计算出female和male的总数分别为多少"><a href="#18、创建一个RDD数据为List-“female”-1-“male”-5-“female”-5-“male”-2-，请计算出female和male的总数分别为多少" class="headerlink" title="18、创建一个RDD数据为List((“female”,1),(“male”,5),(“female”,5),(“male”,2))，请计算出female和male的总数分别为多少"></a>18、创建一个RDD数据为List((“female”,1),(“male”,5),(“female”,5),(“male”,2))，请计算出female和male的总数分别为多少</h2><pre><code>//inputRDD.reduceByKey(_+_) 这个key是如何被指定的？默认以（String,Int）中的String作为key吗
val inputRDD:RDD[(String,Int)] = sc.makeRDD(List((&quot;female&quot;, 1), (&quot;male&quot;, 5), (&quot;female&quot;, 5), (&quot;male&quot;, 2)))
inputRDD.reduceByKey(_+_).foreach(x =&gt; println(x))  
</code></pre>
<h2 id="19、创建一个有两个分区的-RDD数据为List-“a”-3-“a”-2-“c”-4-“b”-3-“c”-6-“c”-8-，取出每个分区相同key对应值的最大值，然后相加"><a href="#19、创建一个有两个分区的-RDD数据为List-“a”-3-“a”-2-“c”-4-“b”-3-“c”-6-“c”-8-，取出每个分区相同key对应值的最大值，然后相加" class="headerlink" title="19、创建一个有两个分区的 RDD数据为List((“a”,3),(“a”,2),(“c”,4),(“b”,3),(“c”,6),(“c”,8))，取出每个分区相同key对应值的最大值，然后相加"></a>19、创建一个有两个分区的 RDD数据为List((“a”,3),(“a”,2),(“c”,4),(“b”,3),(“c”,6),(“c”,8))，取出每个分区相同key对应值的最大值，然后相加</h2><pre><code>val inputRDD = sc.makeRDD(List((&quot;a&quot;, 3), (&quot;a&quot;, 2), (&quot;c&quot;, 4), (&quot;b&quot;, 3), (&quot;c&quot;, 6), (&quot;c&quot;, 8)),2)
inputRDD.aggregateByKey(0)(
  //分区内计算规则
  (tmp,item) =&gt; &#123;
    println(tmp,item,&quot;---&quot;)
    Math.max(tmp,item)
  &#125;,
  //分区间计算规则
    (tmp,result) =&gt;&#123;
      println(tmp,result,&quot;--&quot;)
      tmp + result
    &#125;
).foreach(x =&gt; println(x))    
</code></pre>
<h2 id="20、-创建一个有两个分区的-pairRDD数据为Array-“a”-88-“b”-95-“a”-91-“b”-93-“a”-95-“b”-98-，根据-key-计算每种-key-的value的平均值"><a href="#20、-创建一个有两个分区的-pairRDD数据为Array-“a”-88-“b”-95-“a”-91-“b”-93-“a”-95-“b”-98-，根据-key-计算每种-key-的value的平均值" class="headerlink" title="20、 创建一个有两个分区的 pairRDD数据为Array((“a”, 88), (“b”, 95), (“a”, 91), (“b”, 93), (“a”, 95), (“b”, 98))，根据 key 计算每种 key 的value的平均值"></a>20、 创建一个有两个分区的 pairRDD数据为Array((“a”, 88), (“b”, 95), (“a”, 91), (“b”, 93), (“a”, 95), (“b”, 98))，根据 key 计算每种 key 的value的平均值</h2><pre><code>// groupByKey的结果是从 1 开始计数的
val inputRDD = sc.makeRDD(Array((&quot;a&quot;, 88), (&quot;b&quot;, 95), (&quot;a&quot;, 91), (&quot;b&quot;, 93), (&quot;a&quot;, 95), (&quot;b&quot;, 98)))
inputRDD.groupByKey().foreach(x =&gt; println(x))
inputRDD.groupByKey().map(x =&gt; &#123;
  println(x._2.sum)
  println(x._2.size)
  x._2.sum / x._2.size
&#125;).foreach(x =&gt; println(x))  
</code></pre>
<h2 id="21、统计出每一个省份广告被点击次数的-TOP3，数据在access-log文件中"><a href="#21、统计出每一个省份广告被点击次数的-TOP3，数据在access-log文件中" class="headerlink" title="21、统计出每一个省份广告被点击次数的 TOP3，数据在access.log文件中"></a>21、统计出每一个省份广告被点击次数的 TOP3，数据在access.log文件中</h2><pre><code>//reduceByKey的key声明字段咋定义的？
val inputRDD = sc.textFile(&quot;src/data/access.log&quot;)
inputRDD.foreach(x =&gt; println(x))
val RDD1 = inputRDD.map &#123; log =&gt;
  val Array(_, province, _, _, advertising) = log.split(&quot; &quot;)
  (province, advertising)
&#125;
RDD1.foreach(x =&gt; println(x))
val RDD2 = RDD1.reduceByKey( _ + _)
val RDD3 = RDD2.sortBy(tp =&gt; tp._2, ascending = false)
RDD3.take(3).foreach(x =&gt; println(x))    
</code></pre>
<h2 id="26、读写-objectFile-文件，把-RDD-保存为objectFile，RDD数据为Array-“a”-1-“b”-2-“c”-3-，并进行读取出来"><a href="#26、读写-objectFile-文件，把-RDD-保存为objectFile，RDD数据为Array-“a”-1-“b”-2-“c”-3-，并进行读取出来" class="headerlink" title="26、读写 objectFile 文件，把 RDD 保存为objectFile，RDD数据为Array((“a”, 1),(“b”, 2),(“c”, 3))，并进行读取出来"></a>26、读写 objectFile 文件，把 RDD 保存为objectFile，RDD数据为Array((“a”, 1),(“b”, 2),(“c”, 3))，并进行读取出来</h2><pre><code>val inputRDD = sc.makeRDD(Array((&quot;a&quot;, 1), (&quot;b&quot;, 2), (&quot;c&quot;, 3)))
//inputRDD.saveAsObjectFile(&quot;src/data/object&quot;)
sc.textFile(&quot;src/data/object/*&quot;).foreach(x =&gt; println(x))    
</code></pre>
<h2 id="28、使用Spark广播变量"><a href="#28、使用Spark广播变量" class="headerlink" title="28、使用Spark广播变量"></a>28、使用Spark广播变量</h2><pre><code>//    # 学生表：
//    id name age gender(0|1)
//    # 要求，输出学生信息，gender必须为男或者女，不能为0,1
//    # 使用广播变量把Map(&quot;0&quot; -&gt; &quot;女&quot;, &quot;1&quot; -&gt; &quot;男&quot;)设置为广播变量   

val inputRDD = sc.textFile(&quot;src/data/student.txt&quot;)
val broad = sc.broadcast(Map(&quot;0&quot; -&gt; &quot;女&quot;, &quot;1&quot; -&gt; &quot;男&quot;))
inputRDD.map&#123;
  line =&gt; val arr = line.trim.split(&quot;,&quot;)
  arr(3) = broad.value.getOrElse(arr.last,null)
    arr.mkString(&quot;,&quot;)
&#125;.foreach(x =&gt; println(x)) 
</code></pre>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/24/Flink_1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/24/Flink_1/" class="post-title-link" itemprop="url">Flink</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-24 09:58:32" itemprop="dateCreated datePublished" datetime="2023-08-24T09:58:32+08:00">2023-08-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-26 22:32:18" itemprop="dateModified" datetime="2023-08-26T22:32:18+08:00">2023-08-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="宰相必起于州部，猛将必发于卒伍"><a href="#宰相必起于州部，猛将必发于卒伍" class="headerlink" title="宰相必起于州部，猛将必发于卒伍"></a>宰相必起于州部，猛将必发于卒伍</h1><h1 id="枫叶云笔记"><a href="#枫叶云笔记" class="headerlink" title="枫叶云笔记"></a><a target="_blank" rel="noopener" href="https://cloud.fynote.com/share/d/GWHLLUWP">枫叶云笔记</a></h1><h2 id="Flink任务提交运行时架构"><a href="#Flink任务提交运行时架构" class="headerlink" title="Flink任务提交运行时架构"></a>Flink任务提交运行时架构</h2><p><img src="/2023/08/24/Flink_1/1.png" alt="Flink任务提交运行时架构"></p>
<ol>
<li>启动Flink集群首先会启动JobManager，Standalone集群模式下同时启动TaskManager，该模式资源也就固定；其他集群部署模式会根据提交任务来动态启动TaskManager；  </li>
<li>当在客户端提交任务后，客户端会将任务转换成JobGraph提交给JobManager；  </li>
<li>JobManager首先启动Dispatcher用于分发作业，运行Flink WebUI提供作业执行信息；  </li>
<li>Dispatcher启动后会启动JobMaster并将JobGraph提交给JobMaster，JobMaster会将JobGraph转换成可执行的ExecutionGraph。  </li>
<li>JobMaster向对应的资源管理器ResourceManager为当前任务申请Slot资源；  </li>
<li>在Standalone资源管理器中会直接找到启动的TaskManager来申请Slot资源，如果资源不足，那么任务执行失败；  </li>
<li>其他资源管理器会启动新的TaskManager，新启动的TaskManager会向ResourceManager进行注册资源，然后ResourceManager再向TaskManager申请Slot资源，如果资源不足会启动新的TaskManager来满足资源；  </li>
<li>TaskManager为对应的JobMaster offer Slot资源；  </li>
<li>JobMaster将要执行的task发送到对应的TaskManager上执行，TaskManager之间可以进行数据交换。</li>
</ol>
<h2 id="三种任务部署模式"><a href="#三种任务部署模式" class="headerlink" title="三种任务部署模式"></a>三种任务部署模式</h2><p>向Flink集群中提交任务有三种任务部署模式，分别如下：  </p>
<p>会话模式 - Session Mode<br>单作业模式 - Per-Job Mode(过时)<br>应用模式 - Application Mode    </p>
<p>以上三种任务提交模式的主要区别在于Flink集群的生命周期不同、资源的分配方式不同以及Flink 应用程序的main方法执行位置（Client客户端&#x2F;JobManager）不同。  </p>
<h3 id="会话模式-Session-Mode"><a href="#会话模式-Session-Mode" class="headerlink" title="会话模式 - Session Mode"></a>会话模式 - Session Mode</h3><p>Session模式下我们首先会启动一个集群，保持一个会话，这个会话中通过客户端提交作业，集群启动时所有的资源都已经确定，所以所有的提交的作业会竞争集群中的资源。这种模式适合单个作业规模小、执行时间短的大量作业。  </p>
<p>优势：只需要一个集群，所有作业提交之后都运行在这一个集群中，所有任务共享集群资源，每个任务执行完成后就释放资源。</p>
<p>缺点：因为集群资源是共享的，所以资源不够了，提交新的作业就会失败，如果一个作业发生故障导致TaskManager宕机，那么所有的作业都会受到影响。  </p>
<h3 id="单作业模式-Per-Job-Mode-过时"><a href="#单作业模式-Per-Job-Mode-过时" class="headerlink" title="单作业模式 - Per-Job Mode(过时)"></a>单作业模式 - Per-Job Mode(过时)</h3><p>Per-job模式是每提交一个作业会启动一个集群，集群只为这个作业而生，这种模式下客户端运行应用程序，然后启动集群，作业被提交给JobManager，进而分发给TaskManager执行，作业执行完成之后集群就会关闭，所有资源也会释放。  </p>
<h3 id="应用模式-Application-Mode"><a href="#应用模式-Application-Mode" class="headerlink" title="应用模式 - Application Mode"></a>应用模式 - Application Mode</h3><p>Session模式和Pre-Job模式都是在客户端将作业提交给JobManager。  </p>
<p>Application模式与Per-job类似，只是不需要客户端，每个Application提交之后就会启动一个JobManager，也就是创建一个集群，这个JobManager只为执行这一个Flink Application而存在，<br>Application中的多个job都会共用该集群，Application执行结束之后JobManager也就关闭了。这种模式下一个Application会动态创建自己的专属集群（JobManager）,所有任务共享该集群,不同<br>Application之间是完全隔离的，在生产环境中建议使用Application模式提交任务。    </p>
<p>Application模式是在JobManager上执行main方法，为每个Flink的Application创建一个Flink集群，如果该Application有多个任务，这些Flink任务共享一个集群。  </p>
<h2 id="Flink-On-Standalone任务提交"><a href="#Flink-On-Standalone任务提交" class="headerlink" title="Flink On Standalone任务提交"></a>Flink On Standalone任务提交</h2><p>Standlone集群部署时采用Session模式来构建集群。  </p>
<p>Flink On Standalone 任务提交支持Session会话模式和Application应用模式，不支持Per-Job单作业模式。  </p>
<h3 id="tandalone-Session模式"><a href="#tandalone-Session模式" class="headerlink" title="tandalone Session模式"></a>tandalone Session模式</h3><p>提交任务之前首先启动Standalone集群($FLINK_HOME&#x2F;bin&#x2F;start-cluster.sh)，然后再提交任务  </p>
<pre><code>[root@node4 ~]# cd /software/flink-1.16.0/bin/
[root@node4 bin]# ./flink run -m node1:8081 -d -c com.mashibing.flinkjava.code.chapter3.SocketWordCount /root/FlinkJavaCode-1.0-SNAPSHOT-jar-with-dependencies.jar
</code></pre>
<p>-m –jobmanager,指定提交任务连接的JobManager地址<br>-c –class,指定运行的class主类<br>-d –detached，任务提交后在后台独立运行，退出客户端，也可不指定<br>-p –parallelism,执行程序的并行度  </p>
<p>也可以通过参数”pipeline.name”来自定义指定Job 名称，提交命令如下：  </p>
<pre><code>[root@node4 bin]# ./flink run -m node1:8081 -d -Dpipeline.name=socket-wc -c com.mashibing.flinkjava.code.chapter3.SocketWordCount /root/FlinkJavaCode-1.0-SNAPSHOT-jar-with-dependencies.jar  
</code></pre>
<h4 id="Standalone-Session模式任务提交流程"><a href="#Standalone-Session模式任务提交流程" class="headerlink" title="Standalone Session模式任务提交流程"></a>Standalone Session模式任务提交流程</h4><p>Standalone Session模式提交任务中首先需要创建Flink集群，集群创建启动的同时Dispatcher、JobMaster、ResourceManager对象一并创建、TaskManager也一并启动，TaskManager会向集群ResourceManager汇报Slot信息，Flink集群资源也就确定了。Standalone Session模式提交任务流程如下：  </p>
<p><img src="/2023/08/24/Flink_1/2.png" alt="Standalone Session模式任务提交流程">  </p>
<pre><code>    1.在客户端提交Flink任务，客户端会将任务转换成JobGraph提交给JobManager。  
    2.Dispatcher将提交任务提交给JobMaster。  
    3.JobMaster向ResourceManager申请Slot资源。  
    4.ResourceManager会在对应的TaskManager上划分Slot资源。  
    5.TaskManager向JobMaster offer Slot资源。  
    6.JobMaster将任务对应的task发送到TaskManager上执行。    
</code></pre>
<h3 id="Standalone-Application模式"><a href="#Standalone-Application模式" class="headerlink" title="Standalone Application模式"></a>Standalone Application模式</h3><p>Standalone Application模式中不会预先创建Flink集群，在提交Flink 任务的同时会创建JobManager，启动Flink集群,然后需要手动启动TaskManager连接该Flink集群，启动的TaskManager会根据$FLINK_HOME&#x2F;conf&#x2F;flink-conf.yaml配置文件中的”jobmanager.rpc.address”配置找JobManager，所以这里选择在node1节点上提交任务并启动JobManager，方便后续其他节点启动TaskManager后连接该节点。  </p>
<h4 id="任务提交指令及顺序"><a href="#任务提交指令及顺序" class="headerlink" title="任务提交指令及顺序"></a>任务提交指令及顺序</h4><h5 id="1-准备Flink-jar包"><a href="#1-准备Flink-jar包" class="headerlink" title="1.准备Flink jar包"></a>1.准备Flink jar包</h5><p>在node1节点上将Flink 打好的”FlinkJavaCode-1.0-SNAPSHOT-jar-with-dependencies.jar”jar包放在 $FLINK_HOME&#x2F;lib目录下  </p>
<h5 id="2-提交任务，在node1-节点上启动-JobManager"><a href="#2-提交任务，在node1-节点上启动-JobManager" class="headerlink" title="2.提交任务，在node1 节点上启动 JobManager"></a>2.提交任务，在node1 节点上启动 JobManager</h5><pre><code>[root@node1 ~]# cd /software/flink-1.16.0/bin/

执行如下命令，启动JobManager  
[root@node1 bin]# ./standalone-job.sh start --job-classname com.mashibing.flinkjava.code.chapter3.SocketWordCount  
</code></pre>
<h5 id="3-启动TaskManager"><a href="#3-启动TaskManager" class="headerlink" title="3. 启动TaskManager"></a>3. 启动TaskManager</h5><p>在node1、node2、node3任意一台节点上启动taskManager，根据$FLINK_HOME&#x2F;conf&#x2F;flinkconf.yaml配置文件中”jobmanager.rpc.address”配置项会找到对应node1 JobManager  </p>
<pre><code>在node1节点上启动TaskManager
[root@node1 ~]# cd /software/flink-1.16.0/bin/
[root@node1 bin]# ./taskmanager.sh start

在node2节点上启动TaskManager
[root@node2 ~]# cd /software/flink-1.16.0/bin/
[root@node2 bin]# ./taskmanager.sh start
</code></pre>
<p>启动两个TaskManager后可以看到Flink WebUI中对应的有2个TaskManager，可以根据自己任务使用资源的情况，手动启动多个TaskManager。    </p>
<h5 id="4-停止集群"><a href="#4-停止集群" class="headerlink" title="4.停止集群"></a>4.停止集群</h5><pre><code>停止启动的JobManager
[root@node1 bin]# ./standalone-job.sh stop

停止启动的TaskManager
[root@node1 bin]# ./taskmanager.sh stop
[root@node2 bin]# ./taskmanager.sh stop  
</code></pre>
<p>可以以同样的方式在其他节点上以Standalone Application模式提交先前的Flink任务，但是每次提交都是当前提交任务独享集群资源。    </p>
<p>Standalone Application模式提交任务中提交任务的同时会启动JobManager创建Flink集群，但是需要手动启动TaskManager，这样提交的任务才能正常运行，如果提交的任务使用资源多，还可以启动多个TaskManager。  </p>
<h4 id="Standalone-Application模式任务提交流程"><a href="#Standalone-Application模式任务提交流程" class="headerlink" title="Standalone Application模式任务提交流程"></a>Standalone Application模式任务提交流程</h4><p><img src="/2023/08/24/Flink_1/3.png" alt="Standalone Application模式任务提交流程">   </p>
<pre><code>    1.在客户端提交Flink任务的同时启动JobManager，客户端会将任务转换成JobGraph提交给JobManager。
    2.Dispatcher会启动JobMaster，Dispatcher将提交任务提交给JobMaster。
    3.JobMaster向ResourceManager申请Slot资源。
    4.手动启动TaskManager，TaskManager会向ResourceManager注册Slot资源
    5.ResourceManager会在对应的TaskManager上划分Slot资源。
    6.TaskManager向JobMaster offer Slot资源。
    7.JobMaster将任务对应的task发送到TaskManager上执行。  
</code></pre>
<h3 id="tandalone-Session模式-和-Standalone-Application模式-任务提交流程的区别"><a href="#tandalone-Session模式-和-Standalone-Application模式-任务提交流程的区别" class="headerlink" title="tandalone Session模式 和 Standalone Application模式 任务提交流程的区别"></a>tandalone Session模式 和 Standalone Application模式 任务提交流程的区别</h3><p>Standalone Session模式中启动Flink集群时JobManager、TaskManager、JobMaster会预先启动<br>Standalone Application模式中提交任务时同时启动集群JobManager、JobMaster，需要手动启动TaskManager</p>
<h2 id="Flink-On-Yarn任务提交"><a href="#Flink-On-Yarn任务提交" class="headerlink" title="Flink On Yarn任务提交"></a>Flink On Yarn任务提交</h2><h3 id="Flink-On-Yarn运行原理"><a href="#Flink-On-Yarn运行原理" class="headerlink" title="Flink On Yarn运行原理"></a>Flink On Yarn运行原理</h3><p><img src="/2023/08/24/Flink_1/4.png" alt="Flink On Yarn运行原理">     </p>
<pre><code>    1.当启动一个新的Flink YARN Client会话时，客户端首先会检查所请求的资源（容器和内存）是否可用，之后，它会上传Flink配置和JAR文件到HDFS。  

    2.客户端的下一步是向ResourceManager请求一个YARN容器启动ApplicationMaster。JobManager和ApplicationMaster(AM)运行在同一个容器中，一旦它们成功地启动了，AM就能够知JobManager的地址，它会为TaskManager生成一个新的Flink配置文件（这样它才能连上JobManager），该文件也同样会被上传到HDFS。另外，AM容器还提供了Flink的Web界面服务。Flink用来提供服务的端是由用户和应用程序ID作为偏移配置的，这使得用户能够并行执行多个YARN会话。    

    3.之后，AM开始为Flink的TaskManager分配容器（Container），从HDFS下载JAR文件和修改过的配置文件，一旦这些步骤完成了，Flink就可以基于Yarn运行任务了  
</code></pre>
<h3 id="代码及Yarn环境准备"><a href="#代码及Yarn环境准备" class="headerlink" title="代码及Yarn环境准备"></a>代码及Yarn环境准备</h3><h4 id="1-准备代码"><a href="#1-准备代码" class="headerlink" title="1.准备代码"></a>1.准备代码</h4><p>Flink On Yarn任务提交支持Session会话模式、Per-Job单作业模式、Application应用模式。  </p>
<p>Flink允许在一个main方法中提交多个job任务，多Job执行的顺序不受部署模式影响，但受启动Job的调用影响，每次调用execute()或者executeAsyc()方法都会触发job执行，我们可以在一个Flink Application中执行多次execute()或者executeAsyc()方法来触发多个job执行，两者区别如下：  </p>
<pre><code>execute()：该方法为阻塞方法，当一个Flink Application中执行多次execute()方法触发多个job时，下一个job的执行会被推迟到该job执行完成后再执行。  

executeAsyc()：该方法为非阻塞方法，一旦调用该方法触发job后，后续还有job也会立即提交执行。
</code></pre>
<p>当一个Flink Application中有多个job时，这些job之间没有直接通信的机制，所以建议编写Flink代码时一个Application中包含一个job即可，目前只有非HA的Application模式可以支持多job运行。  </p>
<pre><code>//1.准备环境
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
//2.读取Socket数据 ,获取ds1和ds2
DataStreamSource&lt;String&gt; ds1 = env.socketTextStream(&quot;node5&quot;, 8888);
DataStreamSource&lt;String&gt; ds2 = env.socketTextStream(&quot;node5&quot;, 9999);

//3.1 对ds1 直接输出原始数据
SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; transDs1 = ds1.flatMap((String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) -&gt; &#123;
    String[] words = line.split(&quot;,&quot;);
    for (String word : words) &#123;
        out.collect(Tuple2.of(word, 1));
    &#125;
&#125;).returns(Types.TUPLE(Types.STRING, Types.INT));
transDs1.print();
env.executeAsync(&quot;first job&quot;);

//3.2 对ds2准备K,V格式数据 ,统计实时WordCount
SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tupleDS = ds2.flatMap((String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) -&gt; &#123;
    String[] words = line.split(&quot;,&quot;);
    for (String word : words) &#123;
        out.collect(Tuple2.of(word, 1));
    &#125;
&#125;).returns(Types.TUPLE(Types.STRING, Types.INT));
tupleDS.keyBy(tp -&gt; tp.f0).sum(1).print();

//5.execute触发执行
env.execute(&quot;second job&quot;);  
</code></pre>
<h4 id="2-yarn-环境准备"><a href="#2-yarn-环境准备" class="headerlink" title="2.yarn 环境准备"></a>2.yarn 环境准备</h4><p>在Per-Job模式中，Flink每个job任务都会启动一个对应的Flink集群，基于Yarn提交后会在Yarn中同时运行多个实时Flink任务，在HDFS中$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;capacity-scheduler.xml中有”yarn.scheduler.capacity.maximum-am-resource-percent”配置项，该项默认值为0.1，表示Yarn集群中运行的所有ApplicationMaster的资源比例上限，默认0.1表示10%，这个参数变相控制了处于活动状态的Application个数，所以这里我们修改该值为0.5，否则后续在Yarn中运行多个Flink Application时只有一个Application处于活动运行状态，其他处于Accepted状态.  </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/18/Java_questions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/18/Java_questions/" class="post-title-link" itemprop="url">java问题集</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-18 11:30:12" itemprop="dateCreated datePublished" datetime="2023-08-18T11:30:12+08:00">2023-08-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-23 11:04:39" itemprop="dateModified" datetime="2023-08-23T11:04:39+08:00">2023-08-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="道虽迩，不行不至；事虽小，不为不成"><a href="#道虽迩，不行不至；事虽小，不为不成" class="headerlink" title="道虽迩，不行不至；事虽小，不为不成"></a>道虽迩，不行不至；事虽小，不为不成</h1><h2 id="java中double类型数据如何指定保留位数？"><a href="#java中double类型数据如何指定保留位数？" class="headerlink" title="java中double类型数据如何指定保留位数？"></a>java中double类型数据如何指定保留位数？</h2><p><img src="/2023/08/18/Java_questions/1.png" alt="java中double类型数据如何指定保留位数"></p>
<h2 id="字符串的内容比较使用-为啥无效？"><a href="#字符串的内容比较使用-为啥无效？" class="headerlink" title="字符串的内容比较使用&#x3D;&#x3D;为啥无效？"></a>字符串的内容比较使用&#x3D;&#x3D;为啥无效？</h2><p>字符串的内容比较应该使用方法equals</p>
<pre><code>if(&quot;丁真&quot;.equals(name) &amp;&amp; &quot;666&quot;.equals(passwd))&#123;
        //if(name == &quot;丁真&quot; &amp;&amp; passwd == &quot;666&quot;)&#123;  字符串判断相等时，不能使用 == ，会无效
        System.out.println(&quot;恭喜你，登录成功~&quot;);
        break;
        &#125;else&#123;
            chance--;
            System.out.println(&quot;你还有&quot;+chance+&quot;次登录机会&quot;);
        &#125;
</code></pre>
<p>equals方法使用细节：  </p>
<pre><code>System.out.println(name.equals(&quot;林黛玉&quot;));
System.out.println(&quot;林黛玉&quot;.equals(name));//T [推荐，可以避免空指针
</code></pre>
<h2 id="Java中系统输入-char-字符-‘男’-应该如何做？"><a href="#Java中系统输入-char-字符-‘男’-应该如何做？" class="headerlink" title="Java中系统输入 char 字符 ‘男’ 应该如何做？"></a>Java中系统输入 char 字符 ‘男’ 应该如何做？</h2><pre><code>Scanner myScanner = new Scanner(System.in);
char gender = myScanner.next().charAt(0);
 if (gender == &#39;男&#39;)&#123;
            System.out.println(&quot;进入男子组&quot;);
        &#125;else if(gender == &#39;女&#39;)&#123;
            System.out.println(&quot;进入女子组&quot;);
        &#125;else&#123;
            System.out.println(&quot;你的性别有误，不能参加决赛 ~ &quot;);
        &#125;
</code></pre>
<h2 id="Java中数组的深拷贝和浅拷贝"><a href="#Java中数组的深拷贝和浅拷贝" class="headerlink" title="Java中数组的深拷贝和浅拷贝"></a>Java中数组的深拷贝和浅拷贝</h2><p>深拷贝： int[] arr2 &#x3D; new int[arr1.length]  ,arr2数组重新开辟了一个数组空间<br>浅拷贝： arr2 &#x3D; arr1 这是将arr1的数组地址赋值给了arr2，这样arr1和arr2其实都指向了同一个数组，此时arr1和arr2的元素都是同步变化的，并没有独立。</p>
<h2 id="Java-中对象在内存中的存在形式是怎样的"><a href="#Java-中对象在内存中的存在形式是怎样的" class="headerlink" title="Java 中对象在内存中的存在形式是怎样的?"></a>Java 中对象在内存中的存在形式是怎样的?</h2><p><img src="/2023/08/18/Java_questions/2.png" alt="Java 中对象在内存中的存在形式"></p>
<p>数值属性存储在堆内存中  </p>
<p>方法存储在方法区中  </p>
<p>方法区中的常量池负责存储数值属性之外的其他属性值  </p>
<h3 id="类和对象的内存分配机制"><a href="#类和对象的内存分配机制" class="headerlink" title="类和对象的内存分配机制"></a>类和对象的内存分配机制</h3><h4 id="Java-内存的结构分析"><a href="#Java-内存的结构分析" class="headerlink" title="Java 内存的结构分析"></a>Java 内存的结构分析</h4><ol>
<li><p>栈： 一般存放基本数据类型(局部变量)  </p>
</li>
<li><p>堆： 存放对象(Cat cat , 数组等)   </p>
</li>
<li><p>方法区：常量池(常量，比如字符串)， 类加载信息</p>
</li>
</ol>
<h2 id="类定义在public-class-代码块内外有何区别？-什么时候需要使用static-class-？"><a href="#类定义在public-class-代码块内外有何区别？-什么时候需要使用static-class-？" class="headerlink" title="类定义在public class 代码块内外有何区别？ 什么时候需要使用static class ？"></a>类定义在public class 代码块内外有何区别？ 什么时候需要使用static class ？</h2><p><img src="/2023/08/18/Java_questions/3.png" alt="类定义在public class 代码块内外有何区别？"></p>
<p><img src="/2023/08/18/Java_questions/4.png" alt="类定义在public class 代码块内外有何区别？"></p>
<p>在别的代码中定义了同名类，本次代码中需要重新定义时，类应当定义在public class代码块中，使用static描述，否则会报错.这个是叫静态内部类吗？  </p>
<p>类本质上就是一种数据类型。可以定义属性，定义方法。使用的时候和数据类型一样使用即可。  </p>
<h3 id="方法的调用机制原理"><a href="#方法的调用机制原理" class="headerlink" title="方法的调用机制原理"></a>方法的调用机制原理</h3><p><img src="/2023/08/18/Java_questions/5.png" alt="方法的调用机制原理"></p>
<h2 id="形参和实参"><a href="#形参和实参" class="headerlink" title="形参和实参"></a>形参和实参</h2><pre><code>getSumAndSub(int n1,int n2)  这就是形参  
getSumAndSub(1,8) 这就是实参
</code></pre>
<h2 id="基本数据类型的传参机制"><a href="#基本数据类型的传参机制" class="headerlink" title="基本数据类型的传参机制"></a>基本数据类型的传参机制</h2><p>基本数据类型，传递的是值(值拷贝),形参的任何改变不影响实参</p>
<p>main方法中的引用类型属性可以被方法修改    </p>
<p>main方法中的基本类型属性无法被方法修改  </p>
<p>引用类型传递的是地址（传递也是值，但是值是地址），可以通过形参影响实参！  </p>
<h2 id="Java中的可变参数"><a href="#Java中的可变参数" class="headerlink" title="Java中的可变参数"></a>Java中的可变参数</h2><p>int… 与 int[] 等价  </p>
<p>可变参数可以和普通类型的参数一起放在形参列表,但必须保证可变参数在最后    </p>
<p>一个形参列表中只能出现一个可变参数  </p>
<p>可变参数的实参可以是数组 </p>
<h2 id="全局变量和局部变量"><a href="#全局变量和局部变量" class="headerlink" title="全局变量和局部变量"></a>全局变量和局部变量</h2><p>全局变量就是类的属性<br>全局变量(属性)可以不赋值,直接使用,因为有默认值  </p>
<p>局部变量是类中方法代码块里定义的变量<br>局部变量必须赋值后,才能使用,因为没有默认值</p>
<p>方法与方法之间的局部变量无法相互调用<br>方法可以直接调用类的属性   </p>
<p>类和类之间的属性可以相互调用  </p>
<h2 id="四种访问修饰符的区别"><a href="#四种访问修饰符的区别" class="headerlink" title="四种访问修饰符的区别"></a>四种访问修饰符的区别</h2><h2 id="类，方法，构造器的定义语法汇总"><a href="#类，方法，构造器的定义语法汇总" class="headerlink" title="类，方法，构造器的定义语法汇总"></a>类，方法，构造器的定义语法汇总</h2><h3 id="类的定义语法"><a href="#类的定义语法" class="headerlink" title="类的定义语法"></a>类的定义语法</h3><pre><code>class Dog&#123;
    
    &#125;
</code></pre>
<h3 id="方法的定义语法"><a href="#方法的定义语法" class="headerlink" title="方法的定义语法"></a>方法的定义语法</h3><pre><code>public void getname(String name)&#123;
    System.out.println(name);	
        &#125;
</code></pre>
<h3 id="构造器的定义语法"><a href="#构造器的定义语法" class="headerlink" title="构造器的定义语法"></a>构造器的定义语法</h3><p>构造器是特殊的方法  </p>
<p>1.方法名和类名一致<br>2.方法无返回值  </p>
<pre><code>class Dog&#123;
    public Dog()&#123;
    
        &#125;

    public Dog(String str,Int num )&#123;
            //...
        &#125;
    &#125;
</code></pre>
<p>构造器的使用  </p>
<pre><code>Dog dog1 = new Dog(&quot;Str1&quot;,10);  
Dog dog2 = new Dog();  
</code></pre>
<h2 id="java代码反编译"><a href="#java代码反编译" class="headerlink" title="java代码反编译"></a>java代码反编译</h2><p>javap指令,反编译    </p>
<h2 id="IDEA的常用快捷键有哪些"><a href="#IDEA的常用快捷键有哪些" class="headerlink" title="IDEA的常用快捷键有哪些 ?"></a>IDEA的常用快捷键有哪些 ?</h2><ol>
<li>删除当前行, 默认是 ctrl + Y 自己配置 ctrl + d  </li>
<li>复制当前行, 自己配置 ctrl + alt + 向下光标  </li>
<li>补全代码 alt + &#x2F;  </li>
<li>添加注释和取消注释 ctrl + &#x2F; 【第一次是添加注释，第二次是取消注释】  </li>
<li>导入该行需要的类 先配置 auto import , 然后使用 alt+enter 即可  </li>
<li>快速格式化代码 ctrl + alt + L  </li>
<li>快速运行程序 自己定义 alt + R  </li>
<li>生成构造器等 alt + insert [提高开发效率]  </li>
<li>查看一个类的层级关系 ctrl + H [学习继承后，非常有用]  </li>
<li>将光标放在一个方法上，输入 ctrl + B , 可以定位到方法 [学继承后，非常有用]  </li>
<li>自动的分配变量名 , 通过 在后面加 .var   </li>
<li>还有很多其它的快捷键</li>
</ol>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/12/Spark-practice/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/12/Spark-practice/" class="post-title-link" itemprop="url">Spark报错经验记录</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-12 19:02:51" itemprop="dateCreated datePublished" datetime="2023-08-12T19:02:51+08:00">2023-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-13 19:42:18" itemprop="dateModified" datetime="2023-08-13T19:42:18+08:00">2023-08-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="大鹏飞兮振八裔，中天摧兮力不济"><a href="#大鹏飞兮振八裔，中天摧兮力不济" class="headerlink" title="大鹏飞兮振八裔，中天摧兮力不济"></a>大鹏飞兮振八裔，中天摧兮力不济</h1><h1 id="使用Spark的报错经验"><a href="#使用Spark的报错经验" class="headerlink" title="使用Spark的报错经验"></a>使用Spark的报错经验</h1><h2 id="IDEA工具中运行Spark程序报缺失scala-compiler-2-12-16-jar包"><a href="#IDEA工具中运行Spark程序报缺失scala-compiler-2-12-16-jar包" class="headerlink" title="IDEA工具中运行Spark程序报缺失scala-compiler-2.12.16.jar包"></a>IDEA工具中运行Spark程序报缺失scala-compiler-2.12.16.jar包</h2><p><img src="/2023/08/12/Spark-practice/1.png" alt="IDEA的Scala SDK设置位置">  </p>
<p>重新设置IDEA中Scala SDK即可  </p>
<h2 id="使用Spark-SQL往Mysql中写入数据，出现中文乱码"><a href="#使用Spark-SQL往Mysql中写入数据，出现中文乱码" class="headerlink" title="使用Spark SQL往Mysql中写入数据，出现中文乱码"></a>使用Spark SQL往Mysql中写入数据，出现中文乱码</h2><h3 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/MASILEJFOAISEGJIAE/article/details/89314591?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-89314591-blog-109695269.235%5Ev38%5Epc_relevant_sort_base2&spm=1001.2101.3001.4242.1&utm_relevant_index=4">解决方案</a></p>
<p>补充一下，如果在调整编码之前就已经创建了数据库和表的话，需要再单独修改数据库和表的编码格式  </p>
<p><img src="/2023/08/12/Spark-practice/2.png" alt="Spark SQL写入Mysql数据的案例代码">    </p>
<pre><code>package com.atguigu.bigdata.spark.core.sql

import org.apache.spark.SparkConf
import org.apache.spark.sql.&#123;SaveMode, SparkSession&#125;

object Spark04_SparkSQL_JDBC &#123;
  def main(args: Array[String]): Unit = &#123;

    // TODO 创建SparkSQL的运行环境
    val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)
    val spark = SparkSession.builder().config(sparkConf).getOrCreate()
    import spark.implicits._

    // 读取MySQL数据
    val df = spark.read
      .format(&quot;jdbc&quot;)
      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306/test&quot;)
      .option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)
      .option(&quot;user&quot;, &quot;root&quot;)
      .option(&quot;password&quot;, &quot;Wo@6930886&quot;)
      .option(&quot;dbtable&quot;, &quot;course&quot;)
      .load()

    df.show
    // 保存数据
    df.write
      .format(&quot;jdbc&quot;)
      .option(&quot;url&quot;,&quot;jdbc:mysql://hadoop101:3306/test?useUnicode=true&amp;characterEncoding=utf8&quot;)
      .option(&quot;driver&quot;,&quot;com.mysql.jdbc.Driver&quot;)
      .option(&quot;user&quot;,&quot;root&quot;)
      .option(&quot;password&quot;,&quot;Wo@6930886&quot;)
      .option(&quot;dbtable&quot;,&quot;course1&quot;)
      .mode(SaveMode.Overwrite)
      .save()

    // TODO 关闭环境
    spark.close()
  &#125;

&#125;
</code></pre>
<h2 id="使用IDEA新建项目时，无法创建Scala代码"><a href="#使用IDEA新建项目时，无法创建Scala代码" class="headerlink" title="使用IDEA新建项目时，无法创建Scala代码"></a>使用IDEA新建项目时，无法创建Scala代码</h2><p>先随便创建一个文件，以scala结尾，之后代码编辑器右上角会弹出“设置Scala SDK”,点击设置之后，就可以正常创建Scala程序了  </p>
<p><img src="/2023/08/12/Spark-practice/3.png" alt="新增一个文件后缀写成scala">  </p>
<p><img src="/2023/08/12/Spark-practice/4.png" alt="可以创建Scala程序了">  </p>
<h2 id="IDEA中Alt-Enter自动导入import失效，且手动输入import语句，apache报错"><a href="#IDEA中Alt-Enter自动导入import失效，且手动输入import语句，apache报错" class="headerlink" title="IDEA中Alt + Enter自动导入import失效，且手动输入import语句，apache报错"></a>IDEA中Alt + Enter自动导入import失效，且手动输入import语句，apache报错</h2><p><img src="/2023/08/12/Spark-practice/5.png" alt="导包报错"> </p>
<p>原因是没有在pom文件中导入Spark相关包    </p>
<p>先在pom文件中配置参数导入Spark相关的包，再修改Maven仓库的版本为3.8.1以前的版本，重新使用maven加载包即可  </p>
<p><img src="/2023/08/12/Spark-practice/7.png" alt="导包报错">   </p>
<p>Pom文件配置示例  </p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
     xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
     xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

&lt;groupId&gt;groupId&lt;/groupId&gt;
&lt;artifactId&gt;SGG_scala_Spark3.0.0_practice&lt;/artifactId&gt;
&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

&lt;dependencies&gt;
&lt;!--        spark--&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-yarn_2.12&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming-kafka-0-10_2.12&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;

&lt;!--mysql--&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;mysql&lt;/groupId&gt;
        &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
        &lt;version&gt;5.1.27&lt;/version&gt;
    &lt;/dependency&gt;

&lt;!--        jackson--&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
        &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;
        &lt;version&gt;2.9.9&lt;/version&gt;
    &lt;/dependency&gt;
&lt;!--        &lt;dependency&gt;--&gt;
&lt;!--            &lt;groupId&gt; com.fasterxml.jackson.core &lt;/groupId&gt;--&gt;
&lt;!--            &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;--&gt;
&lt;!--            &lt;version&gt;2.9.9&lt;/version&gt;--&gt;
&lt;!--        &lt;/dependency&gt;--&gt;
        &lt;dependency&gt;
        &lt;groupId&gt;com.thoughtworks.paranamer&lt;/groupId&gt;
        &lt;artifactId&gt;paranamer&lt;/artifactId&gt;
        &lt;version&gt;2.8&lt;/version&gt;
    &lt;/dependency&gt;



    &lt;!--common--&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
        &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;
        &lt;version&gt;3.3&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/com.alibaba/druid --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
        &lt;artifactId&gt;druid&lt;/artifactId&gt;
        &lt;version&gt;1.1.10&lt;/version&gt;
    &lt;/dependency&gt;




&lt;!--hive--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
            &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
            &lt;version&gt;2.1.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
            &lt;artifactId&gt;hive-metastore&lt;/artifactId&gt;
            &lt;version&gt;2.1.1&lt;/version&gt;
        &lt;/dependency&gt;



    &lt;/dependencies&gt;

&lt;/project&gt;
</code></pre>
<p>清除缓存重启IDEA    </p>
<p><img src="/2023/08/12/Spark-practice/8.png" alt="IDEA中修复import Spark包报错问题">   </p>
<p>OK鸟 ~  </p>
<h2 id="IDEA中下载源码和查看源码的方法"><a href="#IDEA中下载源码和查看源码的方法" class="headerlink" title="IDEA中下载源码和查看源码的方法"></a>IDEA中下载源码和查看源码的方法</h2><p><img src="/2023/08/12/Spark-practice/6.png" alt="IDEA中下载源码"> </p>
<p>ctrl + H 跳到源码内      </p>
<h2 id="IDEA中编写好Scala程序后无法运行程序"><a href="#IDEA中编写好Scala程序后无法运行程序" class="headerlink" title="IDEA中编写好Scala程序后无法运行程序"></a>IDEA中编写好Scala程序后无法运行程序</h2><p><img src="/2023/08/12/Spark-practice/9.png" alt="IDEA中无法运行Scala程序">  </p>
<p>手动构建应用程序  </p>
<p><img src="/2023/08/12/Spark-practice/10.png" alt="手动构建应用程序"></p>
<p>Scala编写Spark运行程序时，应当选用Object而非class类型文件  </p>
<p><img src="/2023/08/12/Spark-practice/11.png" alt="Scala编写Spark程序选用Object">  </p>
<h2 id="IDEA中本地编写Spark程序，提交到Yarn集群上运行出错"><a href="#IDEA中本地编写Spark程序，提交到Yarn集群上运行出错" class="headerlink" title="IDEA中本地编写Spark程序，提交到Yarn集群上运行出错"></a>IDEA中本地编写Spark程序，提交到Yarn集群上运行出错</h2><p>1.修改IDEA中本地代码  </p>
<p>本地开发测试时，val SparkConf &#x3D; new SparkConf().setMaster(“local[<em>]”).setAppName(“SparkSQL_mysql_join_mysql”)<br>打包上传到yarn集群运行前，需要删掉setMaster(“local[</em>]”)，即改成val SparkConf &#x3D; new SparkConf().setAppName(“SparkSQL_mysql_join_mysql”)  </p>
<p><img src="/2023/08/12/Spark-practice/12.png" alt="Spark程序打包前修改Sparkconf参数">  </p>
<p>2.先Maven clean，再构建项目，生成target文件夹，尤其注意，一定要获取到target&#x2F;classes文件夹   </p>
<p><img src="/2023/08/12/Spark-practice/13.png" alt="Spark程序打包"> </p>
<p>3.将打好的Spark程序jar包提交到服务器上  </p>
<p><img src="/2023/08/12/Spark-practice/14.png" alt="Spark程序jar包上传到服务器上">  </p>
<p>4.如果程序运行时需要调用到其他Jar包的类，集群运行时会报错，需要添加jars参数指定jar包位置  </p>
<pre><code>/opt/cloudera/parcels/CDH/lib/spark/bin/spark-submit --jars /opt/module/spark/mysql-connector-java.jar --class com.zyy.sparksql.SparkSQL_mysql_join_mysql --master yarn --deploy-mode cluster /opt/module/spark/SGG_scala_Spark3.0.0_practice-1.0-SNAPSHOT.jar 10  
</code></pre>
<p>5.查看运行结果，运行成功   </p>
<p><img src="/2023/08/12/Spark-practice/15.png" alt="Spark程序运行成功">   </p>
<p><img src="/2023/08/12/Spark-practice/16.png" alt="Spark程序运行成功">    </p>
<p><img src="/2023/08/12/Spark-practice/17.png" alt="Spark History中运行记录"></p>
<p><img src="/2023/08/12/Spark-practice/18.png" alt="Spark History中查看工作流图">   </p>
<h2 id="IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表"><a href="#IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表" class="headerlink" title="IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表"></a>IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表</h2><p><img src="/2023/08/12/Spark-practice/19.png" alt="Hive_2_mysql">  </p>
<p>代码示例 </p>
<pre><code>package com.zyy.sparksql

import org.apache.spark.SparkConf
import org.apache.spark.sql.&#123;SaveMode, SparkSession&#125;

object SparkSQL_Hive_2_Mysql &#123;
  def main(args: Array[String]): Unit = &#123;
    System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;)
    // 1. TODO 创建SparkSQL运行环境

    val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL_Hive&quot;)
    val spark = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()

    // 2. 编写Spark 操作Hive 的计算逻辑
    //spark.sql(&quot;show databases&quot;).show()
    //spark.sql(&quot;use hive_tiaoyou&quot;)
    //spark.sql(&quot;show tables&quot;).show()
    //spark.sql(&quot;select * from order_detail&quot;).show()
    //spark.sql(&quot;select * from province_info as info1 left join province_info info2 on info1.id = info2.id&quot;).show()

    spark.sql(&quot;use hive_tiaoyou&quot;)
    val databases = spark.sql(&quot;show tables&quot;)

    databases.show()

    databases.write.format(&quot;jdbc&quot;)
      .option(&quot;url&quot;,&quot;jdbc:mysql://hadoop101:3306/test?useUnicode=true&amp;characterEncoding=utf8&quot;)
      .option(&quot;driver&quot;,&quot;com.mysql.jdbc.Driver&quot;)
      .option(&quot;user&quot;,&quot;root&quot;)
      .option(&quot;password&quot;,&quot;Wo@6930886&quot;)
      .option(&quot;dbtable&quot;,&quot;test_databases&quot;)
      .mode(SaveMode.Overwrite)
      .save()
    // 3. TODO 关闭环境
    spark.close()
      &#125;
    &#125;  
</code></pre>
<p>Spark要操作Hive之前，需要注意，要先把Hive-site.xml,hdfs-site.xml,core-site.xml,yarn-site.xml这几个文件从集群中下载下来，放到IDEA中resourses文件夹下  </p>
<p>之后通过创建环境时，设置enableHiveSupport，即可通过spark.sql(“Hive语句”)进行访问Hive的数据  </p>
<pre><code>val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL_Hive&quot;)  
val spark = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()
</code></pre>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/11/Spark-Streaming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/11/Spark-Streaming/" class="post-title-link" itemprop="url">Spark-Streaming</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-08-11 22:03:30 / 修改时间：22:57:34" itemprop="dateCreated datePublished" datetime="2023-08-11T22:03:30+08:00">2023-08-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也"><a href="#英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也" class="headerlink" title="英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也"></a>英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也</h1><h1 id="SparkStreaming-概述"><a href="#SparkStreaming-概述" class="headerlink" title="SparkStreaming 概述"></a>SparkStreaming 概述</h1><h2 id="Spark-Streaming-是什么"><a href="#Spark-Streaming-是什么" class="headerlink" title="Spark Streaming 是什么"></a>Spark Streaming 是什么</h2><p>Spark Streaming 用于流式数据的处理。Spark Streaming 支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如：map、reduce、join、window 等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。  </p>
<h2 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h2><p>和 Spark 基于 RDD 的概念很相似，Spark Streaming 使用离散化流(discretized stream)作为抽象表示，叫作 DStream。  </p>
<p>DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此得名“离散化”)。所以简单来将，DStream 就是对 RDD 在实时数据处理场景的一种封装。  </p>
<h2 id="Spark-Streaming-的特点"><a href="#Spark-Streaming-的特点" class="headerlink" title="Spark Streaming 的特点"></a>Spark Streaming 的特点</h2><p>易用    </p>
<p>容错  </p>
<p>易整合到 Spark 体系    </p>
<h2 id="Spark-Streaming-架构"><a href="#Spark-Streaming-架构" class="headerlink" title="Spark Streaming 架构"></a>Spark Streaming 架构</h2><p><img src="/2023/08/11/Spark-Streaming/1.png" alt="整体架构图">  </p>
<p><img src="/2023/08/11/Spark-Streaming/2.png" alt="架构图"> </p>
<h2 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h2><p>背压机制（即 Spark Streaming Backpressure）: 根据JobScheduler 反馈作业的执行信息来动态调整 Receiver 数据接收率。  </p>
<p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用 backpressure 机制，默认值false，即不启用。    </p>
<h2 id="Dstream-入门"><a href="#Dstream-入门" class="headerlink" title="Dstream 入门"></a>Dstream 入门</h2><h3 id="WordCount-案例实操"><a href="#WordCount-案例实操" class="headerlink" title="WordCount 案例实操"></a>WordCount 案例实操</h3><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><pre><code>&lt;dependency&gt;
 	&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
 	&lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt;
 	&lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;    
</code></pre>
<h4 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h4><pre><code>object StreamWordCount &#123;
 def main(args: Array[String]): Unit = &#123;
     //1.初始化 Spark 配置信息
     val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;StreamWordCount&quot;)
     //2.初始化 SparkStreamingContext
     val ssc = new StreamingContext(sparkConf, Seconds(3))
     //3.通过监控端口创建 DStream，读进来的数据为一行行
     val lineStreams = ssc.socketTextStream(&quot;linux1&quot;, 9999)
     //将每一行数据做切分，形成一个个单词
     val wordStreams = lineStreams.flatMap(_.split(&quot; &quot;))
     //将单词映射成元组（word,1）
     val wordAndOneStreams = wordStreams.map((_, 1))
     //将相同的单词次数做统计
     val wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_)
     //打印
     wordAndCountStreams.print()
     //启动 SparkStreamingContext
     ssc.start()
     ssc.awaitTermination()
     &#125;
    &#125;  
</code></pre>
<h4 id="WordCount-解析"><a href="#WordCount-解析" class="headerlink" title="WordCount 解析"></a>WordCount 解析</h4><p>在内部实现上，DStream 是一系列连续的 RDD 来表示。每个 RDD 含有一段时间间隔内的数据。</p>
<p><img src="/2023/08/11/Spark-Streaming/3.png" alt="DStream">   </p>
<p>对数据的操作也是按照 RDD 为单位来进行的  </p>
<p><img src="/2023/08/11/Spark-Streaming/4.png" alt="DStream对数据的操作">     </p>
<p>计算过程由 Spark Engine 来完成  </p>
<p><img src="/2023/08/11/Spark-Streaming/5.png" alt="DStream的计算过程"> </p>
<h2 id="DStream创建"><a href="#DStream创建" class="headerlink" title="DStream创建"></a>DStream创建</h2><h3 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h3><h4 id="Kafka-0-10-Direct-模式"><a href="#Kafka-0-10-Direct-模式" class="headerlink" title="Kafka 0-10 Direct 模式"></a>Kafka 0-10 Direct 模式</h4><h5 id="需求：通过-SparkStreaming-从-Kafka-读取数据，并将读取过来的数据做简单计算，最终打印到控制台。"><a href="#需求：通过-SparkStreaming-从-Kafka-读取数据，并将读取过来的数据做简单计算，最终打印到控制台。" class="headerlink" title="需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。"></a>需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</h5><h5 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h5><pre><code>&lt;dependency&gt;
     &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
     &lt;artifactId&gt;spark-streaming-kafka-0-10_2.12&lt;/artifactId&gt;
     &lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
     &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
     &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;
     &lt;version&gt;2.10.1&lt;/version&gt;
&lt;/dependency&gt;  
</code></pre>
<h5 id="编写代码-1"><a href="#编写代码-1" class="headerlink" title="编写代码"></a>编写代码</h5><pre><code>import org.apache.kafka.clients.consumer.&#123;ConsumerConfig, ConsumerRecord&#125;  
import org.apache.spark.SparkConf  
import org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;  
import org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;  
import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;  
object DirectAPI &#123;
     def main(args: Array[String]): Unit = &#123;
     //1.创建 SparkConf
     val sparkConf: SparkConf = new 
     SparkConf().setAppName(&quot;ReceiverWordCount&quot;).setMaster(&quot;local[*]&quot;)
     //2.创建 StreamingContext
     val ssc = new StreamingContext(sparkConf, Seconds(3))
     //3.定义 Kafka 参数
     val kafkaPara: Map[String, Object] = Map[String, Object](
     ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; &quot;linux1:9092,linux2:9092,linux3:9092&quot;,
     ConsumerConfig.GROUP_ID_CONFIG -&gt; &quot;atguigu&quot;,
     &quot;key.deserializer&quot; -&gt; 
    &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;,
     &quot;value.deserializer&quot; -&gt; 
    &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;
     )
     //4.读取 Kafka 数据创建 DStream
     val kafkaDStream: InputDStream[ConsumerRecord[String, String]] = 
    KafkaUtils.createDirectStream[String, String](ssc,
     LocationStrategies.PreferConsistent,
     ConsumerStrategies.Subscribe[String, String](Set(&quot;atguigu&quot;), kafkaPara))
     //5.将每条消息的 KV 取出
     val valueDStream: DStream[String] = kafkaDStream.map(record =&gt; record.value())
     //6.计算 WordCount
     valueDStream.flatMap(_.split(&quot; &quot;))
     .map((_, 1))
     .reduceByKey(_ + _)
     .print()
     //7.开启任务
     ssc.start()
     ssc.awaitTermination()
     &#125;
    &#125;  
</code></pre>
<h5 id="查看-Kafka-消费进度"><a href="#查看-Kafka-消费进度" class="headerlink" title="查看 Kafka 消费进度"></a>查看 Kafka 消费进度</h5><pre><code>bin/kafka-consumer-groups.sh --describe --bootstrap-server linux1:9092 --group atguigu
</code></pre>
<h2 id="DStream转换"><a href="#DStream转换" class="headerlink" title="DStream转换"></a>DStream转换</h2><p>DStream 上的操作与 RDD 的类似，分为 Transformations（转换）和 Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种 Window 相关的原语。  </p>
<h3 id="无状态转化操作"><a href="#无状态转化操作" class="headerlink" title="无状态转化操作"></a>无状态转化操作</h3><p>无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每<br>一个 RDD。<br>注意，针对键值对的 DStream 转化操作(比如reduceByKey())要添加 import StreamingContext._才能在 Scala 中使用。     </p>
<p><img src="/2023/08/11/Spark-Streaming/6.png" alt="无状态转化操作">   </p>
<p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部<br>是由许多 RDD（批次）组成，且无状态转化操作是分别应用到每个 RDD 上的。  </p>
<p>例如：reduceByKey()会归约每个时间区间中的数据，但不会归约不同区间之间的数据。  </p>
<h4 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h4><p>Transform 允许 DStream 上执行任意的 RDD-to-RDD 函数。即使这些函数并没有在 DStream<br>的 API 中暴露出来，通过该函数可以方便的扩展 Spark API。该函数每一批次调度一次。其实也<br>就是对 DStream 中的 RDD 应用转换。  </p>
<h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>两个流之间的 join 需要两个流的批次大小一致，这样才能做到同时触发计算。计算过程就是对当前批次的两个流中各自的 RDD 进行 join，与两个 RDD 的 join 效果相同。  </p>
<pre><code>import org.apache.spark.SparkConf
import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;
import org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;
object JoinTest &#123;
     def main(args: Array[String]): Unit = &#123;
     //1.创建 SparkConf
     val sparkConf: SparkConf = new 
     SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;JoinTest&quot;)
     //2.创建 StreamingContext
     val ssc = new StreamingContext(sparkConf, Seconds(5))
     //3.从端口获取数据创建流
     val lineDStream1: ReceiverInputDStream[String] = 
     ssc.socketTextStream(&quot;linux1&quot;, 9999)
     val lineDStream2: ReceiverInputDStream[String] = 
     ssc.socketTextStream(&quot;linux2&quot;, 8888)
     //4.将两个流转换为 KV 类型
     val wordToOneDStream: DStream[(String, Int)] = lineDStream1.flatMap(_.split(&quot; &quot;)).map((_, 1))
     val wordToADStream: DStream[(String, String)] = lineDStream2.flatMap(_.split(&quot; &quot;)).map((_, &quot;a&quot;))
     //5.流的 JOIN
     val joinDStream: DStream[(String, (Int, String))] = wordToOneDStream.join(wordToADStream)
     //6.打印
     joinDStream.print()
     //7.启动任务
     ssc.start()
     ssc.awaitTermination()
     &#125;
    &#125;
</code></pre>
<h3 id="有状态转化操作"><a href="#有状态转化操作" class="headerlink" title="有状态转化操作"></a>有状态转化操作</h3><h4 id="UpdateStateByKey"><a href="#UpdateStateByKey" class="headerlink" title="UpdateStateByKey"></a>UpdateStateByKey</h4><p>UpdateStateByKey 原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加 wordcount)。  </p>
<p>updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。      </p>
<p>updateStateByKey 操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功<br>能，需要做下面两步：<br>    1.定义状态，状态可以是一个任意的数据类型。<br>    2.定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更<br>新。  </p>
<h4 id="WindowOperations"><a href="#WindowOperations" class="headerlink" title="WindowOperations"></a>WindowOperations</h4><p>Window Operations 可以设置窗口的大小和滑动窗口的间隔来动态的获取当前 Steaming 的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。  </p>
<p>窗口时长：计算内容的时间范围；  </p>
<p>滑动步长：隔多久触发一次计算。  </p>
<p>注意：这两者都必须为采集周期大小的整数倍。  </p>
<pre><code>object WorldCount &#123;
     def main(args: Array[String]) &#123;
         val conf = new 
         SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;NetworkWordCount&quot;)
         val ssc = new StreamingContext(conf, Seconds(3))
         ssc.checkpoint(&quot;./ck&quot;)
         // Create a DStream that will connect to hostname:port, like localhost:9999
         val lines = ssc.socketTextStream(&quot;linux1&quot;, 9999)
         // Split each line into words
         val words = lines.flatMap(_.split(&quot; &quot;))
         // Count each word in each batch
         val pairs = words.map(word =&gt; (word, 1))
         val wordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b),Seconds(12), Seconds(6))
         // Print the first ten elements of each RDD generated in this DStream to the console
         wordCounts.print()
         ssc.start() // Start the computation
         ssc.awaitTermination() // Wait for the computation to terminate
         &#125;
        &#125;
      
</code></pre>
<p>关于 Window 的操作还有如下方法：  </p>
<pre><code>window(windowLength, slideInterval): 基于对源 DStream 窗化的批次进行计算返回一个新的 Dstream；  

countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；  

reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；  

reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V)对的 DStream 上调用此函数，会返回一个新(K,V)对的 DStream，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值。  

reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的 reduce 值都是通过用前一个窗的 reduce 值来递增计算。  
</code></pre>
<p><img src="/2023/08/11/Spark-Streaming/7.png" alt="可逆的reduce函数">  </p>
<p>countByWindow()和 countByValueAndWindow()作为对数据进行计数操作的简写。countByWindow()返回一个表示每个窗口中元素个数的 DStream，而countByValueAndWindow()返回的 DStream 则包含窗口中每个值的个数。   </p>
<h2 id="DStream-输出"><a href="#DStream-输出" class="headerlink" title="DStream 输出"></a>DStream 输出</h2><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。<br>与 RDD 中的惰性求值类似，如果一个 DStream 及其派生出的 DStream 都没有被执行输出操作，那么这些 DStream 就都不会被求值。如果 StreamingContext 中没有设定输出操作，整个 context 就都不会启动。    </p>
<p>输出操作清单：</p>
<pre><code>print()：在运行流程序的驱动结点上打印 DStream 中每一批次数据的最开始 10 个元素 

saveAsTextFiles(prefix, [suffix])：以 text 文件形式存储这个 DStream 的内容
每一批次的存储文件名基于参数中的 prefix 和 suffix。”prefix-Time_IN_MS[.suffix]”  

saveAsObjectFiles(prefix, [suffix])：以 Java 对象序列化的方式将 Stream 中的数据保存为SequenceFiles . 每一批次的存储文件名基于参数中的为&quot;prefix-TIME_IN_MS[.suffix]&quot;. Python中目前不可用  

saveAsHadoopFiles(prefix, [suffix])：将 Stream 中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为&quot;prefix-TIME_IN_MS[.suffix]&quot;。Python API 中目前不可用  

foreachRDD(func)：这是最通用的输出操作，即将函数 func 用于产生于 stream 的每一个RDD。其中参数传入的函数 func 应该实现将每一个 RDD 中数据推送到外部系统，如将
RDD 存入文件或者通过网络将其写入数据库。  
</code></pre>
<p>通用的输出操作 foreachRDD()，它用来对 DStream 中的 RDD 运行任意计算  </p>
<p>在 foreachRDD()中，可以重用我们在 Spark 中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如 MySQL 的外部数据库中。  </p>
<p>注意：<br>    1) 连接不能写在 driver 层面（序列化）<br>    2) 如果写在 foreach 则每个 RDD 中的每一条数据都创建，得不偿失；<br>    3) 增加 foreachPartition，在分区创建（获取）   </p>
<h2 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h2><p>流式任务需要 7*24 小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。使用外部文件系统来控制内部程序关闭。  </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/11/Spark-SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/11/Spark-SQL/" class="post-title-link" itemprop="url">Spark-SQL</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-08-11 20:19:25 / 修改时间：22:02:53" itemprop="dateCreated datePublished" datetime="2023-08-11T20:19:25+08:00">2023-08-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功"><a href="#无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功" class="headerlink" title="无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功"></a>无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功</h1><h1 id="SparkSQL概述"><a href="#SparkSQL概述" class="headerlink" title="SparkSQL概述"></a>SparkSQL概述</h1><h2 id="SparkSQL是什么？"><a href="#SparkSQL是什么？" class="headerlink" title="SparkSQL是什么？"></a>SparkSQL是什么？</h2><p>Spark SQL是Spark用于结构化数据(structured data)处理的Spark模块。  </p>
<pre><code>➢ 数据兼容方面 SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据；	

➢ 性能优化方面 除了采取 In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等；

➢ 组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展。  
</code></pre>
<p>应用Spark的两个支线：SparkSQL 和 Hive on Spark  </p>
<p>SparkSQL 可以简化 RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。  </p>
<p>Spark SQL 为了简化 RDD 的开发，提高开发效率，提供了2个编程抽象，类似 Spark Core 中的RDD。</p>
<pre><code>➢ DataFrame	
➢ DataSet
</code></pre>
<h2 id="SparkSQL-特点"><a href="#SparkSQL-特点" class="headerlink" title="SparkSQL 特点"></a>SparkSQL 特点</h2><h3 id="易整合"><a href="#易整合" class="headerlink" title="易整合"></a>易整合</h3><p>无缝的整合了 SQL 查询和 Spark 编程  </p>
<h3 id="统一的数据访问"><a href="#统一的数据访问" class="headerlink" title="统一的数据访问"></a>统一的数据访问</h3><p>使用相同的方式连接不同的数据源   </p>
<h3 id="兼容-Hive"><a href="#兼容-Hive" class="headerlink" title="兼容 Hive"></a>兼容 Hive</h3><p>在已有的仓库上直接运行 SQL 或者 HiveQL  </p>
<h3 id="标准数据连接"><a href="#标准数据连接" class="headerlink" title="标准数据连接"></a>标准数据连接</h3><p>通过 JDBC 或者 ODBC 来连接  </p>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。  </p>
<p>DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL得以洞察更多的结构信息，达到大幅提升运行时效率的目标。  </p>
<p>反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在 stage 层面进行简单、通用的流水线优化。  </p>
<p><img src="/2023/08/11/Spark-SQL/1.png" alt="DataFrame和RDD的区别">    </p>
<p>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待。     </p>
<p>DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计划通过 Spark catalyst optimiser 进行优化。  </p>
<p><img src="/2023/08/11/Spark-SQL/2.png" alt="逻辑查询计划优化">     </p>
<p>逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操<br>作的过程。     </p>
<h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>DataSet 是分布式数据集合。  </p>
<p>它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）。  </p>
<pre><code>➢ DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象  
➢ 用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到
DataSet 中的字段名称；  
➢ DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。  
➢ DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。     
</code></pre>
<h2 id="SparkSQL-核心编程"><a href="#SparkSQL-核心编程" class="headerlink" title="SparkSQL 核心编程"></a>SparkSQL 核心编程</h2><p>SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和HiveContext的组合。  </p>
<h3 id="DataFrame-1"><a href="#DataFrame-1" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>Spark SQL 的 DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL表达式。    </p>
<h4 id="创建-DataFrame"><a href="#创建-DataFrame" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h4><p>创建 DataFrame有三种方式：<br>    1.通过 Spark 的数据源进行创建；<br>    2.从一个存在的 RDD 进行转换；<br>    3.从 Hive Table 进行查询返回。   </p>
<h5 id="从-Spark-数据源进行创建"><a href="#从-Spark-数据源进行创建" class="headerlink" title="从 Spark 数据源进行创建"></a>从 Spark 数据源进行创建</h5><pre><code>scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)
df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]
</code></pre>
<h5 id="从一个存在的-RDD-进行转换"><a href="#从一个存在的-RDD-进行转换" class="headerlink" title="从一个存在的 RDD 进行转换"></a>从一个存在的 RDD 进行转换</h5><h5 id="从-Hive-Table-进行查询返回"><a href="#从-Hive-Table-进行查询返回" class="headerlink" title="从 Hive Table 进行查询返回"></a>从 Hive Table 进行查询返回</h5><h3 id="SQL-语法"><a href="#SQL-语法" class="headerlink" title="SQL 语法"></a>SQL 语法</h3><p>SQL 语法风格是指我们查询数据的时候使用 SQL 语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助。    </p>
<ol>
<li><p>读取 JSON 文件创建 DataFrame  </p>
<p> scala&gt; val df &#x3D; spark.read.json(“data&#x2F;user.json”)<br> df: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， username: string]</p>
</li>
<li><p>对 DataFrame 创建一个临时表 </p>
<p> scala&gt; df.createOrReplaceTempView(“people”)</p>
</li>
<li><p>通过 SQL 语句实现查询全表  </p>
<p> scala&gt; val sqlDF &#x3D; spark.sql(“SELECT * FROM people”)<br> sqlDF: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， name: string]</p>
</li>
<li><p>结果展示</p>
<p> scala&gt; sqlDF.show<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|</p>
</li>
</ol>
<p>注意：普通临时表是 Session 范围内的，如果想应用范围内有效，可以使用全局临时表。使<br>用全局临时表时需要全路径访问，如：global_temp.people  </p>
<ol start="5">
<li><p>对于 DataFrame 创建一个全局表  </p>
<p> scala&gt; df.createGlobalTempView(“people”)</p>
</li>
<li><p>通过 SQL 语句实现查询全表  </p>
<p> scala&gt; spark.sql(“SELECT * FROM global_temp.people”).show()<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|<br> +—+——–+<br> scala&gt; spark.newSession().sql(“SELECT * FROM global_temp.people”).show()<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|<br> +—+——–+</p>
</li>
</ol>
<h3 id="DSL-语法"><a href="#DSL-语法" class="headerlink" title="DSL 语法"></a>DSL 语法</h3><p>DataFrame 提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。  </p>
<p>可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了。    </p>
<ol>
<li><p>创建一个 DataFrame  </p>
<p> scala&gt; val df &#x3D; spark.read.json(“data&#x2F;user.json”)<br> df: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， name: string]</p>
</li>
<li><p>查看 DataFrame 的 Schema 信息  </p>
<p> scala&gt; df.printSchema<br> root<br> |– age: Long (nullable &#x3D; true)<br> |– username: string (nullable &#x3D; true)  </p>
</li>
<li><p>只查看”username”列数据  </p>
<p> scala&gt; df.select(“username”).show()<br> +——–+<br> |username|<br> +——–+<br> |zhangsan|<br> | lisi|<br> | wangwu|<br> +——–+  </p>
</li>
<li><p>查看”username”列数据以及”age+1”数据  </p>
<p> 注意:涉及到运算的时候, 每列都必须使用$, 或者采用引号表达式：单引号+字段名<br> scala&gt; df.select($”username”,$”age” + 1).show<br> scala&gt; df.select(‘username, ‘age + 1).show()  </p>
<p> scala&gt; df.select(‘username, ‘age + 1 as “newage”).show()<br> +——–+———+<br> |username|(age + 1)|<br> +——–+———+<br> |zhangsan| 21|<br> | lisi| 31|<br> | wangwu| 41|<br> +——–+———+  </p>
</li>
<li><p>查看”age”大于”30”的数据  </p>
<p> scala&gt; df.filter($”age”&gt;30).show<br> +—+———+<br> |age| username|<br> +—+———+<br> | 40| wangwu|<br> +—+———+  </p>
</li>
<li><p>按照”age”分组，查看数据条数  </p>
<p> scala&gt; df.groupBy(“age”).count.show<br> +—+—–+<br> |age|count|<br> +—+—–+<br> | 20| 1|<br> | 30| 1|<br> | 40| 1|<br> +—+—–+</p>
</li>
</ol>
<h3 id="RDD-转换为-DataFrame"><a href="#RDD-转换为-DataFrame" class="headerlink" title="RDD 转换为 DataFrame"></a>RDD 转换为 DataFrame</h3><p>在 IDEA 中开发程序时，如果需要 RDD 与 DF 或者 DS 之间互相操作，那么需要引入import spark.implicits._   </p>
<p>这里的 spark 不是 Scala 中的包名，而是创建的 sparkSession 对象的变量名称，所以必<br>须先创建 SparkSession 对象再导入。这里的 spark 对象不能使用 var 声明，因为 <strong>Scala 只支持val 修饰的对象的引入</strong>。    </p>
<h4 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h4><pre><code>scala&gt; val idRDD = sc.textFile(&quot;data/id.txt&quot;)
scala&gt; idRDD.toDF(&quot;id&quot;).show
    +---+
    | id|
    +---+
    | 1|
    | 2|
    | 3|
    | 4| 
    +---+  
</code></pre>
<h4 id="通过样例类-RDD-DataFrame"><a href="#通过样例类-RDD-DataFrame" class="headerlink" title="通过样例类 RDD -&gt; DataFrame"></a>通过样例类 RDD -&gt; DataFrame</h4><p>实际开发中，一般通过样例类将 RDD 转换为 DataFrame  </p>
<pre><code>scala&gt; case class User(name:String, age:Int)
defined class User
scala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,40))).map(t=&gt;User(t._1, 
t._2)).toDF.show
    +--------+---+
    | name|age|
    +--------+---+
    |zhangsan| 30|
    | lisi| 40|
    +--------+---+    
</code></pre>
<h3 id="DataFrame-转换为-RDD"><a href="#DataFrame-转换为-RDD" class="headerlink" title="DataFrame 转换为 RDD"></a>DataFrame 转换为 RDD</h3><p>DataFrame 其实就是对 RDD 的封装，所以可以直接获取内部的 RDD  </p>
<h4 id="df-rdd"><a href="#df-rdd" class="headerlink" title="df.rdd"></a>df.rdd</h4><pre><code>scala&gt; val rdd = df.rdd
rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] at rdd at &lt;console&gt;:25  
</code></pre>
<p>注意：此时得到的 RDD 存储类型为 Row   </p>
<h3 id="DataSet-1"><a href="#DataSet-1" class="headerlink" title="DataSet"></a>DataSet</h3><p>DataSet 是具有强类型的数据集合，需要提供对应的类型信息。  </p>
<h4 id="创建-DataSet"><a href="#创建-DataSet" class="headerlink" title="创建 DataSet"></a>创建 DataSet</h4><h5 id="使用样例类序列创建-DataSet"><a href="#使用样例类序列创建-DataSet" class="headerlink" title="使用样例类序列创建 DataSet"></a>使用样例类序列创建 DataSet</h5><pre><code>scala&gt; case class Person(name: String, age: Long)
defined class Person
scala&gt; val caseClassDS = Seq(Person(&quot;zhangsan&quot;,2)).toDS()
caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]
scala&gt; caseClassDS.show
    +---------+---+
    | name|age|
    +---------+---+
    | zhangsan| 2|
    +---------+---+    
</code></pre>
<h5 id="使用基本类型的序列创建-DataSet"><a href="#使用基本类型的序列创建-DataSet" class="headerlink" title="使用基本类型的序列创建 DataSet"></a>使用基本类型的序列创建 DataSet</h5><pre><code>scala&gt; val ds = Seq(1,2,3,4,5).toDS
ds: org.apache.spark.sql.Dataset[Int] = [value: int]  

    scala&gt; ds.show
    +-----+
    |value|
    +-----+
    | 1|
    | 2|
    | 3|
    | 4|
    | 5|
    +-----+    
</code></pre>
<p>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet  </p>
<h3 id="RDD-转换为-DataSet"><a href="#RDD-转换为-DataSet" class="headerlink" title="RDD 转换为 DataSet"></a>RDD 转换为 DataSet</h3><p>SparkSQL 能够自动将包含有 case 类的 RDD 转换成 DataSet，case 类定义了 table 的结构，case 类属性通过反射变成了表的列名。Case 类可以包含诸如 Seq 或者 Array 等复杂的结构。  </p>
<h4 id="toDS"><a href="#toDS" class="headerlink" title="toDS"></a>toDS</h4><pre><code>scala&gt; case class User(name:String, age:Int)
defined class User
scala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).map(t=&gt;User(t._1, t._2)).toDS
res11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
</code></pre>
<h3 id="DataSet-转换为-RDD"><a href="#DataSet-转换为-RDD" class="headerlink" title="DataSet 转换为 RDD"></a>DataSet 转换为 RDD</h3><p>DataSet 其实也是对 RDD 的封装，所以可以直接获取内部的 RDD    </p>
<h4 id="ds-rdd"><a href="#ds-rdd" class="headerlink" title="ds.rdd"></a>ds.rdd</h4><pre><code>scala&gt; val rdd = res11.rdd
rdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at &lt;console&gt;:25
</code></pre>
<h3 id="DataFrame-和-DataSet-转换"><a href="#DataFrame-和-DataSet-转换" class="headerlink" title="DataFrame 和 DataSet 转换"></a>DataFrame 和 DataSet 转换</h3><p>DataFrame 其实是 DataSet 的特例，所以它们之间是可以互相转换的。  </p>
<h4 id="DataFrame-转换为-DataSet"><a href="#DataFrame-转换为-DataSet" class="headerlink" title="DataFrame 转换为 DataSet"></a>DataFrame 转换为 DataSet</h4><h5 id="df-as-样例类"><a href="#df-as-样例类" class="headerlink" title="df.as[样例类]"></a>df.as[样例类]</h5><pre><code>scala&gt; case class User(name:String, age:Int)
defined class User  

scala&gt; val df = sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).toDF(&quot;name&quot;,&quot;age&quot;)
df: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala&gt; val ds = df.as[User]
ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]  
</code></pre>
<h4 id="DataSet-转换为-DataFrame"><a href="#DataSet-转换为-DataFrame" class="headerlink" title="DataSet 转换为 DataFrame"></a>DataSet 转换为 DataFrame</h4><h5 id="ds-toDF"><a href="#ds-toDF" class="headerlink" title="ds.toDF"></a>ds.toDF</h5><pre><code>scala&gt; val ds = df.as[User]
ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]

scala&gt; val df = ds.toDF
df: org.apache.spark.sql.DataFrame = [name: string, age: int]  
</code></pre>
<h3 id="RDD、DataFrame、DataSet-三者的关系"><a href="#RDD、DataFrame、DataSet-三者的关系" class="headerlink" title="RDD、DataFrame、DataSet 三者的关系"></a>RDD、DataFrame、DataSet 三者的关系</h3><p>同样的数据都给到这三个数据结构,计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。    </p>
<h4 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h4><p>都是 spark 平台下的分布式弹性数据集。  </p>
<p>都有惰性机制。  </p>
<p>三者有许多共同的函数，如 filter，排序等。  </p>
<p>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）  </p>
<p>三者都会根据 Spark 的内存情况自动缓存运算。  </p>
<p>三者都有 partition 的概念  </p>
<p>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型   </p>
<h4 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h4><p>RDD 不支持 sparksql 操作。  </p>
<p>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直<br>接访问，只有通过解析才能获取各个字段的值。  </p>
<p>DataFrame 与 DataSet 一般不与 spark mllib 同时使用。  </p>
<p>DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能<br>注册临时表&#x2F;视窗，进行 sql 语句操作。  </p>
<p>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然。    </p>
<p>DataFrame 其实就是 DataSet 的一个特例 type DataFrame &#x3D; Dataset[Row]  </p>
<p>DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了case class 之后可以很自由的获得每一行的信息。      </p>
<p><img src="/2023/08/11/Spark-SQL/3.png" alt="三者的相互转换">  </p>
<h2 id="IDEA开发SparkSQL"><a href="#IDEA开发SparkSQL" class="headerlink" title="IDEA开发SparkSQL"></a>IDEA开发SparkSQL</h2><p>实际开发中，都是使用 IDEA 进行开发的。   </p>
<h3 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h3><pre><code>&lt;dependency&gt;
 &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
 &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;
 &lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>object SparkSQL01_Demo &#123;
 	def main(args: Array[String]): Unit = &#123;
     	//创建上下文环境配置对象
     	val conf: SparkConf = new 
        SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL01_Demo&quot;)
     	//创建 SparkSession 对象
         val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
         //RDD=&gt;DataFrame=&gt;DataSet 转换需要引入隐式转换规则，否则无法转换
         //spark 不是包名，是上下文环境对象名
         import spark.implicits._
         //读取 json 文件 创建 DataFrame &#123;&quot;username&quot;: &quot;lisi&quot;,&quot;age&quot;: 18&#125;
         val df: DataFrame = spark.read.json(&quot;input/test.json&quot;)
         //df.show()
         //SQL 风格语法
         df.createOrReplaceTempView(&quot;user&quot;)
         //spark.sql(&quot;select avg(age) from user&quot;).show
         //DSL 风格语法
         //df.select(&quot;username&quot;,&quot;age&quot;).show()
         //*****RDD=&gt;DataFrame=&gt;DataSet*****
         //RDD
         val rdd1: RDD[(Int, String, Int)] = 
         spark.sparkContext.makeRDD(List((1,&quot;zhangsan&quot;,30),(2,&quot;lisi&quot;,28),(3,&quot;wangwu&quot;,20)))
         //DataFrame
         val df1: DataFrame = rdd1.toDF(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;)
         //df1.show()
         //DateSet
         val ds1: Dataset[User] = df1.as[User]
         //ds1.show()
         //*****DataSet=&gt;DataFrame=&gt;RDD*****
         //DataFrame
         val df2: DataFrame = ds1.toDF()
         //RDD 返回的 RDD 类型为 Row，里面提供的 getXXX 方法可以获取字段值，类似 jdbc 处理结果集，
        但是索引从 0 开始
         val rdd2: RDD[Row] = df2.rdd
         //rdd2.foreach(a=&gt;println(a.getString(1)))
         //*****RDD=&gt;DataSet*****
         rdd1.map&#123;  
        case (id,name,age)=&gt;User(id,name,age)
         &#125;.toDS()
         //*****DataSet=&gt;=&gt;RDD*****
         ds1.rdd
         //释放资源
         spark.stop()
     &#125;
    &#125;
case class User(id:Int,name:String,age:Int)  
</code></pre>
<h3 id="toDF和toDS的用法区别："><a href="#toDF和toDS的用法区别：" class="headerlink" title="toDF和toDS的用法区别："></a>toDF和toDS的用法区别：</h3><p>使用toDF时：  </p>
<pre><code>rdd1.toDF(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;)  指定字段名即可，字段类型会自动解析rdd中的数据进行获取。 
</code></pre>
<p>使用toDS时：  </p>
<pre><code>case class User(id:Int,name:String,age:Int)   
rdd1.toDS  
</code></pre>
<h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><h4 id="创建-DataFrame-1"><a href="#创建-DataFrame-1" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h4><pre><code>scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)
df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]
</code></pre>
<h4 id="注册-UDF"><a href="#注册-UDF" class="headerlink" title="注册 UDF"></a>注册 UDF</h4><pre><code>scala&gt; spark.udf.register(&quot;addName&quot;,(x:String)=&gt; &quot;Name:&quot;+x)
res9: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))
</code></pre>
<h4 id="创建临时表"><a href="#创建临时表" class="headerlink" title="创建临时表"></a>创建临时表</h4><pre><code>scala&gt; df.createOrReplaceTempView(&quot;people&quot;)
</code></pre>
<h4 id="应用-UDF"><a href="#应用-UDF" class="headerlink" title="应用 UDF"></a>应用 UDF</h4><pre><code>scala&gt; spark.sql(&quot;Select addName(name),age from people&quot;).show()   
</code></pre>
<h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><p>用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从 Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以统一采用强类型聚合函数Aggregator。  </p>
<h2 id="数据的加载和保存"><a href="#数据的加载和保存" class="headerlink" title="数据的加载和保存"></a>数据的加载和保存</h2><p>SparkSQL 默认读取和保存的文件格式为 parquet。  </p>
<h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>如果读取不同格式的数据，可以对不同的数据格式进行设定  </p>
<pre><code>scala&gt; spark.read.format(&quot;…&quot;)[.option(&quot;…&quot;)].load(&quot;…&quot;)  

➢ format(&quot;…&quot;)：指定加载的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。
➢ load(&quot;…&quot;)：在&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入加载数据的路径。
➢ option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable     
</code></pre>
<p>也可以直接在文件上进行查询: 文件格式.<code>文件路径</code></p>
<pre><code>scala&gt;spark.sql(&quot;select * from json.`/opt/module/data/user.json`&quot;).show  
</code></pre>
<h3 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h3><p>df.write.save 是保存数据的通用方法  </p>
<pre><code>scala&gt;df.write.format(&quot;…&quot;)[.option(&quot;…&quot;)].save(&quot;…&quot;)  

➢ format(&quot;…&quot;)：指定保存的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。
➢ save (&quot;…&quot;)：在&quot;csv&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入保存数据的路径。
➢ option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable  
</code></pre>
<p>保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。  </p>
<pre><code>df.write.mode(&quot;append&quot;).json(&quot;/opt/module/data/output&quot;)   
</code></pre>
<p><img src="/2023/08/11/Spark-SQL/4.png" alt="SaveMode枚举值">    </p>
<h3 id="修改默认数据源格式"><a href="#修改默认数据源格式" class="headerlink" title="修改默认数据源格式"></a>修改默认数据源格式</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式<br>存储格式。</p>
<p>修改配置项 spark.sql.sources.default，可修改默认数据源格式。   </p>
<p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以<br>通过 SparkSession.read.json()去加载 JSON 文件。  </p>
<p>注意：Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串 </p>
<pre><code>&#123;&quot;name&quot;:&quot;Michael&quot;&#125;
&#123;&quot;name&quot;:&quot;Andy&quot;， &quot;age&quot;:30&#125;
[&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;,&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;]  
</code></pre>
<h3 id="Spark读取本地Json文件的案例"><a href="#Spark读取本地Json文件的案例" class="headerlink" title="Spark读取本地Json文件的案例"></a>Spark读取本地Json文件的案例</h3><h4 id="导入隐式转换"><a href="#导入隐式转换" class="headerlink" title="导入隐式转换"></a>导入隐式转换</h4><pre><code>import spark.implicits._
</code></pre>
<h4 id="加载-JSON-文件"><a href="#加载-JSON-文件" class="headerlink" title="加载 JSON 文件"></a>加载 JSON 文件</h4><pre><code>val path = &quot;/opt/module/spark-local/people.json&quot;
val peopleDF = spark.read.json(path)  
</code></pre>
<h4 id="创建临时表-1"><a href="#创建临时表-1" class="headerlink" title="创建临时表"></a>创建临时表</h4><pre><code>peopleDF.createOrReplaceTempView(&quot;people&quot;)
</code></pre>
<h4 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h4><pre><code>val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)  
teenagerNamesDF.show()
    +------+
    | name|
    +------+
    |Justin|
    +------+  
</code></pre>
<h3 id="Spark读取本地CSV文件的案例"><a href="#Spark读取本地CSV文件的案例" class="headerlink" title="Spark读取本地CSV文件的案例"></a>Spark读取本地CSV文件的案例</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为<br>数据列。  </p>
<pre><code>spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;;&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).load(&quot;data/user.csv&quot;)  
</code></pre>
<h3 id="Spark通过JDBC连接Mysql的案例"><a href="#Spark通过JDBC连接Mysql的案例" class="headerlink" title="Spark通过JDBC连接Mysql的案例"></a>Spark通过JDBC连接Mysql的案例</h3><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对<br>DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用 spark-shell 操作，可在启动 shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下。  </p>
<pre><code>bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar  
</code></pre>
<p>在 Idea 中通过 JDBC 对 Mysql 进行操作的案例代码如下  </p>
<h4 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h4><pre><code>&lt;dependency&gt;
 	&lt;groupId&gt;mysql&lt;/groupId&gt;
 	&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
 	&lt;version&gt;5.1.27&lt;/version&gt;
&lt;/dependency&gt;  
</code></pre>
<h4 id="读取数据-（选用方式一）"><a href="#读取数据-（选用方式一）" class="headerlink" title="读取数据 （选用方式一）"></a>读取数据 （选用方式一）</h4><pre><code>val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)
//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
import spark.implicits._      

//方式 1：通用的 load 方法读取
spark.read.format(&quot;jdbc&quot;)
.option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;)
.option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)
.option(&quot;user&quot;, &quot;root&quot;)
.option(&quot;password&quot;, &quot;123123&quot;)
.option(&quot;dbtable&quot;, &quot;user&quot;)
.load().show  


//方式 2:通用的 load 方法读取 参数另一种形式
spark.read.format(&quot;jdbc&quot;)
    .options(Map(&quot;url&quot;-&gt;&quot;jdbc:mysql://linux1:3306/spark-sql?user=root&amp;password=123123&quot;,
    &quot;dbtable&quot;-&gt;&quot;user&quot;,&quot;driver&quot;-&gt;&quot;com.mysql.jdbc.Driver&quot;)).load().show

//方式 3:使用 jdbc 方法读取
val props: Properties = new Properties()
props.setProperty(&quot;user&quot;, &quot;root&quot;)
props.setProperty(&quot;password&quot;, &quot;123123&quot;)
val df: DataFrame = spark.read.jdbc(&quot;jdbc:mysql://linux1:3306/spark-sql&quot;, 
&quot;user&quot;, props)
df.show  

//释放资源
spark.stop()    
</code></pre>
<h4 id="写入数据-选用方式一"><a href="#写入数据-选用方式一" class="headerlink" title="写入数据  (选用方式一)"></a>写入数据  (选用方式一)</h4><pre><code>case class User2(name: String, age: Long)
。。。
val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)
//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()  
import spark.implicits._    

val rdd: RDD[User2] = spark.sparkContext.makeRDD(List(User2(&quot;lisi&quot;, 20), User2(&quot;zs&quot;, 30)))
val ds: Dataset[User2] = rdd.toDS    

//方式 1：通用的方式 format 指定写出类型

ds.write
.format(&quot;jdbc&quot;)
.option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;)
.option(&quot;user&quot;, &quot;root&quot;)
.option(&quot;password&quot;, &quot;123123&quot;)
.option(&quot;dbtable&quot;, &quot;user&quot;)
.mode(SaveMode.Append)
.save()


//方式 2：通过 jdbc 方法
val props: Properties = new Properties()
props.setProperty(&quot;user&quot;, &quot;root&quot;)
props.setProperty(&quot;password&quot;, &quot;123123&quot;)
ds.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql://linux1:3306/spark-sql&quot;, &quot;user&quot;, props)    

//释放资源  
spark.stop() 
</code></pre>
<h4 id="使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作"><a href="#使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作" class="headerlink" title="使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作"></a>使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作</h4><pre><code>val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)
//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
import spark.implicits._      

//方式 1：通用的 load 方法读取
res1 = spark.read.format(&quot;jdbc&quot;)
.option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;)
.option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)
.option(&quot;user&quot;, &quot;root&quot;)
.option(&quot;password&quot;, &quot;123123&quot;)
.option(&quot;dbtable&quot;, &quot;user&quot;)
.load()     

res2 = spark.sql(&quot;select * from user1 where age &gt; 10&quot;)

res2.write
.format(&quot;jdbc&quot;)
.option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;)
.option(&quot;user&quot;, &quot;root&quot;)
.option(&quot;password&quot;, &quot;123123&quot;)
.option(&quot;dbtable&quot;, &quot;user&quot;)
.mode(SaveMode.Append)
.save() 


//释放资源  
spark.stop()   
</code></pre>
<h3 id="Spark操作Hive"><a href="#Spark操作Hive" class="headerlink" title="Spark操作Hive"></a>Spark操作Hive</h3><p>若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到<br>Spark 的配置文件目录中($SPARK_HOME&#x2F;conf)。   </p>
<p>需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 &#x2F;user&#x2F;hive&#x2F;warehouse 目录中(如果你的 classpath 中有配好的hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。   </p>
<p>spark-shell 默认是 Hive 支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。    </p>
<p>在实际使用中, 几乎没有任何人会使用内置的 Hive    </p>
<h4 id="Spark访问外部Hive的前置条件"><a href="#Spark访问外部Hive的前置条件" class="headerlink" title="Spark访问外部Hive的前置条件"></a>Spark访问外部Hive的前置条件</h4><p>如果想连接外部已经部署好的 Hive，需要通过以下几个步骤：  </p>
<pre><code>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下
➢ 把 Mysql 的驱动 copy 到 jars/目录下
➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下
➢ 重启 spark-shell   
</code></pre>
<h4 id="Spark-shell中访问Hive"><a href="#Spark-shell中访问Hive" class="headerlink" title="Spark-shell中访问Hive"></a>Spark-shell中访问Hive</h4><pre><code>scala&gt; spark.sql(&quot;show tables&quot;).show   
</code></pre>
<h4 id="运行-Spark-SQL-CLI"><a href="#运行-Spark-SQL-CLI" class="headerlink" title="运行 Spark SQL CLI"></a>运行 Spark SQL CLI</h4><p>Spark SQL CLI 可以很方便的在本地运行 Hive 元数据服务以及从命令行执行查询任务。在Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一 Hive 窗口    </p>
<pre><code>bin/spark-sql    
</code></pre>
<h4 id="运行-Spark-beeline"><a href="#运行-Spark-beeline" class="headerlink" title="运行 Spark beeline"></a>运行 Spark beeline</h4><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。Spark Thrift Server 的目的也只是取代HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。<br>如果想连接 Thrift Server，需要通过以下几个步骤：  </p>
<pre><code>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下	
➢ 把 Mysql 的驱动 copy 到 jars/目录下
➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下
➢ 启动 Thrift Server    

sbin/start-thriftserver.sh      
</code></pre>
<h5 id="使用-beeline-连接-Thrift-Server"><a href="#使用-beeline-连接-Thrift-Server" class="headerlink" title="使用 beeline 连接 Thrift Server"></a>使用 beeline 连接 Thrift Server</h5><pre><code>bin/beeline -u jdbc:hive2://linux1:10000 -n root  
</code></pre>
<h4 id="Spark操作Hive的代码示例"><a href="#Spark操作Hive的代码示例" class="headerlink" title="Spark操作Hive的代码示例"></a>Spark操作Hive的代码示例</h4><h5 id="导入依赖-1"><a href="#导入依赖-1" class="headerlink" title="导入依赖"></a>导入依赖</h5><pre><code>&lt;dependency&gt;
     &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
     &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;
     &lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
     &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
     &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
     &lt;version&gt;1.2.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
     &lt;groupId&gt;mysql&lt;/groupId&gt;
     &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
     &lt;version&gt;5.1.27&lt;/version&gt;
&lt;/dependency&gt;  
</code></pre>
<h5 id="拷贝Hive-Site-xml"><a href="#拷贝Hive-Site-xml" class="headerlink" title="拷贝Hive-Site.xml"></a>拷贝Hive-Site.xml</h5><p>将 hive-site.xml 文件拷贝到项目的 resources 目录中</p>
<h5 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h5><pre><code>//创建 SparkSession
val spark: SparkSession = SparkSession
.builder()
.enableHiveSupport()
.master(&quot;local[*]&quot;)	
.appName(&quot;sql&quot;)
.getOrCreate()  
</code></pre>
<p>在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址:<br>config(“spark.sql.warehouse.dir”, “hdfs:&#x2F;&#x2F;linux1:8020&#x2F;user&#x2F;hive&#x2F;warehouse”)</p>
<p>代码最前面增加如下代码解决权限不足的问题：  </p>
<p>System.setProperty(“HADOOP_USER_NAME”, “root”)</p>
<p>此处的 root 改为你们自己的 hadoop 用户名称      </p>
<h5 id="整理后代码实现"><a href="#整理后代码实现" class="headerlink" title="整理后代码实现"></a>整理后代码实现</h5><pre><code>//创建 SparkSession
System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)
val spark: SparkSession = SparkSession
.builder()
.config(&quot;spark.sql.warehouse.dir&quot;, &quot;hdfs://linux1:8020/user/hive/warehouse&quot;)
.enableHiveSupport()
.master(&quot;local[*]&quot;)	
.appName(&quot;show databases&quot;)
.getOrCreate() 
</code></pre>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/07/Java_datastrcut/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/07/Java_datastrcut/" class="post-title-link" itemprop="url">Java数据结构和算法</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-07 11:10:06" itemprop="dateCreated datePublished" datetime="2023-08-07T11:10:06+08:00">2023-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:02:31" itemprop="dateModified" datetime="2023-08-11T13:02:31+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="君子藏器于身，待时而动"><a href="#君子藏器于身，待时而动" class="headerlink" title="君子藏器于身，待时而动"></a>君子藏器于身，待时而动</h1><h2 id="线性结构和非线性结构"><a href="#线性结构和非线性结构" class="headerlink" title="线性结构和非线性结构"></a>线性结构和非线性结构</h2><h3 id="线性结构"><a href="#线性结构" class="headerlink" title="线性结构"></a>线性结构</h3><p>线性结构作为最常用的数据结构，其特点是数据元素之间存在一对一的线性关系  </p>
<p>线性结构有两种不同的存储结构，即顺序存储结构和链式存储结构。顺序存储的线性表称为顺序表，顺序表中的存储元素是连续的  </p>
<p>链式存储的线性表称为链表，链表中的存储元素不一定是连续的，元素节点中存放数据元素以及相邻元素的地址信息  </p>
<p>线性结构常见的有：数组、队列、链表和栈，后面我们会详细讲解   </p>
<p>非线性结构包括：二维数组，多维数组，广义表，树结构，图结构  </p>
<h1 id="稀疏数组"><a href="#稀疏数组" class="headerlink" title="稀疏数组"></a>稀疏数组</h1><p>当一个数组中大部分元素为０，或者为同一个值的数组时，可以使用稀疏数组来保存该数组。  </p>
<p>稀疏数组的处理方法是:  </p>
<p>记录数组一共有几行几列，有多少个不同的值  </p>
<p>把具有不同值的元素的行列及值记录在一个小规模的数组中，从而缩小程序的规模  </p>
<p><img src="/2023/08/07/Java_datastrcut/1.png" alt="稀疏数组">   </p>
<p><img src="/2023/08/07/Java_datastrcut/2.png" alt="稀疏数组转换思路">    </p>
<p>二维表转稀疏数组代码实现：    </p>
<pre><code>package com.zyy;

public class SparseArray &#123;

    public static void main(String[] args)&#123;
        //创建一个原始的二维数组 11*11
        // 0: 表示没有棋子，1表示黑子 2表示蓝子
        int chessArr1[][] = new int[11][11];
        chessArr1[1][2] = 1; //第二行第三列 有一颗黑子
        chessArr1[2][3] = 2; //第三行第四列 有一颗蓝子
        chessArr1[4][5] = 2; //第五行第六列 有一颗蓝子
        //输出原始的二维数组
        System.out.println(&quot;原始的二维数组~~&quot;);
        //从二维数组中拿出每一行数据，返回为一维数组int[] row
        for(int[] row:chessArr1) &#123;
            //从拿到的每一行数据中拿到每一个值
            for (int data:row)&#123;
                System.out.printf(&quot;%d\t&quot;,data);
            &#125;
            System.out.println();
        &#125;

        // 将二维数组转稀疏数组的思想
        //1.先遍历二维数组，得到非0数据的个数
        int sum = 0;
        for (int i = 0; i &lt; 11; i++)&#123;
            for (int j = 0;j &lt; 11; j++)&#123;
                if (chessArr1[i][j] != 0 )&#123;
                    sum++;
                &#125;
            &#125;
        &#125;

        //2.创建对应的稀疏数组
        //由统计出来的非0数个数+1，构成稀疏数组的行数
        //稀疏数组的列数固定为3，记录行坐标，列坐标，值
        int sparseArr[][] = new int[sum+1][3];
        //给稀疏数组赋值
        sparseArr[0][0] = 11;
        sparseArr[0][1] = 11;
        sparseArr[0][2] = sum;
        //遍历二维数组，将非0的值存放到sparseArr中
        int count = 0;  //count用于记录是第几个非0数据
        for(int i = 0;i &lt; 11; i++)&#123;
            for (int j = 0 ;j &lt; 11; j++)&#123;
                if(chessArr1[i][j] != 0 )&#123;
                    count++;
                    //记录第count个非0数据的行i
                    sparseArr[count][0] = i;
                    //记录第count个非0数据的列j
                    sparseArr[count][1] = j;
                    //记录第count个非0数据的值
                    sparseArr[count][2] = chessArr1[i][j];
                &#125;
            &#125;
        &#125;

        //输出稀疏数组的形式
        System.out.println();
        System.out.println(&quot;得到稀疏数组为~~~&quot;);
        //二维数组结构为[[数组1],[数组2],[数组3]]
        //所以sparseArr.length实际上是在统计外层一维数组的长度
        for(int i = 0; i &lt; sparseArr.length;i++)&#123;
            System.out.printf(&quot;%d\t%d\t%d\t\n&quot;,sparseArr[i][0],sparseArr[i][1],sparseArr[i][2]);
        &#125;

        System.out.println();

        //将稀疏数组 -》 恢复成 原始的二维数组

        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组，比如上面的chessArr2 = int[11][11]
        //2.在读取稀疏数组后几行的数据，并赋值给原始的二维数组，即可

        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组

        int chessArr2[][] = new int[sparseArr[0][0]][sparseArr[0][1]];

        //2.在读取稀疏数组后几行的数据（从第二行开始），并赋值给原始的二维数组即可
        for (int i = 1; i &lt; sparseArr.length; i++)&#123;
            chessArr2[sparseArr[i][0]][sparseArr[i][1]] = sparseArr[i][2];
        &#125;

        //输出恢复后的二维数组
        System.out.println();
        System.out.println(&quot;恢复后的二维数组&quot;);

        for (int[] row:chessArr2)&#123;
            for(int data:row)&#123;
                System.out.printf(&quot;%d\t&quot;,data);
            &#125;
            //每行数据打印完之后，执行换行
            System.out.println();
        &#125;
    &#125;
&#125;


原始的二维数组~~
0	0	0	0	0	0	0	0	0	0	0	
0	0	1	0	0	0	0	0	0	0	0	
0	0	0	2	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	2	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	

得到稀疏数组为~~~
11	11	3	
1	2	1	
2	3	2	
4	5	2	


恢复后的二维数组
0	0	0	0	0	0	0	0	0	0	0	
0	0	1	0	0	0	0	0	0	0	0	
0	0	0	2	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	2	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
</code></pre>
<h1 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h1><p>队列是一个有序列表，可以用数组或者链表来实现。遵循先进先出原则（FIFO）</p>
<p><img src="/2023/08/07/Java_datastrcut/3.png" alt="数组模拟队列">  </p>
<h2 id="数组模拟队列"><a href="#数组模拟队列" class="headerlink" title="数组模拟队列"></a>数组模拟队列</h2><pre><code>package com.zyy;
import java.util.Scanner;

public class ArrayQueueDemo &#123;

public static void main(String[] args) &#123;

    // 测试一把
    // 创建一个队列
    ArrayQueue queue = new ArrayQueue(5);
    char key ;//接收用户输入
    Scanner scanner = new Scanner(System.in);
    boolean loop = true;
    //输出一个菜单
    while(loop)&#123;
        System.out.println(&quot;s(show):显示队列&quot;);
        System.out.println(&quot;e(exit):退出程序&quot;);
        System.out.println(&quot;a(add):添加数据到队列&quot;);
        System.out.println(&quot;g(get):从队列取出数据&quot;);
        System.out.println(&quot;h(head):查看队列头的数据&quot;);
        key = scanner.next().charAt(0);//接收一个字符
        switch(key)&#123;
            case &#39;s&#39;:
                queue.showQueue();
                break;
            case &#39;a&#39;:
                System.out.println(&quot;输入一个数&quot;);
                int value = scanner.nextInt();
                queue.addQueue(value);
                break;
            case &#39;g&#39;://取出数据
                try&#123;
                    int res = queue.getQueue();
                    System.out.printf(&quot;取出的数据是%d\n&quot;,res);
                &#125;catch (Exception e)&#123;
                    //TODO:handle exception
                    System.out.println(e.getMessage());
                &#125;
                break;
            case &#39;h&#39;://查看队列头的数据
                try&#123;
                    int res = queue.headQueue();
                    System.out.printf(&quot;队列头的数据是%d\n&quot;,res);
                &#125;catch(Exception e)&#123;
                    //TODO:handle exception
                    System.out.println(e.getMessage());
                &#125;
                break;
            case &#39;e&#39;://退出
                scanner.close();
                loop = false;
                break;
            default:
                break;
        &#125;
    &#125;
    System.out.println(&quot;程序退出~~&quot;);
&#125;

    //使用数组模拟队列-编写一个ArrayQueue类
static class ArrayQueue&#123;
        private int maxSize;// 表示数组的最大容量
        private int front;//队列头
        private int rear;//队列尾
        private int[] arr;//该数据用于存放数据，模拟队列

        //创建队列的构造器
        public ArrayQueue(int arrMaxSize)&#123;
            maxSize = arrMaxSize;
            arr = new int[maxSize];
            front = -1;//指向队列头部，分析出front是指向队列头的前一个位置
            rear = -1;//指向队列尾，指向队列尾的数据（即就是队列最后一个数据）
        &#125;

        //判断队列是否满
        public boolean isFull()&#123;
            return rear == maxSize - 1;
        &#125;

        //判断队列是否为空
        public boolean isEmpty()&#123;
            return rear == front;
        &#125;

        //添加数据到队列
        public void addQueue(int n)&#123;
            //判断队列是否满
            if(isFull())&#123;
                System.out.println(&quot;队列满，不能加入数据~&quot;);
                return;
            &#125;
            rear++;//让rear后移
            arr[rear] = n;
        &#125;

        //获取队列的数据，出队列
        public int getQueue()&#123;
            //判断队列是否为空
            if(isEmpty())&#123;
                //通过抛出异常
                throw new RuntimeException(&quot;队列空，不能取数据&quot;);
            &#125;
            front++;//front后移
            return arr[front];
        &#125;

        //显示队列的所有数据
        public void showQueue()&#123;
            //遍历
            if(isEmpty())&#123;
                System.out.println(&quot;队列空的，没有数据~~&quot;);
                return;
            &#125;
            for(int i=0;i&lt;arr.length;i++)&#123;
                System.out.printf(&quot;arr[%d]=%d\n&quot;,i,arr[i]);
            &#125;
        &#125;

        //显示队列的头数据，注意不是取出数据
        public int headQueue()&#123;
            //判断
            if(isEmpty())&#123;
                throw new RuntimeException(&quot;队列空的，没有数据~~&quot;);
            &#125;
            return arr[front + 1];
        &#125;

    &#125;

&#125;
</code></pre>
<h3 id="使用数组模拟队列存在的问题："><a href="#使用数组模拟队列存在的问题：" class="headerlink" title="使用数组模拟队列存在的问题："></a>使用数组模拟队列存在的问题：</h3><p>数组只能使用一次，因为front和rear指针无法再回头指向已经走过的数组位置  </p>
<h3 id="优化方案："><a href="#优化方案：" class="headerlink" title="优化方案："></a>优化方案：</h3><p>通过取模运算，让front和rear指针能循环指向已经走过的数组位置，让数组复用  </p>
<p>分析说明:  </p>
<p>1):尾索引的下一个为头索引时表示队列满，即将队列容量空出一个作为约定，这个在做判断队列满的时候需要注意（rear+1）%maxSize &#x3D;&#x3D; front 满</p>
<p>2):rear &#x3D;&#x3D; front(空)</p>
<p>3):与数组模拟队列不同，数组模拟环形队列时，front指向队列的第一个元素，front的初始值为0 ，rear指向队列的最后一个元素的最后一个位置，因为希望空出一个空间作为约定，rear的初始值为0</p>
<p>4):队列中的有效数据个数计算方法:  </p>
<p>(rear+maxSize-front)%maxSize</p>
<p><img src="/2023/08/07/Java_datastrcut/4.png" alt="循环队列相关判断条件"></p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/06/hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/06/hadoop/" class="post-title-link" itemprop="url">hadoop学习笔记</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-06 10:45:20" itemprop="dateCreated datePublished" datetime="2023-08-06T10:45:20+08:00">2023-08-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:09:37" itemprop="dateModified" datetime="2023-08-11T13:09:37+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="博观而约取，厚积而薄发"><a href="#博观而约取，厚积而薄发" class="headerlink" title="博观而约取，厚积而薄发"></a>博观而约取，厚积而薄发</h1><p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA">https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA</a><br>提取码：mvcs   </p>
<h1 id="免密登录原理"><a href="#免密登录原理" class="headerlink" title="免密登录原理"></a>免密登录原理</h1><p><img src="/2023/08/06/hadoop/4.png" alt="免密登录原理">    </p>
<h1 id="HDFS架构概述"><a href="#HDFS架构概述" class="headerlink" title="HDFS架构概述"></a>HDFS架构概述</h1><p><img src="/2023/08/06/hadoop/1.png" alt="HDFS架构概述">  </p>
<p>HDFS适合一次写入，多次读出的场景，且不支持文件的修改  </p>
<p>HDFS的缺点：仅支持数据append，不支持文件的随机修改  </p>
<p><img src="/2023/08/06/hadoop/5.png" alt="HDFS组成架构">  </p>
<p><img src="/2023/08/06/hadoop/6.png" alt="HDFS组成架构">   </p>
<p>HDFS文件块大小：在Hadoop2.x版本中是128M  </p>
<p>寻址时间为传输时间的1%时，是最佳状态  </p>
<p>HDFS块的大小设置主要取决于磁盘传输速率  </p>
<h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><p><img src="/2023/08/06/hadoop/7.png" alt="HDFS写数据流程">    </p>
<p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据    </p>
<h3 id="HDFS副本节点选择"><a href="#HDFS副本节点选择" class="headerlink" title="HDFS副本节点选择"></a>HDFS副本节点选择</h3><p><img src="/2023/08/06/hadoop/8.png" alt="HDFS副本节点选择">       </p>
<h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><p><img src="/2023/08/06/hadoop/9.png" alt="HDFS读数据流程">    </p>
<h1 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h1><p>NameNode中的元数据是存储在内存中，在内存上维护一个Edits文件，磁盘上维护一个FsImage文件，每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据  </p>
<p>需要定期进行FsImage和Edits的合并，由SecondaryNamenode完成，专门负责FsImage和Edits的合并  </p>
<h2 id="NameNode工作机制"><a href="#NameNode工作机制" class="headerlink" title="NameNode工作机制"></a>NameNode工作机制</h2><p><img src="/2023/08/06/hadoop/10.png" alt="NameNode工作机制">    </p>
<h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录    </p>
<p>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中  </p>
<p><img src="/2023/08/06/hadoop/11.png" alt="集群安全模式">    </p>
<p>（1）bin&#x2F;hdfs dfsadmin -safemode get		（功能描述：查看安全模式状态）  </p>
<p>（2）bin&#x2F;hdfs dfsadmin -safemode enter  	（功能描述：进入安全模式状态）  </p>
<p>（3）bin&#x2F;hdfs dfsadmin -safemode leave	（功能描述：离开安全模式状态）  </p>
<p>（4）bin&#x2F;hdfs dfsadmin -safemode wait	（功能描述：等待安全模式状态）  </p>
<h2 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h2><p><img src="/2023/08/06/hadoop/12.png" alt="DataNode工作机制">   </p>
<h3 id="DataNode如何保证数据完整性？"><a href="#DataNode如何保证数据完整性？" class="headerlink" title="DataNode如何保证数据完整性？"></a>DataNode如何保证数据完整性？</h3><p>1）当DataNode读取Block的时候，它会计算CheckSum。  </p>
<p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。  </p>
<p>3）Client读取其他DataNode上的Block。  </p>
<p>4）DataNode在其文件创建后周期验证CheckSum    </p>
<p><img src="/2023/08/06/hadoop/13.png" alt="DataNode数据完整性">    </p>
<p>HDFS中默认DataNode掉线的超时时长为10分钟+30秒    </p>
<h3 id="DataNode配置多目录"><a href="#DataNode配置多目录" class="headerlink" title="DataNode配置多目录"></a>DataNode配置多目录</h3><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本   </p>
<p>hdfs-site.xml  </p>
<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<h2 id="HDFS-HA故障转移机制"><a href="#HDFS-HA故障转移机制" class="headerlink" title="HDFS-HA故障转移机制"></a>HDFS-HA故障转移机制</h2><p><img src="/2023/08/06/hadoop/14.png" alt="HDFS-HA故障转移机制">   </p>
<h1 id="YARN架构概述"><a href="#YARN架构概述" class="headerlink" title="YARN架构概述"></a>YARN架构概述</h1><p><img src="/2023/08/06/hadoop/2.png" alt="YARN架构概述">    </p>
<p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序  </p>
<p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成</p>
<h2 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h2><p><img src="/2023/08/06/hadoop/25.png" alt="Yarn工作机制">    </p>
<p>工作机制详解  </p>
<pre><code>（1）MR程序提交到客户端所在的节点。  
（2）YarnRunner向ResourceManager申请一个Application。  
（3）RM将该应用程序的资源路径返回给YarnRunner。
（4）该程序将运行所需资源提交到HDFS上。
（5）程序资源提交完毕后，申请运行mrAppMaster。
（6）RM将用户的请求初始化成一个Task。
（7）其中一个NodeManager领取到Task任务。
（8）该NodeManager创建容器Container，并产生MRAppmaster。
（9）Container从HDFS上拷贝资源到本地。
（10）MRAppmaster向RM 申请运行MapTask资源。
（11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。
（12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。
（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。  
（14）ReduceTask向MapTask获取相应分区的数据。
（15）程序运行完毕后，MR会向RM申请注销自己
</code></pre>
<p>总结：程序提交之后，找RM申请Application,告知运行程序需要的资源。RM生成一个资源分配Task任务放进yarn队列。这个Task会随机分配给Nodemanager，NodeManager领取任务后会根据资源要求创建Container容器，开始运行程序，运行完毕后向RM报告，注销资源占用和Task任务。  </p>
<h3 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h3><p>1.先进先出调度器（FIFO）  </p>
<p>2.容量调度器（Capacity Scheduler）  </p>
<p>3．公平调度器（Fair Scheduler）  </p>
<h1 id="MapReduce架构概述"><a href="#MapReduce架构概述" class="headerlink" title="MapReduce架构概述"></a>MapReduce架构概述</h1><p><img src="/2023/08/06/hadoop/3.png" alt="MapReduce架构概述">     </p>
<h2 id="MapReduce核心思想"><a href="#MapReduce核心思想" class="headerlink" title="MapReduce核心思想"></a>MapReduce核心思想</h2><p><img src="/2023/08/06/hadoop/15.png" alt="MapReduce核心思想">   </p>
<p>MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行  </p>
<h3 id="MapTask并行度决定机制"><a href="#MapTask并行度决定机制" class="headerlink" title="MapTask并行度决定机制"></a>MapTask并行度决定机制</h3><p>数据块：Block是HDFS物理上把数据分成一块一块。  </p>
<p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储  </p>
<p><img src="/2023/08/06/hadoop/16.png" alt="MapTask并行度决定机制">  </p>
<h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><p><img src="/2023/08/06/hadoop/17.png" alt="MapReduce工作流程">  </p>
<p><img src="/2023/08/06/hadoop/18.png" alt="MapReduce工作流程">  </p>
<p>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快,默认100M   </p>
<h2 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h2><p><img src="/2023/08/06/hadoop/19.png" alt="Shuffle机制">    </p>
<h2 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h2><p><img src="/2023/08/06/hadoop/20.png" alt="MapTask工作机制">    </p>
<h2 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a>ReduceTask工作机制</h2><p><img src="/2023/08/06/hadoop/21.png" alt="ReduceTask工作机制">  </p>
<p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置  </p>
<h1 id="Hadoop文件压缩"><a href="#Hadoop文件压缩" class="headerlink" title="Hadoop文件压缩"></a>Hadoop文件压缩</h1><p><img src="/2023/08/06/hadoop/22.png" alt="Hadoop文件压缩">  </p>
<h3 id="Hadoop文件压缩性能比较"><a href="#Hadoop文件压缩性能比较" class="headerlink" title="Hadoop文件压缩性能比较"></a>Hadoop文件压缩性能比较</h3><p><img src="/2023/08/06/hadoop/23.png" alt="Hadoop文件压缩性能比较">  </p>
<p>Gzip压缩：每个文件压缩后都在130M以内的（一个块大小内），都可以考虑Gzip压缩格式  ，不支持Split  </p>
<p>Bzip压缩：支持Split，压缩率很高，但压缩&#x2F;解压缩 速度慢  </p>
<p>LZO压缩：支持Split，合理的压缩率，是Hadoop中最流行的压缩格式    </p>
<p>Snappy压缩：不支持Split，压缩率比Gzip低，但Hadoop本身不支持，需要安装  </p>
<h3 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h3><p><img src="/2023/08/06/hadoop/24.png" alt="压缩位置选择">   </p>
<p>压缩可以在MapReduce作用的任意阶段启用    </p>
<p>速度是最优先考虑的因素，而不是压缩率  </p>
<h1 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h1><p>是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。   </p>
<p>ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功 能稳定的系统提供给用户    </p>
<h2 id="Zookeeper工作机制"><a href="#Zookeeper工作机制" class="headerlink" title="Zookeeper工作机制"></a>Zookeeper工作机制</h2><p><img src="/2023/08/06/hadoop/26.png" alt="Zookeeper工作机制">  </p>
<h3 id="Zookeeper特点"><a href="#Zookeeper特点" class="headerlink" title="Zookeeper特点"></a>Zookeeper特点</h3><p><img src="/2023/08/06/hadoop/27.png" alt="Zookeeper特点">    </p>
<h3 id="Zookeeper的数据结构"><a href="#Zookeeper的数据结构" class="headerlink" title="Zookeeper的数据结构"></a>Zookeeper的数据结构</h3><p><img src="/2023/08/06/hadoop/28.png" alt="Zookeeper的数据结构">    </p>
<h3 id="软负载均衡"><a href="#软负载均衡" class="headerlink" title="软负载均衡"></a>软负载均衡</h3><p>在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求  </p>
<h2 id="选举机制（面试重点）"><a href="#选举机制（面试重点）" class="headerlink" title="选举机制（面试重点）"></a>选举机制（面试重点）</h2><p><img src="/2023/08/06/hadoop/29.png" alt="Zookeeper选举机制-第一次启动">    </p>
<p><img src="/2023/08/06/hadoop/30.png" alt="Zookeeper选举机制-非第一次启动">   </p>
<h3 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h3><p>1）启动客户端    </p>
<pre><code>[atguigu@hadoop102 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop102:2181
</code></pre>
<p>2）显示所有操作命令  </p>
<pre><code>[zk: hadoop102:2181(CONNECTED) 1] help  
</code></pre>
<h4 id="znode-节点数据信息"><a href="#znode-节点数据信息" class="headerlink" title="znode 节点数据信息"></a>znode 节点数据信息</h4><p>1）查看当前znode中所包含的内容   </p>
<pre><code>[zk: hadoop102:2181(CONNECTED) 0] ls /  
</code></pre>
<p>2）查看当前节点详细数据  </p>
<pre><code>[zk: hadoop102:2181(CONNECTED) 5] ls -s /
[zookeeper]cZxid = 0x0
ctime = Thu Jan 01 08:00:00 CST 1970
mZxid = 0x0
mtime = Thu Jan 01 08:00:00 CST 1970
pZxid = 0x0
cversion = -1
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 1    
</code></pre>
<p>（1）czxid：创建节点的事务 zxid  </p>
<p>每次修改 ZooKeeper 状态都会产生一个 ZooKeeper 事务 ID。事务ID是 ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1在zxid2之前发生。  </p>
<p>（2）ctime：znode 被创建的毫秒数（从 1970 年开始）  </p>
<p>（3）mzxid：znode 最后更新的事务 zxid  </p>
<p>（4）mtime：znode 最后修改的毫秒数（从 1970 年开始）  </p>
<p>（5）pZxid：znode 最后更新的子节点 zxid    </p>
<p>（6）cversion：znode 子节点变化号，znode子节点修改次数  </p>
<p>（7）dataversion：znode 数据变化号  </p>
<p>（8）aclVersion：znode 访问控制列表的变化号  </p>
<p>（9）ephemeralOwner：如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。  </p>
<p>（10）dataLength：znode 的数据长度  </p>
<p>（11）numChildren：znode 子节点数量  </p>
<h4 id="节点类型（持久-短暂-有序号-无序号）"><a href="#节点类型（持久-短暂-有序号-无序号）" class="headerlink" title="节点类型（持久&#x2F;短暂&#x2F;有序号&#x2F;无序号）"></a>节点类型（持久&#x2F;短暂&#x2F;有序号&#x2F;无序号）</h4><p><img src="/2023/08/06/hadoop/31.png" alt="Zookeeper节点类型">   </p>
<p>1）分别创建2个普通节点（永久节点 + 不带序号）   </p>
<pre><code>[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;diaochan&quot; 
注意：创建节点时，要赋值  
</code></pre>
<p>2）获得节点的值  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 5] get -s /sanguo  
</code></pre>
<p>3）创建带序号的节点（永久节点 + 带序号）  </p>
<p>（1）先创建一个普通的根节点&#x2F;sanguo&#x2F;weiguo    </p>
<pre><code>[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot;
</code></pre>
<p>（2）创建带序号的节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/zhangliao &quot;zhangliao&quot;  
</code></pre>
<p>如果原来没有序号节点，序号从 0 开始依次递增。如果原节点下已有 2 个节点，则再排序时从 2 开始，以此类推。    </p>
<p>4）创建短暂节点（短暂节点 + 不带序号 or 带序号）   </p>
<p>（1）创建短暂的不带序号的节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot;
</code></pre>
<p>（2）创建短暂的带序号的节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 2] create -e -s /sanguo/wuguo &quot;zhouyu&quot;    
</code></pre>
<p>（3）在当前客户端是能查看到的  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 3] ls /sanguo   
</code></pre>
<p>（4）修改节点数据值   </p>
<pre><code>[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo &quot;simayi&quot;  
</code></pre>
<h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><p>监听机制保证 ZooKeeper 保存的任何的数据的任何改变都能快速的响应到监听了该节点的应用程序。  </p>
<p><img src="/2023/08/06/hadoop/32.png" alt="监听器原理">      </p>
<h4 id="节点删除与查看"><a href="#节点删除与查看" class="headerlink" title="节点删除与查看"></a>节点删除与查看</h4><p>1）删除节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin
</code></pre>
<p>2）递归删除节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 15] deleteall /sanguo/shuguo
</code></pre>
<p>3）查看节点状态  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 17] stat /sanguo
</code></pre>
<h4 id="客户端向服务端写数据流程"><a href="#客户端向服务端写数据流程" class="headerlink" title="客户端向服务端写数据流程"></a>客户端向服务端写数据流程</h4><p><img src="/2023/08/06/hadoop/33.png" alt="客户端向服务端写数据流程">  </p>
<h2 id="ZooKeeper-分布式锁"><a href="#ZooKeeper-分布式锁" class="headerlink" title="ZooKeeper 分布式锁"></a>ZooKeeper 分布式锁</h2><p>“进程1” 在使用该资源的时候，会先去获得锁，保持独占，这样其他进程就无法访问该资源,用完该资源以后就将锁释放掉,保证了分布式系统中多个进程能够有序的访问该临界资源。  </p>
<p><img src="/2023/08/06/hadoop/34.png" alt="Zookeeper分布式锁">      </p>
<h2 id="Zookeeper企业面试真题（面试重点）总结"><a href="#Zookeeper企业面试真题（面试重点）总结" class="headerlink" title="Zookeeper企业面试真题（面试重点）总结"></a>Zookeeper企业面试真题（面试重点）总结</h2><h3 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h3><p>半数机制，超过半数的投票通过，即通过。  </p>
<p>（1）第一次启动选举规则：  </p>
<pre><code>投票过半数时，服务器 id 大的胜出  
</code></pre>
<p>（2）第二次启动选举规则： </p>
<pre><code>①EPOCH 大的直接胜出  
②EPOCH 相同，事务 id 大的胜出  
③事务 id 相同，服务器 id 大的胜出  
</code></pre>
<p>生产集群安装多少 zk 合适？  </p>
<pre><code>安装奇数台  
</code></pre>
<p>生产经验：  </p>
<pre><code>10 台服务器：3 台 zk  
20 台服务器：5 台 zk  
100 台服务器：11 台 zk  
200 台服务器：11 台 zk    
</code></pre>
<p>服务器台数多：好处，提高可靠性；坏处：提高通信延时  </p>
<p>常用命令  </p>
<pre><code>ls、get、create、delete
</code></pre>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/04/JavaSE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/04/JavaSE/" class="post-title-link" itemprop="url">JavaSE</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-04 17:50:53" itemprop="dateCreated datePublished" datetime="2023-08-04T17:50:53+08:00">2023-08-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:02:10" itemprop="dateModified" datetime="2023-08-11T13:02:10+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>天行健，君子以自强不息~  </p>
<h2 id="Java-核心机制-Java-虚拟机-JVM-java-virtual-machine"><a href="#Java-核心机制-Java-虚拟机-JVM-java-virtual-machine" class="headerlink" title="Java 核心机制-Java 虚拟机 [JVM java virtual machine]"></a>Java 核心机制-Java 虚拟机 [JVM java virtual machine]</h2><p>JVM 是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器，包含在JDK中  </p>
<p>对于不同的平台，有不同的虚拟机    </p>
<h2 id="什么是-JDK，JRE"><a href="#什么是-JDK，JRE" class="headerlink" title="什么是 JDK，JRE"></a>什么是 JDK，JRE</h2><h3 id="JDK-JRE-java-的开发工具-java-javac-javadoc-javap-等"><a href="#JDK-JRE-java-的开发工具-java-javac-javadoc-javap-等" class="headerlink" title="JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等]"></a>JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等]</h3><p>JDK 是提供给 Java 开发人员使用的，其中包含了 java 的开发工具，也包括了 JRE。所以安装了 JDK，就不用在单独安装JRE了  </p>
<h3 id="JRE-JVM-Java-的核心类库-类"><a href="#JRE-JVM-Java-的核心类库-类" class="headerlink" title="JRE &#x3D; JVM + Java 的核心类库[类]"></a>JRE &#x3D; JVM + Java 的核心类库[类]</h3><p>包括 Java 虚拟机(JVM Java Virtual Machine)和 Java 程序所需的核心类库等，如果想要运行一个开发好的 Java 程序，计算机中只需要安装 JRE 即可    </p>
<h4 id="Java-转义字符"><a href="#Java-转义字符" class="headerlink" title="Java 转义字符"></a>Java 转义字符</h4><p>\t ：一个制表位，实现对齐的功能  </p>
<p>\n ：换行符  </p>
<p>\ ：一个\  </p>
<p>&quot; :一个”  </p>
<p>&#39; ：一个’ \r :一个回车 System.out.println(“韩顺平教育\r 北京”);  </p>
<h4 id="Java-中的注释类型"><a href="#Java-中的注释类型" class="headerlink" title="Java 中的注释类型"></a>Java 中的注释类型</h4><ol>
<li><p>单行注释 &#x2F;&#x2F;  </p>
</li>
<li><p>多行注释 &#x2F;* *&#x2F;    </p>
<p> 多行注释里面不允许有多行注释嵌套</p>
</li>
<li><p>文档注释 &#x2F;** *&#x2F;</p>
</li>
</ol>
<p><strong>文档注释：</strong>  </p>
<p>&#x2F;**  </p>
<ul>
<li>@author 韩顺平  </li>
<li>@version 1.0<br>*&#x2F;</li>
</ul>
<h4 id="Java-代码规范"><a href="#Java-代码规范" class="headerlink" title="Java 代码规范"></a>Java 代码规范</h4><p><img src="/2023/08/04/JavaSE/1.png" alt="Java代码规范">    </p>
<h3 id="DOS-介绍"><a href="#DOS-介绍" class="headerlink" title="DOS 介绍"></a>DOS 介绍</h3><p>Dos： Disk Operating System 磁盘操作系统  </p>
<h4 id="常用的-dos-命令"><a href="#常用的-dos-命令" class="headerlink" title="常用的 dos 命令"></a>常用的 dos 命令</h4><ol>
<li><p>查看当前目录是有什么内容 dir  </p>
<p> dir dir d:\abc2\test200</p>
</li>
<li><p>切换到其他盘下：盘符号 cd : change directory</p>
</li>
</ol>
<p>案例演示：切换到 c 盘 cd &#x2F;D c:  </p>
<ol start="3">
<li>切换到当前盘的其他目录下 (使用相对路径和绝对路径演示), ..\表示上一级目录</li>
</ol>
<p>案例演示： cd d:\abc2\test200 cd ....\abc2\test200  </p>
<ol start="4">
<li>切换到上一级：</li>
</ol>
<p>案例演示： cd .. 5) 切换到根目录：cd \  </p>
<p>案例演示：cd \  </p>
<ol start="6">
<li><p>查看指定的目录下所有的子级目录 tree  </p>
</li>
<li><p>清屏 cls [苍老师]  </p>
</li>
<li><p>退出 DOS exit</p>
</li>
</ol>
<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p><strong>变量三要素：变量名+值+数据类型</strong>  </p>
<p>变量相当于内存中一个数据存储空间的表示，你可以把变量看做是一个房间的门牌号，通过门牌号我们可以找到房间，而通过变量名可以访问到变量(值)    </p>
<ol>
<li><p>声明变量  </p>
<p> int a;  </p>
</li>
<li><p>赋值  </p>
<p> a &#x3D; 60; &#x2F;&#x2F;应该这么说: 把 60 赋给 a</p>
</li>
</ol>
<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>每一种数据都定义了明确的数据类型，在内存中分配了不同大小的内存空间(字节)。  </p>
<p><img src="/2023/08/04/JavaSE/2.png" alt="数据类型">    </p>
<h4 id="整型的类型"><a href="#整型的类型" class="headerlink" title="整型的类型"></a>整型的类型</h4><p><img src="/2023/08/04/JavaSE/3.png" alt="数据类型">     </p>
<p>&#x2F;&#x2F;Java 的整型常量（具体值）默认为 int 型，声明 long 型常量须后加‘l’或‘L’  </p>
<pre><code>int n1 = 1;//4 个字节
long n3 = 1L;//长整型  
</code></pre>
<h4 id="浮点类型"><a href="#浮点类型" class="headerlink" title="浮点类型"></a>浮点类型</h4><p><img src="/2023/08/04/JavaSE/4.png" alt="数据类型">    </p>
<p>&#x2F;&#x2F;Java 的浮点型常量(具体值)默认为 double 型，声明 float 型常量，须后加‘f’或‘F’  </p>
<pre><code>float num2 = 1.1F;    
double num3 = 1.1; 
double num4 = 1.1f; 
</code></pre>
<p>十进制数形式：如：5.12 512.0f .512 (必须有小数点）   </p>
<p>Java类的组织形式  </p>
<p><img src="/2023/08/04/JavaSE/5.png" alt="Java类的组织形式">   </p>
<h4 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h4><p><img src="/2023/08/04/JavaSE/6.png" alt="字符编码">  </p>
<h4 id="基本数据类型转换"><a href="#基本数据类型转换" class="headerlink" title="基本数据类型转换"></a>基本数据类型转换</h4><h5 id="自动类型转换"><a href="#自动类型转换" class="headerlink" title="自动类型转换"></a>自动类型转换</h5><p><img src="/2023/08/04/JavaSE/7.png" alt="自动类型转换">    </p>
<h5 id="强制类型转换"><a href="#强制类型转换" class="headerlink" title="强制类型转换"></a>强制类型转换</h5><p>自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符 ( )，但可能造成精度降低或溢出,格外要注意  </p>
<p><img src="/2023/08/04/JavaSE/8.png" alt="强制类型转换">  </p>
<h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><h3 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h3><p>说明逻辑运算规则：  </p>
<ol>
<li><p>a&amp;b : &amp; 叫逻辑与：规则：当 a 和 b 同时为 true ,则结果为 true, 否则为 false  </p>
</li>
<li><p>a&amp;&amp;b : &amp;&amp; 叫短路与：规则：当 a 和 b 同时为 true ,则结果为 true,否则为 false  </p>
</li>
<li><p>a|b : | 叫逻辑或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p>
</li>
<li><p>a||b : || 叫短路或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p>
</li>
<li><p>!a : 叫取反，或者非运算。当 a 为 true, 则结果为 false, 当 a 为 false 是，结果为 true  </p>
</li>
<li><p>a^b: 叫逻辑异或，当 a 和 b 不同时，则结果为 true, 否则为 false</p>
</li>
</ol>
<p><strong>&amp;&amp; 和 &amp; 使用区别</strong>  </p>
<ol>
<li><p>&amp;&amp;短路与：如果第一个条件为 false，则第二个条件不会判断，最终结果为 false，效率高  </p>
</li>
<li><p>&amp; 逻辑与：不管第一个条件是否为 false，第二个条件都要判断，效率低</p>
</li>
</ol>
<p><strong>|| 和 | 使用区别</strong>  </p>
<ol>
<li><p>||短路或：如果第一个条件为 true，则第二个条件不会判断，最终结果为 true，效率高  </p>
</li>
<li><p>| 逻辑或：不管第一个条件是否为 true，第二个条件都要判断，效率低</p>
</li>
</ol>
<h3 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h3><p>条件表达式 ? 表达式 1: 表达式 2;  </p>
<p>运算规则：  </p>
<p>1.如果条件表达式为 true，运算后的结果是表达式 1；  </p>
<p>2.如果条件表达式为 false，运算后的结果是表达式 2；  </p>
<p>口诀: [一灯大师：一真大师]  </p>
<h5 id="接收控制台输入Scanner"><a href="#接收控制台输入Scanner" class="headerlink" title="接收控制台输入Scanner"></a>接收控制台输入Scanner</h5><pre><code>import java.util.Scanner;    
Scanner myScanner = new Scanner(System.in);  
</code></pre>
<h5 id="原码、反码、补码-重点-难点"><a href="#原码、反码、补码-重点-难点" class="headerlink" title="原码、反码、补码(重点 难点)"></a>原码、反码、补码(重点 难点)</h5><p><img src="/2023/08/04/JavaSE/9.png">    </p>
<h2 id="程序控制结构"><a href="#程序控制结构" class="headerlink" title="程序控制结构"></a>程序控制结构</h2><p>主要有三大流程控制语句。  </p>
<ol>
<li><p>顺序控制  </p>
</li>
<li><p>分支控制    </p>
<ol>
<li>单分支 if  </li>
<li>双分支 if-else  </li>
<li>多分支 if-else if -….-else  </li>
<li>switch分支<br> switch(表达式){<br>         case xxx<br>                 }</li>
</ol>
</li>
<li><p>循环控制</p>
</li>
</ol>
<p><strong>for 循环控制</strong><br>    for(循环变量初始化;循环条件;循环变量迭代){<br>        循环操作(可以多条语句)	<br>            }  </p>
<pre><code>eg:  

for(int i = 1;i&lt;=10;i++)&#123;
    System.out.println(&quot;Hello World ~ ！&quot;)	
&#125;
</code></pre>
<p><strong>while 循环控制</strong>  </p>
<pre><code>循环变量初始化;  
while（循环条件）&#123;
    循环体（语句）；
    循环变量迭代；		
&#125;  

eg:		
int i = 1;
while (i &lt;= 10)&#123;
    System.out.println(&quot;Hello World ~ !&quot;)  
    i ++ 		
&#125;  
</code></pre>
<p><strong>do..while 循环控制</strong>  </p>
<pre><code>循环变量初始化;
do&#123;	
    循环体(语句);
    循环变量迭代;
&#125;while(循环条件);  
</code></pre>
<p>先执行，再判断，也就是说，一定会至少执行一次    </p>
<h2 id="跳转控制语句-break"><a href="#跳转控制语句-break" class="headerlink" title="跳转控制语句-break"></a>跳转控制语句-break</h2><p>break 语句用于终止某个语句块的执行，一般使用在 switch 或者循环[for , while , do-while]中  </p>
<h2 id="跳转控制语句-continue"><a href="#跳转控制语句-continue" class="headerlink" title="跳转控制语句-continue"></a>跳转控制语句-continue</h2><p>continue 语句用于结束本次循环，继续执行下一次循环  </p>
<h2 id="跳转控制语句-return"><a href="#跳转控制语句-return" class="headerlink" title="跳转控制语句-return"></a>跳转控制语句-return</h2><p>return 使用在方法，表示跳出所在的方法  </p>
<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>数组可以存放多个同一类型的数据。数组也是一种数据类型，是<strong>引用类型</strong>  </p>
<p><img src="/2023/08/04/JavaSE/10.png" alt="数组的使用">  </p>
<pre><code>方式一：int a[] = new Int[5]   

方式二：
       int[] a;
       a = new Int[10];   

方式三：
   	   int a[] = &#123;2,3,4,5,6&#125;  
</code></pre>
<h3 id="数组赋值机制"><a href="#数组赋值机制" class="headerlink" title="数组赋值机制"></a>数组赋值机制</h3><p><img src="/2023/08/04/JavaSE/11.png" alt="数组的使用">      </p>
<h1 id="面向对象编程-基础部分"><a href="#面向对象编程-基础部分" class="headerlink" title="面向对象编程(基础部分)"></a>面向对象编程(基础部分)</h1><p>类和对象的区别和联系</p>
<ol>
<li><p>类是抽象的，概念的，代表一类事物,比如人类,猫类.., 即它是数据类型  </p>
</li>
<li><p>对象是具体的，实际的，代表一个具体事物, 即 是实例   </p>
</li>
<li><p>类是对象的模板，对象是类的一个个体，对应一个实例</p>
</li>
</ol>
<h2 id="对象在内存中存在形式-重要的-必须搞清楚"><a href="#对象在内存中存在形式-重要的-必须搞清楚" class="headerlink" title="对象在内存中存在形式(重要的)必须搞清楚"></a>对象在内存中存在形式(重要的)必须搞清楚</h2><p><img src="/2023/08/04/JavaSE/12.png" alt="对象在内存中存在形式">     </p>
<h3 id="类和对象的内存分配机制"><a href="#类和对象的内存分配机制" class="headerlink" title="类和对象的内存分配机制"></a>类和对象的内存分配机制</h3><p>Java 内存的结构分析  </p>
<ol>
<li><p>栈： 一般存放基本数据类型(局部变量)  </p>
</li>
<li><p>堆： 存放对象(Cat cat , 数组等)  </p>
</li>
<li><p>方法区：常量池(常量，比如字符串)， 类加载信息</p>
</li>
</ol>
<h2 id="属性-成员变量-字段"><a href="#属性-成员变量-字段" class="headerlink" title="属性&#x2F;成员变量&#x2F;字段"></a>属性&#x2F;成员变量&#x2F;字段</h2><p>成员变量 &#x3D; 属性 &#x3D; field(字段)  </p>
<p>属性是类的一个组成部分，一般是基本数据类型,也可是引用类型(对象，数组)    </p>
<h2 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h2><p>Cat cat1 &#x3D; new Cat();  </p>
<h3 id="成员方法"><a href="#成员方法" class="headerlink" title="成员方法"></a>成员方法</h3><pre><code>class Person &#123;
    String name;
    int age;  
    //方法(成员方法)  
    public void speak() &#123;
        System.out.println(&quot;我是一个好人&quot;);
        &#125;
    &#125;  
</code></pre>
<h3 id="方法的调用机制原理"><a href="#方法的调用机制原理" class="headerlink" title="方法的调用机制原理"></a>方法的调用机制原理</h3><p><img src="/2023/08/04/JavaSE/13.png">  </p>
<h3 id="成员方法的好处"><a href="#成员方法的好处" class="headerlink" title="成员方法的好处"></a>成员方法的好处</h3><ol>
<li><p>提高代码的复用性  </p>
</li>
<li><p>可以将实现的细节封装起来，然后供其他用户来调用即可</p>
</li>
</ol>
<h3 id="成员方法的定义"><a href="#成员方法的定义" class="headerlink" title="成员方法的定义"></a>成员方法的定义</h3><pre><code>访问修饰符 返回数据类型 方法名（形参列表..） &#123;	
    //方法体语句；
    return 返回值;
&#125;
</code></pre>
<h3 id="传参"><a href="#传参" class="headerlink" title="传参"></a>传参</h3><p>引用类型传递的是地址（传递也是值，但是值是地址），可以通过形参影响实参！  </p>
<h3 id="方法递归调用-非常非常重要，比较难"><a href="#方法递归调用-非常非常重要，比较难" class="headerlink" title="方法递归调用(非常非常重要，比较难)"></a>方法递归调用(非常非常重要，比较难)</h3><p>递归就是方法自己调用自己  </p>
<p>递归重要规则  </p>
<p><img src="/2023/08/04/JavaSE/14.png"></p>
<h2 id="方法重载-OverLoad"><a href="#方法重载-OverLoad" class="headerlink" title="方法重载(OverLoad)"></a>方法重载(OverLoad)</h2><p>java 中允许同一个类中，多个同名方法的存在，但要求 形参列表不一致！  </p>
<p>案例：类：MyCalculator 方法：calculate  </p>
<ol>
<li>calculate(int n1, int n2) &#x2F;&#x2F;两个整数的和  </li>
<li>calculate(int n1, double n2) &#x2F;&#x2F;一个整数，一个 double 的和  </li>
<li>calculate(double n2, int n1)&#x2F;&#x2F;一个 double ,一个 Int 和  </li>
<li>calculate(int n1, int n2,int n3)&#x2F;&#x2F;三个 int 的和</li>
</ol>
<p><img src="/2023/08/04/JavaSE/15.png" alt="方法重载">  </p>
<h3 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h3><p>java 允许将同一个类中多个同名同功能但参数个数不同的方法，封装成一个方法。<br>就可以通过可变参数实现</p>
<p>eg:方法 sum 【可以计算 2 个数的和，3 个数的和 ， 4. 5， 。。】  </p>
<pre><code>//1. int... 表示接受的是可变参数，类型是 int ,即可以接收多个 int(0-多)
//2. 使用可变参数时，可以当做数组来使用 即 nums 可以当做数组
//3. 遍历 nums 求和即可
public int sum(int... nums) &#123;
//System.out.println(&quot;接收的参数个数=&quot; + nums.length);
int res = 0;
for(int i = 0; i &lt; nums.length; i++) &#123;
res += nums[i];
&#125;
return res;
&#125;
&#125;  
</code></pre>
<h3 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h3><p>变量：  </p>
<p>1.全局变量（属性）</p>
<p>2.局部变量（局部变量一般是指在成员方法中定义的变量）</p>
<p>全局变量和局部变量可以重名</p>
<p><img src="/2023/08/04/JavaSE/16.png" alt="变量作用域">  </p>
<p>全局变量和局部变量的区别  </p>
<p><img src="/2023/08/04/JavaSE/17.png" alt="全局变量和局部变量的区别">    </p>
<h3 id="构造方法-构造器"><a href="#构造方法-构造器" class="headerlink" title="构造方法&#x2F;构造器"></a>构造方法&#x2F;构造器</h3><p>在创建人类的对象时，就直接指定这个对象的年龄和姓名，该怎么做? 这时就可以使用构造器  </p>
<p>[修饰符] 方法名(形参列表){<br>方法体;<br>}   </p>
<ol>
<li><p>构造器的修饰符可以默认， 也可以是 public protected private  </p>
</li>
<li><p>构造器没有返回值   </p>
</li>
<li><p>方法名 和类名字必须一样  </p>
</li>
<li><p>参数列表 和 成员方法一样的规则  </p>
</li>
<li><p>构造器的调用, 由系统完成</p>
</li>
</ol>
<p>构造方法又叫构造器(constructor)，是类的一种特殊的方法，它的主要作用是完成对新对象的初始化  </p>
<p><img src="/2023/08/04/JavaSE/18.png" alt="构造器使用注意事项"></p>
<p><img src="/2023/08/04/JavaSE/19.png" alt="构造器使用注意事项"></p>
<h3 id="this-关键字"><a href="#this-关键字" class="headerlink" title="this 关键字"></a>this 关键字</h3><p><img src="/2023/08/04/JavaSE/20.png" alt="This关键字"></p>
<ol>
<li><p>this 关键字可以用来访问本类的属性、方法、构造器  </p>
</li>
<li><p>this 用于区分当前类的属性和局部变量  </p>
</li>
<li><p>访问成员方法的语法：this.方法名(参数列表);    </p>
</li>
<li><p>访问构造器语法：this(参数列表); 注意只能在构造器中使用(即只能在构造器中访问另外一个构造器, 必须放在第一条语句)  </p>
</li>
<li><p>this 不能在类定义的外部使用，只能在类定义的方法中使用。</p>
</li>
</ol>
<h1 id="面向对象编程-中级部分"><a href="#面向对象编程-中级部分" class="headerlink" title="面向对象编程(中级部分)"></a>面向对象编程(中级部分)</h1><p>IDEA 常用快捷键  </p>
<ol>
<li><p>删除当前行, 默认是 ctrl + Y 自己配置 ctrl + d  </p>
</li>
<li><p>复制当前行, 自己配置 ctrl + alt + 向下光标  </p>
</li>
<li><p>补全代码 alt + &#x2F;  </p>
</li>
<li><p>添加注释和取消注释 ctrl + &#x2F; 【第一次是添加注释，第二次是取消注释】  </p>
</li>
<li><p>导入该行需要的类 先配置 auto import , 然后使用 alt+enter 即可  </p>
</li>
<li><p>快速格式化代码 ctrl + alt + L  </p>
</li>
<li><p>快速运行程序 自己定义 alt + R  </p>
</li>
<li><p>生成构造器等 alt + insert [提高开发效率]  </p>
</li>
<li><p>查看一个类的层级关系 ctrl + H [学习继承后，非常有用]  </p>
</li>
<li><p>将光标放在一个方法上，输入 ctrl + B , 可以定位到方法 [学继承后，非常有用]  </p>
</li>
<li><p>自动的分配变量名 , 通过 在后面假 .var [老师最喜欢的]  </p>
</li>
<li><p>还有很多其它的快捷键</p>
</li>
</ol>
<h2 id="包"><a href="#包" class="headerlink" title="包"></a>包</h2><p><img src="/2023/08/04/JavaSE/21.png" alt="包">  </p>
<p>包的本质 </p>
<p><img src="/2023/08/04/JavaSE/22.png">  </p>
<p>包的命名：  </p>
<p>com.公司名.项目名.业务模块名  </p>
<h3 id="Java常用的包"><a href="#Java常用的包" class="headerlink" title="Java常用的包"></a>Java常用的包</h3><p>一个包下,包含很多的类,java 中常用的包有:  </p>
<ol>
<li><p>java.lang.* &#x2F;&#x2F;lang 包是基本包，默认引入，不需要再引入.  </p>
</li>
<li><p>java.util.* &#x2F;&#x2F;util 包，系统提供的工具包, 工具类，使用 Scanner  </p>
</li>
<li><p>java.net.* &#x2F;&#x2F;网络包，网络开发  </p>
</li>
<li><p>java.awt.* &#x2F;&#x2F;是做 java 的界面开发，GUI</p>
</li>
</ol>
<p>引入包的语法：  </p>
<p>import 包;  </p>
<p>eg:  import java.util.*  &#x2F;&#x2F;表示将java.util包所有都引入    </p>
<p>我们需要使用到哪个类，就导入哪个类即可，不建议使用*导入   </p>
<h2 id="访问修饰符"><a href="#访问修饰符" class="headerlink" title="访问修饰符"></a>访问修饰符</h2><p>java 提供四种访问控制修饰符号，用于控制方法和属性(成员变量)的访问权限（范围）   </p>
<ol>
<li><p>公开级别:用 public 修饰,对外公开  </p>
</li>
<li><p>受保护级别:用 protected 修饰,对子类和同一个包中的类公开  </p>
</li>
<li><p>默认级别:没有修饰符号,向同一个包的类公开.   </p>
</li>
<li><p>私有级别:用 private 修饰,只有类本身可以访问,不对外公开</p>
</li>
</ol>
<p><img src="/2023/08/04/JavaSE/23.png" alt="访问控制符">  </p>
<p><img src="/2023/08/04/JavaSE/24.png" alt="访问控制符使用说明">   </p>
<h2 id="面向对象编程三大特征"><a href="#面向对象编程三大特征" class="headerlink" title="面向对象编程三大特征"></a>面向对象编程三大特征</h2><p>封装、继承和多态    </p>
<h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><p><img src="/2023/08/04/JavaSE/25.png" alt="封装"></p>
<p>封装的实现步骤    </p>
<p><img src="/2023/08/04/JavaSE/26.png" alt="封装的实现步骤">    </p>
<h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>继承可以解决代码复用,让我们的编程更加靠近人类思维.当多个类存在相同的属性(变量)和方法时,可以从这些类中抽象出父类,在父类中定义这些相同的属性和方法，所有的子类不需要重新定义这些属性和方法，只需要通过 extends 来声明继承父类即可  </p>
<p><img src="/2023/08/04/JavaSE/27.png" alt="继承">  </p>
<p>继承的基本语法：  </p>
<p>class 子类 extends 父类 {</p>
<p>}<br>子类就会自动拥有父类定义的属性和方法<br>子类又叫超类，基类<br>子类又叫派生类  </p>
<p>继承给编程带来的便利  </p>
<ol>
<li><p>代码的复用性提高了  </p>
</li>
<li><p>代码的扩展性和维护性提高了</p>
</li>
</ol>
<p>**继承的细节问题： ** </p>
<ol>
<li><p>子类继承了所有的属性和方法，非私有的属性和方法可以在子类直接访问, 但是私有属性和方法不能在子类直接访问，要通过父类提供公共的方法去访问  </p>
</li>
<li><p>子类必须调用父类的构造器,完成父类的初始化  </p>
</li>
<li><p>当创建子类对象时，不管使用子类的哪个构造器，默认情况下总会去调用父类的无参构造器，如果父类没有提供无参构造器，则必须在子类的构造器中用 super 去指定使用父类的哪个构造器完成对父类的初始化工作，否则，编译不会通过(怎么理解。)   </p>
</li>
<li><p>如果希望指定去调用父类的某个构造器，则显式的调用一下 : super(参数列表)  </p>
</li>
<li><p>super 在使用时，必须放在构造器第一行(super 只能在构造器中使用)  </p>
</li>
<li><p>super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器  </p>
</li>
<li><p>java 所有类都是 Object 类的子类, Object 是所有类的基类.  </p>
</li>
<li><p>父类构造器的调用不限于直接父类！将一直往上追溯直到 Object 类(顶级父类)    </p>
</li>
<li><p>子类最多只能继承一个父类(指直接继承)，即 java 中是单继承机制。<br>思考：如何让 A 类继承 B 类和 C 类？ 【A 继承 B， B 继承 C】  </p>
</li>
<li><p>不能滥用继承，子类和父类之间必须满足 is-a 的逻辑关系</p>
</li>
</ol>
<p><strong>输入 ctrl + H 可以看到类的继承关系</strong></p>
<pre><code>public class Sub extends Base &#123; //子类
    public Sub(String name, int age) &#123;
    //1. 调用父类的无参构造器, 如下或者什么都不写,默认就是调用 super()
    //super();//父类的无参构造器
    //2. 调用父类的 Base(String name) 构造器
    //super(&quot;hsp&quot;);
    //调用父类的 Base(String name, int age) 构造器
    super(&quot;king&quot;, 20);
    //细节： super 在使用时，必须放在构造器第一行
    //细节: super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器
    //this() 不能再使用了
    System.out.println(&quot;子类 Sub(String name, int age)构造器被调用....&quot;);
    &#125;
</code></pre>
<p>子类创建的内存布局  </p>
<p><img src="/2023/08/04/JavaSE/28.png" alt="子类创建的内存布局">    </p>
<h3 id="super-关键字"><a href="#super-关键字" class="headerlink" title="super 关键字"></a>super 关键字</h3><p>super 代表父类的引用，用于访问父类的属性、方法、构造器  </p>
<p><img src="/2023/08/04/JavaSE/29.png" alt="super关键字">  </p>
<p>&#x2F;&#x2F; (1)先找本类，如果有，则调用  </p>
<p>&#x2F;&#x2F; (2)如果没有，则找父类(如果有，并可以调用，则调用)  </p>
<p>&#x2F;&#x2F; (3)如果父类没有，则继续找父类的父类,整个规则，就是一样的,直到 Object 类  </p>
<p>&#x2F;&#x2F; 提示：如果查找方法的过程中，找到了，但是不能访问， 则报错, cannot access  </p>
<p>&#x2F;&#x2F; 如果查找方法的过程中，没有找到，则提示方法不存在    </p>
<p><img src="/2023/08/04/JavaSE/30.png" alt="Super关键字的用法细节">      </p>
<p><strong>super 和 this 的比较</strong>    </p>
<p><img src="/2023/08/04/JavaSE/31.png" alt="super和this的比较">    </p>
<h2 id="方法重写-覆盖-override"><a href="#方法重写-覆盖-override" class="headerlink" title="方法重写&#x2F;覆盖(override)"></a>方法重写&#x2F;覆盖(override)</h2><p><img src="/2023/08/04/JavaSE/32.png" alt="方法重写">  </p>
<p><img src="/2023/08/04/JavaSE/33.png" alt="方法重写的注意事项">    </p>
<p><strong>方法重载和方法重写的区别</strong>  </p>
<p><img src="/2023/08/04/JavaSE/34.png" alt="方法重载和方法重写的区别">  </p>
<h2 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h2><p>多态是建立在封装和继承基础之上的<br><img src="/2023/08/04/JavaSE/35.png" alt="多态">     </p>
<p>多态的具体体现  </p>
<ol>
<li>方法的多态</li>
</ol>
<p>重写和重载就体现多态    </p>
<ol start="2">
<li>对象的多态 (核心，困难，重点)</li>
</ol>
<p><img src="/2023/08/04/JavaSE/36.png" alt="多态案例">     </p>
<p>多态的前提是：两个对象(类)存在继承关系  </p>
<p>多态的向上转型   </p>
<p><img src="/2023/08/04/JavaSE/37.png" alt="多态的向上转型">   </p>
<p>多态向下转型  </p>
<p><img src="/2023/08/04/JavaSE/38.png" alt="多态的向下转型">    </p>
<p>属性没有重写之说！属性的值看编译类型   </p>
<p>instanceOf 比较操作符，用于判断对象的运行类型是否为 XX 类型或 XX 类型的子类型  </p>
<p>java 的动态绑定机制(非常非常重要.)  page 365</p>
<p>JDBC page 1119  </p>
<p>正则表达式 page 1210</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/" class="post-title-link" itemprop="url">数仓建模</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-03 18:25:47" itemprop="dateCreated datePublished" datetime="2023-08-03T18:25:47+08:00">2023-08-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 12:58:35" itemprop="dateModified" datetime="2023-08-11T12:58:35+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="人生在勤，不索何获"><a href="#人生在勤，不索何获" class="headerlink" title="人生在勤，不索何获"></a>人生在勤，不索何获</h1><p><a target="_blank" rel="noopener" href="https://help.aliyun.com/zh/dataworks/user-guide/dataworks-data-modeling/?spm=a2c4g.11186623.0.0.6cbf2c36NLQ1IR">阿里数仓建模理论</a></p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=2042878838&auto=1&height=66"></iframe>

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张宴银"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">张宴银</p>
  <div class="site-description" itemprop="description">初级以内我无敌，中级以上我一换一</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; Sat Jul 29 2023 08:00:00 GMT+0800 (中国标准时间) – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张宴银</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>



    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共54.8k字</span>
</div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
