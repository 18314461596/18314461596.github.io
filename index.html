<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="初级以内我无敌，中级以上我一换一">
<meta property="og:type" content="website">
<meta property="og:title" content="第五门徒">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="第五门徒">
<meta property="og:description" content="初级以内我无敌，中级以上我一换一">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张宴银">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>第五门徒</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="第五门徒" type="application/rss+xml">
</head>




<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">第五门徒</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="heartbeat fa-fw"></i>公益 404</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/11/Spark-Streaming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/11/Spark-Streaming/" class="post-title-link" itemprop="url">Spark-Streaming</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-08-11 22:03:30 / 修改时间：22:57:34" itemprop="dateCreated datePublished" datetime="2023-08-11T22:03:30+08:00">2023-08-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也"><a href="#英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也" class="headerlink" title="英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也"></a>英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也</h1><h1 id="SparkStreaming-概述"><a href="#SparkStreaming-概述" class="headerlink" title="SparkStreaming 概述"></a>SparkStreaming 概述</h1><h2 id="Spark-Streaming-是什么"><a href="#Spark-Streaming-是什么" class="headerlink" title="Spark Streaming 是什么"></a>Spark Streaming 是什么</h2><p>Spark Streaming 用于流式数据的处理。Spark Streaming 支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如：map、reduce、join、window 等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。  </p>
<h2 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h2><p>和 Spark 基于 RDD 的概念很相似，Spark Streaming 使用离散化流(discretized stream)作为抽象表示，叫作 DStream。  </p>
<p>DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此得名“离散化”)。所以简单来将，DStream 就是对 RDD 在实时数据处理场景的一种封装。  </p>
<h2 id="Spark-Streaming-的特点"><a href="#Spark-Streaming-的特点" class="headerlink" title="Spark Streaming 的特点"></a>Spark Streaming 的特点</h2><p>易用    </p>
<p>容错  </p>
<p>易整合到 Spark 体系    </p>
<h2 id="Spark-Streaming-架构"><a href="#Spark-Streaming-架构" class="headerlink" title="Spark Streaming 架构"></a>Spark Streaming 架构</h2><p><img src="/2023/08/11/Spark-Streaming/1.png" alt="整体架构图">  </p>
<p><img src="/2023/08/11/Spark-Streaming/2.png" alt="架构图"> </p>
<h2 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h2><p>背压机制（即 Spark Streaming Backpressure）: 根据JobScheduler 反馈作业的执行信息来动态调整 Receiver 数据接收率。  </p>
<p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用 backpressure 机制，默认值false，即不启用。    </p>
<h2 id="Dstream-入门"><a href="#Dstream-入门" class="headerlink" title="Dstream 入门"></a>Dstream 入门</h2><h3 id="WordCount-案例实操"><a href="#WordCount-案例实操" class="headerlink" title="WordCount 案例实操"></a>WordCount 案例实操</h3><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><pre><code>&lt;dependency&gt;
 	&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
 	&lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt;
 	&lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;    
</code></pre>
<h4 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h4><pre><code>object StreamWordCount &#123;
 def main(args: Array[String]): Unit = &#123;
     //1.初始化 Spark 配置信息
     val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;StreamWordCount&quot;)
     //2.初始化 SparkStreamingContext
     val ssc = new StreamingContext(sparkConf, Seconds(3))
     //3.通过监控端口创建 DStream，读进来的数据为一行行
     val lineStreams = ssc.socketTextStream(&quot;linux1&quot;, 9999)
     //将每一行数据做切分，形成一个个单词
     val wordStreams = lineStreams.flatMap(_.split(&quot; &quot;))
     //将单词映射成元组（word,1）
     val wordAndOneStreams = wordStreams.map((_, 1))
     //将相同的单词次数做统计
     val wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_)
     //打印
     wordAndCountStreams.print()
     //启动 SparkStreamingContext
     ssc.start()
     ssc.awaitTermination()
     &#125;
    &#125;  
</code></pre>
<h4 id="WordCount-解析"><a href="#WordCount-解析" class="headerlink" title="WordCount 解析"></a>WordCount 解析</h4><p>在内部实现上，DStream 是一系列连续的 RDD 来表示。每个 RDD 含有一段时间间隔内的数据。</p>
<p><img src="/2023/08/11/Spark-Streaming/3.png" alt="DStream">   </p>
<p>对数据的操作也是按照 RDD 为单位来进行的  </p>
<p><img src="/2023/08/11/Spark-Streaming/4.png" alt="DStream对数据的操作">     </p>
<p>计算过程由 Spark Engine 来完成  </p>
<p><img src="/2023/08/11/Spark-Streaming/5.png" alt="DStream的计算过程"> </p>
<h2 id="DStream创建"><a href="#DStream创建" class="headerlink" title="DStream创建"></a>DStream创建</h2><h3 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h3><h4 id="Kafka-0-10-Direct-模式"><a href="#Kafka-0-10-Direct-模式" class="headerlink" title="Kafka 0-10 Direct 模式"></a>Kafka 0-10 Direct 模式</h4><h5 id="需求：通过-SparkStreaming-从-Kafka-读取数据，并将读取过来的数据做简单计算，最终打印到控制台。"><a href="#需求：通过-SparkStreaming-从-Kafka-读取数据，并将读取过来的数据做简单计算，最终打印到控制台。" class="headerlink" title="需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。"></a>需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</h5><h5 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h5><pre><code>&lt;dependency&gt;
     &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
     &lt;artifactId&gt;spark-streaming-kafka-0-10_2.12&lt;/artifactId&gt;
     &lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
     &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
     &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;
     &lt;version&gt;2.10.1&lt;/version&gt;
&lt;/dependency&gt;  
</code></pre>
<h5 id="编写代码-1"><a href="#编写代码-1" class="headerlink" title="编写代码"></a>编写代码</h5><pre><code>import org.apache.kafka.clients.consumer.&#123;ConsumerConfig, ConsumerRecord&#125;  
import org.apache.spark.SparkConf  
import org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;  
import org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;  
import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;  
object DirectAPI &#123;
     def main(args: Array[String]): Unit = &#123;
     //1.创建 SparkConf
     val sparkConf: SparkConf = new 
     SparkConf().setAppName(&quot;ReceiverWordCount&quot;).setMaster(&quot;local[*]&quot;)
     //2.创建 StreamingContext
     val ssc = new StreamingContext(sparkConf, Seconds(3))
     //3.定义 Kafka 参数
     val kafkaPara: Map[String, Object] = Map[String, Object](
     ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; &quot;linux1:9092,linux2:9092,linux3:9092&quot;,
     ConsumerConfig.GROUP_ID_CONFIG -&gt; &quot;atguigu&quot;,
     &quot;key.deserializer&quot; -&gt; 
    &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;,
     &quot;value.deserializer&quot; -&gt; 
    &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;
     )
     //4.读取 Kafka 数据创建 DStream
     val kafkaDStream: InputDStream[ConsumerRecord[String, String]] = 
    KafkaUtils.createDirectStream[String, String](ssc,
     LocationStrategies.PreferConsistent,
     ConsumerStrategies.Subscribe[String, String](Set(&quot;atguigu&quot;), kafkaPara))
     //5.将每条消息的 KV 取出
     val valueDStream: DStream[String] = kafkaDStream.map(record =&gt; record.value())
     //6.计算 WordCount
     valueDStream.flatMap(_.split(&quot; &quot;))
     .map((_, 1))
     .reduceByKey(_ + _)
     .print()
     //7.开启任务
     ssc.start()
     ssc.awaitTermination()
     &#125;
    &#125;  
</code></pre>
<h5 id="查看-Kafka-消费进度"><a href="#查看-Kafka-消费进度" class="headerlink" title="查看 Kafka 消费进度"></a>查看 Kafka 消费进度</h5><pre><code>bin/kafka-consumer-groups.sh --describe --bootstrap-server linux1:9092 --group atguigu
</code></pre>
<h2 id="DStream转换"><a href="#DStream转换" class="headerlink" title="DStream转换"></a>DStream转换</h2><p>DStream 上的操作与 RDD 的类似，分为 Transformations（转换）和 Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种 Window 相关的原语。  </p>
<h3 id="无状态转化操作"><a href="#无状态转化操作" class="headerlink" title="无状态转化操作"></a>无状态转化操作</h3><p>无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每<br>一个 RDD。<br>注意，针对键值对的 DStream 转化操作(比如reduceByKey())要添加 import StreamingContext._才能在 Scala 中使用。     </p>
<p><img src="/2023/08/11/Spark-Streaming/6.png" alt="无状态转化操作">   </p>
<p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部<br>是由许多 RDD（批次）组成，且无状态转化操作是分别应用到每个 RDD 上的。  </p>
<p>例如：reduceByKey()会归约每个时间区间中的数据，但不会归约不同区间之间的数据。  </p>
<h4 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h4><p>Transform 允许 DStream 上执行任意的 RDD-to-RDD 函数。即使这些函数并没有在 DStream<br>的 API 中暴露出来，通过该函数可以方便的扩展 Spark API。该函数每一批次调度一次。其实也<br>就是对 DStream 中的 RDD 应用转换。  </p>
<h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>两个流之间的 join 需要两个流的批次大小一致，这样才能做到同时触发计算。计算过程就是对当前批次的两个流中各自的 RDD 进行 join，与两个 RDD 的 join 效果相同。  </p>
<pre><code>import org.apache.spark.SparkConf
import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;
import org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;
object JoinTest &#123;
     def main(args: Array[String]): Unit = &#123;
     //1.创建 SparkConf
     val sparkConf: SparkConf = new 
     SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;JoinTest&quot;)
     //2.创建 StreamingContext
     val ssc = new StreamingContext(sparkConf, Seconds(5))
     //3.从端口获取数据创建流
     val lineDStream1: ReceiverInputDStream[String] = 
     ssc.socketTextStream(&quot;linux1&quot;, 9999)
     val lineDStream2: ReceiverInputDStream[String] = 
     ssc.socketTextStream(&quot;linux2&quot;, 8888)
     //4.将两个流转换为 KV 类型
     val wordToOneDStream: DStream[(String, Int)] = lineDStream1.flatMap(_.split(&quot; &quot;)).map((_, 1))
     val wordToADStream: DStream[(String, String)] = lineDStream2.flatMap(_.split(&quot; &quot;)).map((_, &quot;a&quot;))
     //5.流的 JOIN
     val joinDStream: DStream[(String, (Int, String))] = wordToOneDStream.join(wordToADStream)
     //6.打印
     joinDStream.print()
     //7.启动任务
     ssc.start()
     ssc.awaitTermination()
     &#125;
    &#125;
</code></pre>
<h3 id="有状态转化操作"><a href="#有状态转化操作" class="headerlink" title="有状态转化操作"></a>有状态转化操作</h3><h4 id="UpdateStateByKey"><a href="#UpdateStateByKey" class="headerlink" title="UpdateStateByKey"></a>UpdateStateByKey</h4><p>UpdateStateByKey 原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加 wordcount)。  </p>
<p>updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。      </p>
<p>updateStateByKey 操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功<br>能，需要做下面两步：<br>    1.定义状态，状态可以是一个任意的数据类型。<br>    2.定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更<br>新。  </p>
<h4 id="WindowOperations"><a href="#WindowOperations" class="headerlink" title="WindowOperations"></a>WindowOperations</h4><p>Window Operations 可以设置窗口的大小和滑动窗口的间隔来动态的获取当前 Steaming 的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。  </p>
<p>窗口时长：计算内容的时间范围；  </p>
<p>滑动步长：隔多久触发一次计算。  </p>
<p>注意：这两者都必须为采集周期大小的整数倍。  </p>
<pre><code>object WorldCount &#123;
     def main(args: Array[String]) &#123;
         val conf = new 
         SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;NetworkWordCount&quot;)
         val ssc = new StreamingContext(conf, Seconds(3))
         ssc.checkpoint(&quot;./ck&quot;)
         // Create a DStream that will connect to hostname:port, like localhost:9999
         val lines = ssc.socketTextStream(&quot;linux1&quot;, 9999)
         // Split each line into words
         val words = lines.flatMap(_.split(&quot; &quot;))
         // Count each word in each batch
         val pairs = words.map(word =&gt; (word, 1))
         val wordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b),Seconds(12), Seconds(6))
         // Print the first ten elements of each RDD generated in this DStream to the console
         wordCounts.print()
         ssc.start() // Start the computation
         ssc.awaitTermination() // Wait for the computation to terminate
         &#125;
        &#125;
      
</code></pre>
<p>关于 Window 的操作还有如下方法：  </p>
<pre><code>window(windowLength, slideInterval): 基于对源 DStream 窗化的批次进行计算返回一个新的 Dstream；  

countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；  

reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；  

reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V)对的 DStream 上调用此函数，会返回一个新(K,V)对的 DStream，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值。  

reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的 reduce 值都是通过用前一个窗的 reduce 值来递增计算。  
</code></pre>
<p><img src="/2023/08/11/Spark-Streaming/7.png" alt="可逆的reduce函数">  </p>
<p>countByWindow()和 countByValueAndWindow()作为对数据进行计数操作的简写。countByWindow()返回一个表示每个窗口中元素个数的 DStream，而countByValueAndWindow()返回的 DStream 则包含窗口中每个值的个数。   </p>
<h2 id="DStream-输出"><a href="#DStream-输出" class="headerlink" title="DStream 输出"></a>DStream 输出</h2><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。<br>与 RDD 中的惰性求值类似，如果一个 DStream 及其派生出的 DStream 都没有被执行输出操作，那么这些 DStream 就都不会被求值。如果 StreamingContext 中没有设定输出操作，整个 context 就都不会启动。    </p>
<p>输出操作清单：</p>
<pre><code>print()：在运行流程序的驱动结点上打印 DStream 中每一批次数据的最开始 10 个元素 

saveAsTextFiles(prefix, [suffix])：以 text 文件形式存储这个 DStream 的内容
每一批次的存储文件名基于参数中的 prefix 和 suffix。”prefix-Time_IN_MS[.suffix]”  

saveAsObjectFiles(prefix, [suffix])：以 Java 对象序列化的方式将 Stream 中的数据保存为SequenceFiles . 每一批次的存储文件名基于参数中的为&quot;prefix-TIME_IN_MS[.suffix]&quot;. Python中目前不可用  

saveAsHadoopFiles(prefix, [suffix])：将 Stream 中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为&quot;prefix-TIME_IN_MS[.suffix]&quot;。Python API 中目前不可用  

foreachRDD(func)：这是最通用的输出操作，即将函数 func 用于产生于 stream 的每一个RDD。其中参数传入的函数 func 应该实现将每一个 RDD 中数据推送到外部系统，如将
RDD 存入文件或者通过网络将其写入数据库。  
</code></pre>
<p>通用的输出操作 foreachRDD()，它用来对 DStream 中的 RDD 运行任意计算  </p>
<p>在 foreachRDD()中，可以重用我们在 Spark 中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如 MySQL 的外部数据库中。  </p>
<p>注意：<br>    1) 连接不能写在 driver 层面（序列化）<br>    2) 如果写在 foreach 则每个 RDD 中的每一条数据都创建，得不偿失；<br>    3) 增加 foreachPartition，在分区创建（获取）   </p>
<h2 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h2><p>流式任务需要 7*24 小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。使用外部文件系统来控制内部程序关闭。  </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/11/Spark-SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/11/Spark-SQL/" class="post-title-link" itemprop="url">Spark-SQL</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-08-11 20:19:25 / 修改时间：22:02:53" itemprop="dateCreated datePublished" datetime="2023-08-11T20:19:25+08:00">2023-08-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功"><a href="#无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功" class="headerlink" title="无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功"></a>无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功</h1><h1 id="SparkSQL概述"><a href="#SparkSQL概述" class="headerlink" title="SparkSQL概述"></a>SparkSQL概述</h1><h2 id="SparkSQL是什么？"><a href="#SparkSQL是什么？" class="headerlink" title="SparkSQL是什么？"></a>SparkSQL是什么？</h2><p>Spark SQL是Spark用于结构化数据(structured data)处理的Spark模块。  </p>
<pre><code>➢ 数据兼容方面 SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据；	

➢ 性能优化方面 除了采取 In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等；

➢ 组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展。  
</code></pre>
<p>应用Spark的两个支线：SparkSQL 和 Hive on Spark  </p>
<p>SparkSQL 可以简化 RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。  </p>
<p>Spark SQL 为了简化 RDD 的开发，提高开发效率，提供了2个编程抽象，类似 Spark Core 中的RDD。</p>
<pre><code>➢ DataFrame	
➢ DataSet
</code></pre>
<h2 id="SparkSQL-特点"><a href="#SparkSQL-特点" class="headerlink" title="SparkSQL 特点"></a>SparkSQL 特点</h2><h3 id="易整合"><a href="#易整合" class="headerlink" title="易整合"></a>易整合</h3><p>无缝的整合了 SQL 查询和 Spark 编程  </p>
<h3 id="统一的数据访问"><a href="#统一的数据访问" class="headerlink" title="统一的数据访问"></a>统一的数据访问</h3><p>使用相同的方式连接不同的数据源   </p>
<h3 id="兼容-Hive"><a href="#兼容-Hive" class="headerlink" title="兼容 Hive"></a>兼容 Hive</h3><p>在已有的仓库上直接运行 SQL 或者 HiveQL  </p>
<h3 id="标准数据连接"><a href="#标准数据连接" class="headerlink" title="标准数据连接"></a>标准数据连接</h3><p>通过 JDBC 或者 ODBC 来连接  </p>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。  </p>
<p>DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL得以洞察更多的结构信息，达到大幅提升运行时效率的目标。  </p>
<p>反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在 stage 层面进行简单、通用的流水线优化。  </p>
<p><img src="/2023/08/11/Spark-SQL/1.png" alt="DataFrame和RDD的区别">    </p>
<p>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待。     </p>
<p>DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计划通过 Spark catalyst optimiser 进行优化。  </p>
<p><img src="/2023/08/11/Spark-SQL/2.png" alt="逻辑查询计划优化">     </p>
<p>逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操<br>作的过程。     </p>
<h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>DataSet 是分布式数据集合。  </p>
<p>它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）。  </p>
<pre><code>➢ DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象  
➢ 用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到
DataSet 中的字段名称；  
➢ DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。  
➢ DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。     
</code></pre>
<h2 id="SparkSQL-核心编程"><a href="#SparkSQL-核心编程" class="headerlink" title="SparkSQL 核心编程"></a>SparkSQL 核心编程</h2><p>SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和HiveContext的组合。  </p>
<h3 id="DataFrame-1"><a href="#DataFrame-1" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>Spark SQL 的 DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL表达式。    </p>
<h4 id="创建-DataFrame"><a href="#创建-DataFrame" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h4><p>创建 DataFrame有三种方式：<br>    1.通过 Spark 的数据源进行创建；<br>    2.从一个存在的 RDD 进行转换；<br>    3.从 Hive Table 进行查询返回。   </p>
<h5 id="从-Spark-数据源进行创建"><a href="#从-Spark-数据源进行创建" class="headerlink" title="从 Spark 数据源进行创建"></a>从 Spark 数据源进行创建</h5><pre><code>scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)
df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]
</code></pre>
<h5 id="从一个存在的-RDD-进行转换"><a href="#从一个存在的-RDD-进行转换" class="headerlink" title="从一个存在的 RDD 进行转换"></a>从一个存在的 RDD 进行转换</h5><h5 id="从-Hive-Table-进行查询返回"><a href="#从-Hive-Table-进行查询返回" class="headerlink" title="从 Hive Table 进行查询返回"></a>从 Hive Table 进行查询返回</h5><h3 id="SQL-语法"><a href="#SQL-语法" class="headerlink" title="SQL 语法"></a>SQL 语法</h3><p>SQL 语法风格是指我们查询数据的时候使用 SQL 语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助。    </p>
<ol>
<li><p>读取 JSON 文件创建 DataFrame  </p>
<p> scala&gt; val df &#x3D; spark.read.json(“data&#x2F;user.json”)<br> df: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， username: string]</p>
</li>
<li><p>对 DataFrame 创建一个临时表 </p>
<p> scala&gt; df.createOrReplaceTempView(“people”)</p>
</li>
<li><p>通过 SQL 语句实现查询全表  </p>
<p> scala&gt; val sqlDF &#x3D; spark.sql(“SELECT * FROM people”)<br> sqlDF: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， name: string]</p>
</li>
<li><p>结果展示</p>
<p> scala&gt; sqlDF.show<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|</p>
</li>
</ol>
<p>注意：普通临时表是 Session 范围内的，如果想应用范围内有效，可以使用全局临时表。使<br>用全局临时表时需要全路径访问，如：global_temp.people  </p>
<ol start="5">
<li><p>对于 DataFrame 创建一个全局表  </p>
<p> scala&gt; df.createGlobalTempView(“people”)</p>
</li>
<li><p>通过 SQL 语句实现查询全表  </p>
<p> scala&gt; spark.sql(“SELECT * FROM global_temp.people”).show()<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|<br> +—+——–+<br> scala&gt; spark.newSession().sql(“SELECT * FROM global_temp.people”).show()<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|<br> +—+——–+</p>
</li>
</ol>
<h3 id="DSL-语法"><a href="#DSL-语法" class="headerlink" title="DSL 语法"></a>DSL 语法</h3><p>DataFrame 提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。  </p>
<p>可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了。    </p>
<ol>
<li><p>创建一个 DataFrame  </p>
<p> scala&gt; val df &#x3D; spark.read.json(“data&#x2F;user.json”)<br> df: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， name: string]</p>
</li>
<li><p>查看 DataFrame 的 Schema 信息  </p>
<p> scala&gt; df.printSchema<br> root<br> |– age: Long (nullable &#x3D; true)<br> |– username: string (nullable &#x3D; true)  </p>
</li>
<li><p>只查看”username”列数据  </p>
<p> scala&gt; df.select(“username”).show()<br> +——–+<br> |username|<br> +——–+<br> |zhangsan|<br> | lisi|<br> | wangwu|<br> +——–+  </p>
</li>
<li><p>查看”username”列数据以及”age+1”数据  </p>
<p> 注意:涉及到运算的时候, 每列都必须使用$, 或者采用引号表达式：单引号+字段名<br> scala&gt; df.select($”username”,$”age” + 1).show<br> scala&gt; df.select(‘username, ‘age + 1).show()  </p>
<p> scala&gt; df.select(‘username, ‘age + 1 as “newage”).show()<br> +——–+———+<br> |username|(age + 1)|<br> +——–+———+<br> |zhangsan| 21|<br> | lisi| 31|<br> | wangwu| 41|<br> +——–+———+  </p>
</li>
<li><p>查看”age”大于”30”的数据  </p>
<p> scala&gt; df.filter($”age”&gt;30).show<br> +—+———+<br> |age| username|<br> +—+———+<br> | 40| wangwu|<br> +—+———+  </p>
</li>
<li><p>按照”age”分组，查看数据条数  </p>
<p> scala&gt; df.groupBy(“age”).count.show<br> +—+—–+<br> |age|count|<br> +—+—–+<br> | 20| 1|<br> | 30| 1|<br> | 40| 1|<br> +—+—–+</p>
</li>
</ol>
<h3 id="RDD-转换为-DataFrame"><a href="#RDD-转换为-DataFrame" class="headerlink" title="RDD 转换为 DataFrame"></a>RDD 转换为 DataFrame</h3><p>在 IDEA 中开发程序时，如果需要 RDD 与 DF 或者 DS 之间互相操作，那么需要引入import spark.implicits._   </p>
<p>这里的 spark 不是 Scala 中的包名，而是创建的 sparkSession 对象的变量名称，所以必<br>须先创建 SparkSession 对象再导入。这里的 spark 对象不能使用 var 声明，因为 <strong>Scala 只支持val 修饰的对象的引入</strong>。    </p>
<h4 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h4><pre><code>scala&gt; val idRDD = sc.textFile(&quot;data/id.txt&quot;)
scala&gt; idRDD.toDF(&quot;id&quot;).show
    +---+
    | id|
    +---+
    | 1|
    | 2|
    | 3|
    | 4| 
    +---+  
</code></pre>
<h4 id="通过样例类-RDD-DataFrame"><a href="#通过样例类-RDD-DataFrame" class="headerlink" title="通过样例类 RDD -&gt; DataFrame"></a>通过样例类 RDD -&gt; DataFrame</h4><p>实际开发中，一般通过样例类将 RDD 转换为 DataFrame  </p>
<pre><code>scala&gt; case class User(name:String, age:Int)
defined class User
scala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,40))).map(t=&gt;User(t._1, 
t._2)).toDF.show
    +--------+---+
    | name|age|
    +--------+---+
    |zhangsan| 30|
    | lisi| 40|
    +--------+---+    
</code></pre>
<h3 id="DataFrame-转换为-RDD"><a href="#DataFrame-转换为-RDD" class="headerlink" title="DataFrame 转换为 RDD"></a>DataFrame 转换为 RDD</h3><p>DataFrame 其实就是对 RDD 的封装，所以可以直接获取内部的 RDD  </p>
<h4 id="df-rdd"><a href="#df-rdd" class="headerlink" title="df.rdd"></a>df.rdd</h4><pre><code>scala&gt; val rdd = df.rdd
rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] at rdd at &lt;console&gt;:25  
</code></pre>
<p>注意：此时得到的 RDD 存储类型为 Row   </p>
<h3 id="DataSet-1"><a href="#DataSet-1" class="headerlink" title="DataSet"></a>DataSet</h3><p>DataSet 是具有强类型的数据集合，需要提供对应的类型信息。  </p>
<h4 id="创建-DataSet"><a href="#创建-DataSet" class="headerlink" title="创建 DataSet"></a>创建 DataSet</h4><h5 id="使用样例类序列创建-DataSet"><a href="#使用样例类序列创建-DataSet" class="headerlink" title="使用样例类序列创建 DataSet"></a>使用样例类序列创建 DataSet</h5><pre><code>scala&gt; case class Person(name: String, age: Long)
defined class Person
scala&gt; val caseClassDS = Seq(Person(&quot;zhangsan&quot;,2)).toDS()
caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]
scala&gt; caseClassDS.show
    +---------+---+
    | name|age|
    +---------+---+
    | zhangsan| 2|
    +---------+---+    
</code></pre>
<h5 id="使用基本类型的序列创建-DataSet"><a href="#使用基本类型的序列创建-DataSet" class="headerlink" title="使用基本类型的序列创建 DataSet"></a>使用基本类型的序列创建 DataSet</h5><pre><code>scala&gt; val ds = Seq(1,2,3,4,5).toDS
ds: org.apache.spark.sql.Dataset[Int] = [value: int]  

    scala&gt; ds.show
    +-----+
    |value|
    +-----+
    | 1|
    | 2|
    | 3|
    | 4|
    | 5|
    +-----+    
</code></pre>
<p>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet  </p>
<h3 id="RDD-转换为-DataSet"><a href="#RDD-转换为-DataSet" class="headerlink" title="RDD 转换为 DataSet"></a>RDD 转换为 DataSet</h3><p>SparkSQL 能够自动将包含有 case 类的 RDD 转换成 DataSet，case 类定义了 table 的结构，case 类属性通过反射变成了表的列名。Case 类可以包含诸如 Seq 或者 Array 等复杂的结构。  </p>
<h4 id="toDS"><a href="#toDS" class="headerlink" title="toDS"></a>toDS</h4><pre><code>scala&gt; case class User(name:String, age:Int)
defined class User
scala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).map(t=&gt;User(t._1, t._2)).toDS
res11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
</code></pre>
<h3 id="DataSet-转换为-RDD"><a href="#DataSet-转换为-RDD" class="headerlink" title="DataSet 转换为 RDD"></a>DataSet 转换为 RDD</h3><p>DataSet 其实也是对 RDD 的封装，所以可以直接获取内部的 RDD    </p>
<h4 id="ds-rdd"><a href="#ds-rdd" class="headerlink" title="ds.rdd"></a>ds.rdd</h4><pre><code>scala&gt; val rdd = res11.rdd
rdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at &lt;console&gt;:25
</code></pre>
<h3 id="DataFrame-和-DataSet-转换"><a href="#DataFrame-和-DataSet-转换" class="headerlink" title="DataFrame 和 DataSet 转换"></a>DataFrame 和 DataSet 转换</h3><p>DataFrame 其实是 DataSet 的特例，所以它们之间是可以互相转换的。  </p>
<h4 id="DataFrame-转换为-DataSet"><a href="#DataFrame-转换为-DataSet" class="headerlink" title="DataFrame 转换为 DataSet"></a>DataFrame 转换为 DataSet</h4><h5 id="df-as-样例类"><a href="#df-as-样例类" class="headerlink" title="df.as[样例类]"></a>df.as[样例类]</h5><pre><code>scala&gt; case class User(name:String, age:Int)
defined class User  

scala&gt; val df = sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).toDF(&quot;name&quot;,&quot;age&quot;)
df: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala&gt; val ds = df.as[User]
ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]  
</code></pre>
<h4 id="DataSet-转换为-DataFrame"><a href="#DataSet-转换为-DataFrame" class="headerlink" title="DataSet 转换为 DataFrame"></a>DataSet 转换为 DataFrame</h4><h5 id="ds-toDF"><a href="#ds-toDF" class="headerlink" title="ds.toDF"></a>ds.toDF</h5><pre><code>scala&gt; val ds = df.as[User]
ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]

scala&gt; val df = ds.toDF
df: org.apache.spark.sql.DataFrame = [name: string, age: int]  
</code></pre>
<h3 id="RDD、DataFrame、DataSet-三者的关系"><a href="#RDD、DataFrame、DataSet-三者的关系" class="headerlink" title="RDD、DataFrame、DataSet 三者的关系"></a>RDD、DataFrame、DataSet 三者的关系</h3><p>同样的数据都给到这三个数据结构,计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。    </p>
<h4 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h4><p>都是 spark 平台下的分布式弹性数据集。  </p>
<p>都有惰性机制。  </p>
<p>三者有许多共同的函数，如 filter，排序等。  </p>
<p>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）  </p>
<p>三者都会根据 Spark 的内存情况自动缓存运算。  </p>
<p>三者都有 partition 的概念  </p>
<p>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型   </p>
<h4 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h4><p>RDD 不支持 sparksql 操作。  </p>
<p>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直<br>接访问，只有通过解析才能获取各个字段的值。  </p>
<p>DataFrame 与 DataSet 一般不与 spark mllib 同时使用。  </p>
<p>DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能<br>注册临时表&#x2F;视窗，进行 sql 语句操作。  </p>
<p>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然。    </p>
<p>DataFrame 其实就是 DataSet 的一个特例 type DataFrame &#x3D; Dataset[Row]  </p>
<p>DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了case class 之后可以很自由的获得每一行的信息。      </p>
<p><img src="/2023/08/11/Spark-SQL/3.png" alt="三者的相互转换">  </p>
<h2 id="IDEA开发SparkSQL"><a href="#IDEA开发SparkSQL" class="headerlink" title="IDEA开发SparkSQL"></a>IDEA开发SparkSQL</h2><p>实际开发中，都是使用 IDEA 进行开发的。   </p>
<h3 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h3><pre><code>&lt;dependency&gt;
 &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
 &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;
 &lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>object SparkSQL01_Demo &#123;
 	def main(args: Array[String]): Unit = &#123;
     	//创建上下文环境配置对象
     	val conf: SparkConf = new 
        SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL01_Demo&quot;)
     	//创建 SparkSession 对象
         val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
         //RDD=&gt;DataFrame=&gt;DataSet 转换需要引入隐式转换规则，否则无法转换
         //spark 不是包名，是上下文环境对象名
         import spark.implicits._
         //读取 json 文件 创建 DataFrame &#123;&quot;username&quot;: &quot;lisi&quot;,&quot;age&quot;: 18&#125;
         val df: DataFrame = spark.read.json(&quot;input/test.json&quot;)
         //df.show()
         //SQL 风格语法
         df.createOrReplaceTempView(&quot;user&quot;)
         //spark.sql(&quot;select avg(age) from user&quot;).show
         //DSL 风格语法
         //df.select(&quot;username&quot;,&quot;age&quot;).show()
         //*****RDD=&gt;DataFrame=&gt;DataSet*****
         //RDD
         val rdd1: RDD[(Int, String, Int)] = 
         spark.sparkContext.makeRDD(List((1,&quot;zhangsan&quot;,30),(2,&quot;lisi&quot;,28),(3,&quot;wangwu&quot;,20)))
         //DataFrame
         val df1: DataFrame = rdd1.toDF(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;)
         //df1.show()
         //DateSet
         val ds1: Dataset[User] = df1.as[User]
         //ds1.show()
         //*****DataSet=&gt;DataFrame=&gt;RDD*****
         //DataFrame
         val df2: DataFrame = ds1.toDF()
         //RDD 返回的 RDD 类型为 Row，里面提供的 getXXX 方法可以获取字段值，类似 jdbc 处理结果集，
        但是索引从 0 开始
         val rdd2: RDD[Row] = df2.rdd
         //rdd2.foreach(a=&gt;println(a.getString(1)))
         //*****RDD=&gt;DataSet*****
         rdd1.map&#123;  
        case (id,name,age)=&gt;User(id,name,age)
         &#125;.toDS()
         //*****DataSet=&gt;=&gt;RDD*****
         ds1.rdd
         //释放资源
         spark.stop()
     &#125;
    &#125;
case class User(id:Int,name:String,age:Int)  
</code></pre>
<h3 id="toDF和toDS的用法区别："><a href="#toDF和toDS的用法区别：" class="headerlink" title="toDF和toDS的用法区别："></a>toDF和toDS的用法区别：</h3><p>使用toDF时：  </p>
<pre><code>rdd1.toDF(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;)  指定字段名即可，字段类型会自动解析rdd中的数据进行获取。 
</code></pre>
<p>使用toDS时：  </p>
<pre><code>case class User(id:Int,name:String,age:Int)   
rdd1.toDS  
</code></pre>
<h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><h4 id="创建-DataFrame-1"><a href="#创建-DataFrame-1" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h4><pre><code>scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)
df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]
</code></pre>
<h4 id="注册-UDF"><a href="#注册-UDF" class="headerlink" title="注册 UDF"></a>注册 UDF</h4><pre><code>scala&gt; spark.udf.register(&quot;addName&quot;,(x:String)=&gt; &quot;Name:&quot;+x)
res9: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))
</code></pre>
<h4 id="创建临时表"><a href="#创建临时表" class="headerlink" title="创建临时表"></a>创建临时表</h4><pre><code>scala&gt; df.createOrReplaceTempView(&quot;people&quot;)
</code></pre>
<h4 id="应用-UDF"><a href="#应用-UDF" class="headerlink" title="应用 UDF"></a>应用 UDF</h4><pre><code>scala&gt; spark.sql(&quot;Select addName(name),age from people&quot;).show()   
</code></pre>
<h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><p>用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从 Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以统一采用强类型聚合函数Aggregator。  </p>
<h2 id="数据的加载和保存"><a href="#数据的加载和保存" class="headerlink" title="数据的加载和保存"></a>数据的加载和保存</h2><p>SparkSQL 默认读取和保存的文件格式为 parquet。  </p>
<h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>如果读取不同格式的数据，可以对不同的数据格式进行设定  </p>
<pre><code>scala&gt; spark.read.format(&quot;…&quot;)[.option(&quot;…&quot;)].load(&quot;…&quot;)  

➢ format(&quot;…&quot;)：指定加载的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。
➢ load(&quot;…&quot;)：在&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入加载数据的路径。
➢ option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable     
</code></pre>
<p>也可以直接在文件上进行查询: 文件格式.<code>文件路径</code></p>
<pre><code>scala&gt;spark.sql(&quot;select * from json.`/opt/module/data/user.json`&quot;).show  
</code></pre>
<h3 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h3><p>df.write.save 是保存数据的通用方法  </p>
<pre><code>scala&gt;df.write.format(&quot;…&quot;)[.option(&quot;…&quot;)].save(&quot;…&quot;)  

➢ format(&quot;…&quot;)：指定保存的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。
➢ save (&quot;…&quot;)：在&quot;csv&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入保存数据的路径。
➢ option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable  
</code></pre>
<p>保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。  </p>
<pre><code>df.write.mode(&quot;append&quot;).json(&quot;/opt/module/data/output&quot;)   
</code></pre>
<p><img src="/2023/08/11/Spark-SQL/4.png" alt="SaveMode枚举值">    </p>
<h3 id="修改默认数据源格式"><a href="#修改默认数据源格式" class="headerlink" title="修改默认数据源格式"></a>修改默认数据源格式</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式<br>存储格式。</p>
<p>修改配置项 spark.sql.sources.default，可修改默认数据源格式。   </p>
<p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以<br>通过 SparkSession.read.json()去加载 JSON 文件。  </p>
<p>注意：Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串 </p>
<pre><code>&#123;&quot;name&quot;:&quot;Michael&quot;&#125;
&#123;&quot;name&quot;:&quot;Andy&quot;， &quot;age&quot;:30&#125;
[&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;,&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;]  
</code></pre>
<h3 id="Spark读取本地Json文件的案例"><a href="#Spark读取本地Json文件的案例" class="headerlink" title="Spark读取本地Json文件的案例"></a>Spark读取本地Json文件的案例</h3><h4 id="导入隐式转换"><a href="#导入隐式转换" class="headerlink" title="导入隐式转换"></a>导入隐式转换</h4><pre><code>import spark.implicits._
</code></pre>
<h4 id="加载-JSON-文件"><a href="#加载-JSON-文件" class="headerlink" title="加载 JSON 文件"></a>加载 JSON 文件</h4><pre><code>val path = &quot;/opt/module/spark-local/people.json&quot;
val peopleDF = spark.read.json(path)  
</code></pre>
<h4 id="创建临时表-1"><a href="#创建临时表-1" class="headerlink" title="创建临时表"></a>创建临时表</h4><pre><code>peopleDF.createOrReplaceTempView(&quot;people&quot;)
</code></pre>
<h4 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h4><pre><code>val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)  
teenagerNamesDF.show()
    +------+
    | name|
    +------+
    |Justin|
    +------+  
</code></pre>
<h3 id="Spark读取本地CSV文件的案例"><a href="#Spark读取本地CSV文件的案例" class="headerlink" title="Spark读取本地CSV文件的案例"></a>Spark读取本地CSV文件的案例</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为<br>数据列。  </p>
<pre><code>spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;;&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).load(&quot;data/user.csv&quot;)  
</code></pre>
<h3 id="Spark通过JDBC连接Mysql的案例"><a href="#Spark通过JDBC连接Mysql的案例" class="headerlink" title="Spark通过JDBC连接Mysql的案例"></a>Spark通过JDBC连接Mysql的案例</h3><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对<br>DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用 spark-shell 操作，可在启动 shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下。  </p>
<pre><code>bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar  
</code></pre>
<p>在 Idea 中通过 JDBC 对 Mysql 进行操作的案例代码如下  </p>
<h4 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h4><pre><code>&lt;dependency&gt;
 	&lt;groupId&gt;mysql&lt;/groupId&gt;
 	&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
 	&lt;version&gt;5.1.27&lt;/version&gt;
&lt;/dependency&gt;  
</code></pre>
<h4 id="读取数据-（选用方式一）"><a href="#读取数据-（选用方式一）" class="headerlink" title="读取数据 （选用方式一）"></a>读取数据 （选用方式一）</h4><pre><code>val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)
//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
import spark.implicits._      

//方式 1：通用的 load 方法读取
spark.read.format(&quot;jdbc&quot;)
.option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;)
.option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)
.option(&quot;user&quot;, &quot;root&quot;)
.option(&quot;password&quot;, &quot;123123&quot;)
.option(&quot;dbtable&quot;, &quot;user&quot;)
.load().show  


//方式 2:通用的 load 方法读取 参数另一种形式
spark.read.format(&quot;jdbc&quot;)
    .options(Map(&quot;url&quot;-&gt;&quot;jdbc:mysql://linux1:3306/spark-sql?user=root&amp;password=123123&quot;,
    &quot;dbtable&quot;-&gt;&quot;user&quot;,&quot;driver&quot;-&gt;&quot;com.mysql.jdbc.Driver&quot;)).load().show

//方式 3:使用 jdbc 方法读取
val props: Properties = new Properties()
props.setProperty(&quot;user&quot;, &quot;root&quot;)
props.setProperty(&quot;password&quot;, &quot;123123&quot;)
val df: DataFrame = spark.read.jdbc(&quot;jdbc:mysql://linux1:3306/spark-sql&quot;, 
&quot;user&quot;, props)
df.show  

//释放资源
spark.stop()    
</code></pre>
<h4 id="写入数据-选用方式一"><a href="#写入数据-选用方式一" class="headerlink" title="写入数据  (选用方式一)"></a>写入数据  (选用方式一)</h4><pre><code>case class User2(name: String, age: Long)
。。。
val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)
//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()  
import spark.implicits._    

val rdd: RDD[User2] = spark.sparkContext.makeRDD(List(User2(&quot;lisi&quot;, 20), User2(&quot;zs&quot;, 30)))
val ds: Dataset[User2] = rdd.toDS    

//方式 1：通用的方式 format 指定写出类型

ds.write
.format(&quot;jdbc&quot;)
.option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;)
.option(&quot;user&quot;, &quot;root&quot;)
.option(&quot;password&quot;, &quot;123123&quot;)
.option(&quot;dbtable&quot;, &quot;user&quot;)
.mode(SaveMode.Append)
.save()


//方式 2：通过 jdbc 方法
val props: Properties = new Properties()
props.setProperty(&quot;user&quot;, &quot;root&quot;)
props.setProperty(&quot;password&quot;, &quot;123123&quot;)
ds.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql://linux1:3306/spark-sql&quot;, &quot;user&quot;, props)    

//释放资源  
spark.stop() 
</code></pre>
<h4 id="使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作"><a href="#使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作" class="headerlink" title="使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作"></a>使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作</h4><pre><code>val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)
//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
import spark.implicits._      

//方式 1：通用的 load 方法读取
res1 = spark.read.format(&quot;jdbc&quot;)
.option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;)
.option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)
.option(&quot;user&quot;, &quot;root&quot;)
.option(&quot;password&quot;, &quot;123123&quot;)
.option(&quot;dbtable&quot;, &quot;user&quot;)
.load()     

res2 = spark.sql(&quot;select * from user1 where age &gt; 10&quot;)

res2.write
.format(&quot;jdbc&quot;)
.option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;)
.option(&quot;user&quot;, &quot;root&quot;)
.option(&quot;password&quot;, &quot;123123&quot;)
.option(&quot;dbtable&quot;, &quot;user&quot;)
.mode(SaveMode.Append)
.save() 


//释放资源  
spark.stop()   
</code></pre>
<h3 id="Spark操作Hive"><a href="#Spark操作Hive" class="headerlink" title="Spark操作Hive"></a>Spark操作Hive</h3><p>若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到<br>Spark 的配置文件目录中($SPARK_HOME&#x2F;conf)。   </p>
<p>需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 &#x2F;user&#x2F;hive&#x2F;warehouse 目录中(如果你的 classpath 中有配好的hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。   </p>
<p>spark-shell 默认是 Hive 支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。    </p>
<p>在实际使用中, 几乎没有任何人会使用内置的 Hive    </p>
<h4 id="Spark访问外部Hive的前置条件"><a href="#Spark访问外部Hive的前置条件" class="headerlink" title="Spark访问外部Hive的前置条件"></a>Spark访问外部Hive的前置条件</h4><p>如果想连接外部已经部署好的 Hive，需要通过以下几个步骤：  </p>
<pre><code>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下
➢ 把 Mysql 的驱动 copy 到 jars/目录下
➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下
➢ 重启 spark-shell   
</code></pre>
<h4 id="Spark-shell中访问Hive"><a href="#Spark-shell中访问Hive" class="headerlink" title="Spark-shell中访问Hive"></a>Spark-shell中访问Hive</h4><pre><code>scala&gt; spark.sql(&quot;show tables&quot;).show   
</code></pre>
<h4 id="运行-Spark-SQL-CLI"><a href="#运行-Spark-SQL-CLI" class="headerlink" title="运行 Spark SQL CLI"></a>运行 Spark SQL CLI</h4><p>Spark SQL CLI 可以很方便的在本地运行 Hive 元数据服务以及从命令行执行查询任务。在Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一 Hive 窗口    </p>
<pre><code>bin/spark-sql    
</code></pre>
<h4 id="运行-Spark-beeline"><a href="#运行-Spark-beeline" class="headerlink" title="运行 Spark beeline"></a>运行 Spark beeline</h4><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。Spark Thrift Server 的目的也只是取代HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。<br>如果想连接 Thrift Server，需要通过以下几个步骤：  </p>
<pre><code>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下	
➢ 把 Mysql 的驱动 copy 到 jars/目录下
➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下
➢ 启动 Thrift Server    

sbin/start-thriftserver.sh      
</code></pre>
<h5 id="使用-beeline-连接-Thrift-Server"><a href="#使用-beeline-连接-Thrift-Server" class="headerlink" title="使用 beeline 连接 Thrift Server"></a>使用 beeline 连接 Thrift Server</h5><pre><code>bin/beeline -u jdbc:hive2://linux1:10000 -n root  
</code></pre>
<h4 id="Spark操作Hive的代码示例"><a href="#Spark操作Hive的代码示例" class="headerlink" title="Spark操作Hive的代码示例"></a>Spark操作Hive的代码示例</h4><h5 id="导入依赖-1"><a href="#导入依赖-1" class="headerlink" title="导入依赖"></a>导入依赖</h5><pre><code>&lt;dependency&gt;
     &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
     &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;
     &lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
     &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
     &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
     &lt;version&gt;1.2.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
     &lt;groupId&gt;mysql&lt;/groupId&gt;
     &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
     &lt;version&gt;5.1.27&lt;/version&gt;
&lt;/dependency&gt;  
</code></pre>
<h5 id="拷贝Hive-Site-xml"><a href="#拷贝Hive-Site-xml" class="headerlink" title="拷贝Hive-Site.xml"></a>拷贝Hive-Site.xml</h5><p>将 hive-site.xml 文件拷贝到项目的 resources 目录中</p>
<h5 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h5><pre><code>//创建 SparkSession
val spark: SparkSession = SparkSession
.builder()
.enableHiveSupport()
.master(&quot;local[*]&quot;)	
.appName(&quot;sql&quot;)
.getOrCreate()  
</code></pre>
<p>在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址:<br>config(“spark.sql.warehouse.dir”, “hdfs:&#x2F;&#x2F;linux1:8020&#x2F;user&#x2F;hive&#x2F;warehouse”)</p>
<p>代码最前面增加如下代码解决权限不足的问题：  </p>
<p>System.setProperty(“HADOOP_USER_NAME”, “root”)</p>
<p>此处的 root 改为你们自己的 hadoop 用户名称      </p>
<h5 id="整理后代码实现"><a href="#整理后代码实现" class="headerlink" title="整理后代码实现"></a>整理后代码实现</h5><pre><code>//创建 SparkSession
System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)
val spark: SparkSession = SparkSession
.builder()
.config(&quot;spark.sql.warehouse.dir&quot;, &quot;hdfs://linux1:8020/user/hive/warehouse&quot;)
.enableHiveSupport()
.master(&quot;local[*]&quot;)	
.appName(&quot;show databases&quot;)
.getOrCreate() 
</code></pre>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/07/Java_datastrcut/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/07/Java_datastrcut/" class="post-title-link" itemprop="url">Java数据结构和算法</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-07 11:10:06" itemprop="dateCreated datePublished" datetime="2023-08-07T11:10:06+08:00">2023-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:02:31" itemprop="dateModified" datetime="2023-08-11T13:02:31+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="君子藏器于身，待时而动"><a href="#君子藏器于身，待时而动" class="headerlink" title="君子藏器于身，待时而动"></a>君子藏器于身，待时而动</h1><h2 id="线性结构和非线性结构"><a href="#线性结构和非线性结构" class="headerlink" title="线性结构和非线性结构"></a>线性结构和非线性结构</h2><h3 id="线性结构"><a href="#线性结构" class="headerlink" title="线性结构"></a>线性结构</h3><p>线性结构作为最常用的数据结构，其特点是数据元素之间存在一对一的线性关系  </p>
<p>线性结构有两种不同的存储结构，即顺序存储结构和链式存储结构。顺序存储的线性表称为顺序表，顺序表中的存储元素是连续的  </p>
<p>链式存储的线性表称为链表，链表中的存储元素不一定是连续的，元素节点中存放数据元素以及相邻元素的地址信息  </p>
<p>线性结构常见的有：数组、队列、链表和栈，后面我们会详细讲解   </p>
<p>非线性结构包括：二维数组，多维数组，广义表，树结构，图结构  </p>
<h1 id="稀疏数组"><a href="#稀疏数组" class="headerlink" title="稀疏数组"></a>稀疏数组</h1><p>当一个数组中大部分元素为０，或者为同一个值的数组时，可以使用稀疏数组来保存该数组。  </p>
<p>稀疏数组的处理方法是:  </p>
<p>记录数组一共有几行几列，有多少个不同的值  </p>
<p>把具有不同值的元素的行列及值记录在一个小规模的数组中，从而缩小程序的规模  </p>
<p><img src="/2023/08/07/Java_datastrcut/1.png" alt="稀疏数组">   </p>
<p><img src="/2023/08/07/Java_datastrcut/2.png" alt="稀疏数组转换思路">    </p>
<p>二维表转稀疏数组代码实现：    </p>
<pre><code>package com.zyy;

public class SparseArray &#123;

    public static void main(String[] args)&#123;
        //创建一个原始的二维数组 11*11
        // 0: 表示没有棋子，1表示黑子 2表示蓝子
        int chessArr1[][] = new int[11][11];
        chessArr1[1][2] = 1; //第二行第三列 有一颗黑子
        chessArr1[2][3] = 2; //第三行第四列 有一颗蓝子
        chessArr1[4][5] = 2; //第五行第六列 有一颗蓝子
        //输出原始的二维数组
        System.out.println(&quot;原始的二维数组~~&quot;);
        //从二维数组中拿出每一行数据，返回为一维数组int[] row
        for(int[] row:chessArr1) &#123;
            //从拿到的每一行数据中拿到每一个值
            for (int data:row)&#123;
                System.out.printf(&quot;%d\t&quot;,data);
            &#125;
            System.out.println();
        &#125;

        // 将二维数组转稀疏数组的思想
        //1.先遍历二维数组，得到非0数据的个数
        int sum = 0;
        for (int i = 0; i &lt; 11; i++)&#123;
            for (int j = 0;j &lt; 11; j++)&#123;
                if (chessArr1[i][j] != 0 )&#123;
                    sum++;
                &#125;
            &#125;
        &#125;

        //2.创建对应的稀疏数组
        //由统计出来的非0数个数+1，构成稀疏数组的行数
        //稀疏数组的列数固定为3，记录行坐标，列坐标，值
        int sparseArr[][] = new int[sum+1][3];
        //给稀疏数组赋值
        sparseArr[0][0] = 11;
        sparseArr[0][1] = 11;
        sparseArr[0][2] = sum;
        //遍历二维数组，将非0的值存放到sparseArr中
        int count = 0;  //count用于记录是第几个非0数据
        for(int i = 0;i &lt; 11; i++)&#123;
            for (int j = 0 ;j &lt; 11; j++)&#123;
                if(chessArr1[i][j] != 0 )&#123;
                    count++;
                    //记录第count个非0数据的行i
                    sparseArr[count][0] = i;
                    //记录第count个非0数据的列j
                    sparseArr[count][1] = j;
                    //记录第count个非0数据的值
                    sparseArr[count][2] = chessArr1[i][j];
                &#125;
            &#125;
        &#125;

        //输出稀疏数组的形式
        System.out.println();
        System.out.println(&quot;得到稀疏数组为~~~&quot;);
        //二维数组结构为[[数组1],[数组2],[数组3]]
        //所以sparseArr.length实际上是在统计外层一维数组的长度
        for(int i = 0; i &lt; sparseArr.length;i++)&#123;
            System.out.printf(&quot;%d\t%d\t%d\t\n&quot;,sparseArr[i][0],sparseArr[i][1],sparseArr[i][2]);
        &#125;

        System.out.println();

        //将稀疏数组 -》 恢复成 原始的二维数组

        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组，比如上面的chessArr2 = int[11][11]
        //2.在读取稀疏数组后几行的数据，并赋值给原始的二维数组，即可

        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组

        int chessArr2[][] = new int[sparseArr[0][0]][sparseArr[0][1]];

        //2.在读取稀疏数组后几行的数据（从第二行开始），并赋值给原始的二维数组即可
        for (int i = 1; i &lt; sparseArr.length; i++)&#123;
            chessArr2[sparseArr[i][0]][sparseArr[i][1]] = sparseArr[i][2];
        &#125;

        //输出恢复后的二维数组
        System.out.println();
        System.out.println(&quot;恢复后的二维数组&quot;);

        for (int[] row:chessArr2)&#123;
            for(int data:row)&#123;
                System.out.printf(&quot;%d\t&quot;,data);
            &#125;
            //每行数据打印完之后，执行换行
            System.out.println();
        &#125;
    &#125;
&#125;


原始的二维数组~~
0	0	0	0	0	0	0	0	0	0	0	
0	0	1	0	0	0	0	0	0	0	0	
0	0	0	2	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	2	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	

得到稀疏数组为~~~
11	11	3	
1	2	1	
2	3	2	
4	5	2	


恢复后的二维数组
0	0	0	0	0	0	0	0	0	0	0	
0	0	1	0	0	0	0	0	0	0	0	
0	0	0	2	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	2	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
0	0	0	0	0	0	0	0	0	0	0	
</code></pre>
<h1 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h1><p>队列是一个有序列表，可以用数组或者链表来实现。遵循先进先出原则（FIFO）</p>
<p><img src="/2023/08/07/Java_datastrcut/3.png" alt="数组模拟队列">  </p>
<h2 id="数组模拟队列"><a href="#数组模拟队列" class="headerlink" title="数组模拟队列"></a>数组模拟队列</h2><pre><code>package com.zyy;
import java.util.Scanner;

public class ArrayQueueDemo &#123;

public static void main(String[] args) &#123;

    // 测试一把
    // 创建一个队列
    ArrayQueue queue = new ArrayQueue(5);
    char key ;//接收用户输入
    Scanner scanner = new Scanner(System.in);
    boolean loop = true;
    //输出一个菜单
    while(loop)&#123;
        System.out.println(&quot;s(show):显示队列&quot;);
        System.out.println(&quot;e(exit):退出程序&quot;);
        System.out.println(&quot;a(add):添加数据到队列&quot;);
        System.out.println(&quot;g(get):从队列取出数据&quot;);
        System.out.println(&quot;h(head):查看队列头的数据&quot;);
        key = scanner.next().charAt(0);//接收一个字符
        switch(key)&#123;
            case &#39;s&#39;:
                queue.showQueue();
                break;
            case &#39;a&#39;:
                System.out.println(&quot;输入一个数&quot;);
                int value = scanner.nextInt();
                queue.addQueue(value);
                break;
            case &#39;g&#39;://取出数据
                try&#123;
                    int res = queue.getQueue();
                    System.out.printf(&quot;取出的数据是%d\n&quot;,res);
                &#125;catch (Exception e)&#123;
                    //TODO:handle exception
                    System.out.println(e.getMessage());
                &#125;
                break;
            case &#39;h&#39;://查看队列头的数据
                try&#123;
                    int res = queue.headQueue();
                    System.out.printf(&quot;队列头的数据是%d\n&quot;,res);
                &#125;catch(Exception e)&#123;
                    //TODO:handle exception
                    System.out.println(e.getMessage());
                &#125;
                break;
            case &#39;e&#39;://退出
                scanner.close();
                loop = false;
                break;
            default:
                break;
        &#125;
    &#125;
    System.out.println(&quot;程序退出~~&quot;);
&#125;

    //使用数组模拟队列-编写一个ArrayQueue类
static class ArrayQueue&#123;
        private int maxSize;// 表示数组的最大容量
        private int front;//队列头
        private int rear;//队列尾
        private int[] arr;//该数据用于存放数据，模拟队列

        //创建队列的构造器
        public ArrayQueue(int arrMaxSize)&#123;
            maxSize = arrMaxSize;
            arr = new int[maxSize];
            front = -1;//指向队列头部，分析出front是指向队列头的前一个位置
            rear = -1;//指向队列尾，指向队列尾的数据（即就是队列最后一个数据）
        &#125;

        //判断队列是否满
        public boolean isFull()&#123;
            return rear == maxSize - 1;
        &#125;

        //判断队列是否为空
        public boolean isEmpty()&#123;
            return rear == front;
        &#125;

        //添加数据到队列
        public void addQueue(int n)&#123;
            //判断队列是否满
            if(isFull())&#123;
                System.out.println(&quot;队列满，不能加入数据~&quot;);
                return;
            &#125;
            rear++;//让rear后移
            arr[rear] = n;
        &#125;

        //获取队列的数据，出队列
        public int getQueue()&#123;
            //判断队列是否为空
            if(isEmpty())&#123;
                //通过抛出异常
                throw new RuntimeException(&quot;队列空，不能取数据&quot;);
            &#125;
            front++;//front后移
            return arr[front];
        &#125;

        //显示队列的所有数据
        public void showQueue()&#123;
            //遍历
            if(isEmpty())&#123;
                System.out.println(&quot;队列空的，没有数据~~&quot;);
                return;
            &#125;
            for(int i=0;i&lt;arr.length;i++)&#123;
                System.out.printf(&quot;arr[%d]=%d\n&quot;,i,arr[i]);
            &#125;
        &#125;

        //显示队列的头数据，注意不是取出数据
        public int headQueue()&#123;
            //判断
            if(isEmpty())&#123;
                throw new RuntimeException(&quot;队列空的，没有数据~~&quot;);
            &#125;
            return arr[front + 1];
        &#125;

    &#125;

&#125;
</code></pre>
<h3 id="使用数组模拟队列存在的问题："><a href="#使用数组模拟队列存在的问题：" class="headerlink" title="使用数组模拟队列存在的问题："></a>使用数组模拟队列存在的问题：</h3><p>数组只能使用一次，因为front和rear指针无法再回头指向已经走过的数组位置  </p>
<h3 id="优化方案："><a href="#优化方案：" class="headerlink" title="优化方案："></a>优化方案：</h3><p>通过取模运算，让front和rear指针能循环指向已经走过的数组位置，让数组复用  </p>
<p>分析说明:  </p>
<p>1):尾索引的下一个为头索引时表示队列满，即将队列容量空出一个作为约定，这个在做判断队列满的时候需要注意（rear+1）%maxSize &#x3D;&#x3D; front 满</p>
<p>2):rear &#x3D;&#x3D; front(空)</p>
<p>3):与数组模拟队列不同，数组模拟环形队列时，front指向队列的第一个元素，front的初始值为0 ，rear指向队列的最后一个元素的最后一个位置，因为希望空出一个空间作为约定，rear的初始值为0</p>
<p>4):队列中的有效数据个数计算方法:  </p>
<p>(rear+maxSize-front)%maxSize</p>
<p><img src="/2023/08/07/Java_datastrcut/4.png" alt="循环队列相关判断条件"></p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/06/hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/06/hadoop/" class="post-title-link" itemprop="url">hadoop学习笔记</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-06 10:45:20" itemprop="dateCreated datePublished" datetime="2023-08-06T10:45:20+08:00">2023-08-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:09:37" itemprop="dateModified" datetime="2023-08-11T13:09:37+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="博观而约取，厚积而薄发"><a href="#博观而约取，厚积而薄发" class="headerlink" title="博观而约取，厚积而薄发"></a>博观而约取，厚积而薄发</h1><p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA">https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA</a><br>提取码：mvcs   </p>
<h1 id="免密登录原理"><a href="#免密登录原理" class="headerlink" title="免密登录原理"></a>免密登录原理</h1><p><img src="/2023/08/06/hadoop/4.png" alt="免密登录原理">    </p>
<h1 id="HDFS架构概述"><a href="#HDFS架构概述" class="headerlink" title="HDFS架构概述"></a>HDFS架构概述</h1><p><img src="/2023/08/06/hadoop/1.png" alt="HDFS架构概述">  </p>
<p>HDFS适合一次写入，多次读出的场景，且不支持文件的修改  </p>
<p>HDFS的缺点：仅支持数据append，不支持文件的随机修改  </p>
<p><img src="/2023/08/06/hadoop/5.png" alt="HDFS组成架构">  </p>
<p><img src="/2023/08/06/hadoop/6.png" alt="HDFS组成架构">   </p>
<p>HDFS文件块大小：在Hadoop2.x版本中是128M  </p>
<p>寻址时间为传输时间的1%时，是最佳状态  </p>
<p>HDFS块的大小设置主要取决于磁盘传输速率  </p>
<h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><p><img src="/2023/08/06/hadoop/7.png" alt="HDFS写数据流程">    </p>
<p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据    </p>
<h3 id="HDFS副本节点选择"><a href="#HDFS副本节点选择" class="headerlink" title="HDFS副本节点选择"></a>HDFS副本节点选择</h3><p><img src="/2023/08/06/hadoop/8.png" alt="HDFS副本节点选择">       </p>
<h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><p><img src="/2023/08/06/hadoop/9.png" alt="HDFS读数据流程">    </p>
<h1 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h1><p>NameNode中的元数据是存储在内存中，在内存上维护一个Edits文件，磁盘上维护一个FsImage文件，每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据  </p>
<p>需要定期进行FsImage和Edits的合并，由SecondaryNamenode完成，专门负责FsImage和Edits的合并  </p>
<h2 id="NameNode工作机制"><a href="#NameNode工作机制" class="headerlink" title="NameNode工作机制"></a>NameNode工作机制</h2><p><img src="/2023/08/06/hadoop/10.png" alt="NameNode工作机制">    </p>
<h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录    </p>
<p>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中  </p>
<p><img src="/2023/08/06/hadoop/11.png" alt="集群安全模式">    </p>
<p>（1）bin&#x2F;hdfs dfsadmin -safemode get		（功能描述：查看安全模式状态）  </p>
<p>（2）bin&#x2F;hdfs dfsadmin -safemode enter  	（功能描述：进入安全模式状态）  </p>
<p>（3）bin&#x2F;hdfs dfsadmin -safemode leave	（功能描述：离开安全模式状态）  </p>
<p>（4）bin&#x2F;hdfs dfsadmin -safemode wait	（功能描述：等待安全模式状态）  </p>
<h2 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h2><p><img src="/2023/08/06/hadoop/12.png" alt="DataNode工作机制">   </p>
<h3 id="DataNode如何保证数据完整性？"><a href="#DataNode如何保证数据完整性？" class="headerlink" title="DataNode如何保证数据完整性？"></a>DataNode如何保证数据完整性？</h3><p>1）当DataNode读取Block的时候，它会计算CheckSum。  </p>
<p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。  </p>
<p>3）Client读取其他DataNode上的Block。  </p>
<p>4）DataNode在其文件创建后周期验证CheckSum    </p>
<p><img src="/2023/08/06/hadoop/13.png" alt="DataNode数据完整性">    </p>
<p>HDFS中默认DataNode掉线的超时时长为10分钟+30秒    </p>
<h3 id="DataNode配置多目录"><a href="#DataNode配置多目录" class="headerlink" title="DataNode配置多目录"></a>DataNode配置多目录</h3><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本   </p>
<p>hdfs-site.xml  </p>
<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<h2 id="HDFS-HA故障转移机制"><a href="#HDFS-HA故障转移机制" class="headerlink" title="HDFS-HA故障转移机制"></a>HDFS-HA故障转移机制</h2><p><img src="/2023/08/06/hadoop/14.png" alt="HDFS-HA故障转移机制">   </p>
<h1 id="YARN架构概述"><a href="#YARN架构概述" class="headerlink" title="YARN架构概述"></a>YARN架构概述</h1><p><img src="/2023/08/06/hadoop/2.png" alt="YARN架构概述">    </p>
<p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序  </p>
<p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成</p>
<h2 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h2><p><img src="/2023/08/06/hadoop/25.png" alt="Yarn工作机制">    </p>
<p>工作机制详解  </p>
<pre><code>（1）MR程序提交到客户端所在的节点。  
（2）YarnRunner向ResourceManager申请一个Application。  
（3）RM将该应用程序的资源路径返回给YarnRunner。
（4）该程序将运行所需资源提交到HDFS上。
（5）程序资源提交完毕后，申请运行mrAppMaster。
（6）RM将用户的请求初始化成一个Task。
（7）其中一个NodeManager领取到Task任务。
（8）该NodeManager创建容器Container，并产生MRAppmaster。
（9）Container从HDFS上拷贝资源到本地。
（10）MRAppmaster向RM 申请运行MapTask资源。
（11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。
（12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。
（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。  
（14）ReduceTask向MapTask获取相应分区的数据。
（15）程序运行完毕后，MR会向RM申请注销自己
</code></pre>
<p>总结：程序提交之后，找RM申请Application,告知运行程序需要的资源。RM生成一个资源分配Task任务放进yarn队列。这个Task会随机分配给Nodemanager，NodeManager领取任务后会根据资源要求创建Container容器，开始运行程序，运行完毕后向RM报告，注销资源占用和Task任务。  </p>
<h3 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h3><p>1.先进先出调度器（FIFO）  </p>
<p>2.容量调度器（Capacity Scheduler）  </p>
<p>3．公平调度器（Fair Scheduler）  </p>
<h1 id="MapReduce架构概述"><a href="#MapReduce架构概述" class="headerlink" title="MapReduce架构概述"></a>MapReduce架构概述</h1><p><img src="/2023/08/06/hadoop/3.png" alt="MapReduce架构概述">     </p>
<h2 id="MapReduce核心思想"><a href="#MapReduce核心思想" class="headerlink" title="MapReduce核心思想"></a>MapReduce核心思想</h2><p><img src="/2023/08/06/hadoop/15.png" alt="MapReduce核心思想">   </p>
<p>MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行  </p>
<h3 id="MapTask并行度决定机制"><a href="#MapTask并行度决定机制" class="headerlink" title="MapTask并行度决定机制"></a>MapTask并行度决定机制</h3><p>数据块：Block是HDFS物理上把数据分成一块一块。  </p>
<p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储  </p>
<p><img src="/2023/08/06/hadoop/16.png" alt="MapTask并行度决定机制">  </p>
<h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><p><img src="/2023/08/06/hadoop/17.png" alt="MapReduce工作流程">  </p>
<p><img src="/2023/08/06/hadoop/18.png" alt="MapReduce工作流程">  </p>
<p>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快,默认100M   </p>
<h2 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h2><p><img src="/2023/08/06/hadoop/19.png" alt="Shuffle机制">    </p>
<h2 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h2><p><img src="/2023/08/06/hadoop/20.png" alt="MapTask工作机制">    </p>
<h2 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a>ReduceTask工作机制</h2><p><img src="/2023/08/06/hadoop/21.png" alt="ReduceTask工作机制">  </p>
<p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置  </p>
<h1 id="Hadoop文件压缩"><a href="#Hadoop文件压缩" class="headerlink" title="Hadoop文件压缩"></a>Hadoop文件压缩</h1><p><img src="/2023/08/06/hadoop/22.png" alt="Hadoop文件压缩">  </p>
<h3 id="Hadoop文件压缩性能比较"><a href="#Hadoop文件压缩性能比较" class="headerlink" title="Hadoop文件压缩性能比较"></a>Hadoop文件压缩性能比较</h3><p><img src="/2023/08/06/hadoop/23.png" alt="Hadoop文件压缩性能比较">  </p>
<p>Gzip压缩：每个文件压缩后都在130M以内的（一个块大小内），都可以考虑Gzip压缩格式  ，不支持Split  </p>
<p>Bzip压缩：支持Split，压缩率很高，但压缩&#x2F;解压缩 速度慢  </p>
<p>LZO压缩：支持Split，合理的压缩率，是Hadoop中最流行的压缩格式    </p>
<p>Snappy压缩：不支持Split，压缩率比Gzip低，但Hadoop本身不支持，需要安装  </p>
<h3 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h3><p><img src="/2023/08/06/hadoop/24.png" alt="压缩位置选择">   </p>
<p>压缩可以在MapReduce作用的任意阶段启用    </p>
<p>速度是最优先考虑的因素，而不是压缩率  </p>
<h1 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h1><p>是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。   </p>
<p>ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功 能稳定的系统提供给用户    </p>
<h2 id="Zookeeper工作机制"><a href="#Zookeeper工作机制" class="headerlink" title="Zookeeper工作机制"></a>Zookeeper工作机制</h2><p><img src="/2023/08/06/hadoop/26.png" alt="Zookeeper工作机制">  </p>
<h3 id="Zookeeper特点"><a href="#Zookeeper特点" class="headerlink" title="Zookeeper特点"></a>Zookeeper特点</h3><p><img src="/2023/08/06/hadoop/27.png" alt="Zookeeper特点">    </p>
<h3 id="Zookeeper的数据结构"><a href="#Zookeeper的数据结构" class="headerlink" title="Zookeeper的数据结构"></a>Zookeeper的数据结构</h3><p><img src="/2023/08/06/hadoop/28.png" alt="Zookeeper的数据结构">    </p>
<h3 id="软负载均衡"><a href="#软负载均衡" class="headerlink" title="软负载均衡"></a>软负载均衡</h3><p>在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求  </p>
<h2 id="选举机制（面试重点）"><a href="#选举机制（面试重点）" class="headerlink" title="选举机制（面试重点）"></a>选举机制（面试重点）</h2><p><img src="/2023/08/06/hadoop/29.png" alt="Zookeeper选举机制-第一次启动">    </p>
<p><img src="/2023/08/06/hadoop/30.png" alt="Zookeeper选举机制-非第一次启动">   </p>
<h3 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h3><p>1）启动客户端    </p>
<pre><code>[atguigu@hadoop102 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop102:2181
</code></pre>
<p>2）显示所有操作命令  </p>
<pre><code>[zk: hadoop102:2181(CONNECTED) 1] help  
</code></pre>
<h4 id="znode-节点数据信息"><a href="#znode-节点数据信息" class="headerlink" title="znode 节点数据信息"></a>znode 节点数据信息</h4><p>1）查看当前znode中所包含的内容   </p>
<pre><code>[zk: hadoop102:2181(CONNECTED) 0] ls /  
</code></pre>
<p>2）查看当前节点详细数据  </p>
<pre><code>[zk: hadoop102:2181(CONNECTED) 5] ls -s /
[zookeeper]cZxid = 0x0
ctime = Thu Jan 01 08:00:00 CST 1970
mZxid = 0x0
mtime = Thu Jan 01 08:00:00 CST 1970
pZxid = 0x0
cversion = -1
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 1    
</code></pre>
<p>（1）czxid：创建节点的事务 zxid  </p>
<p>每次修改 ZooKeeper 状态都会产生一个 ZooKeeper 事务 ID。事务ID是 ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1在zxid2之前发生。  </p>
<p>（2）ctime：znode 被创建的毫秒数（从 1970 年开始）  </p>
<p>（3）mzxid：znode 最后更新的事务 zxid  </p>
<p>（4）mtime：znode 最后修改的毫秒数（从 1970 年开始）  </p>
<p>（5）pZxid：znode 最后更新的子节点 zxid    </p>
<p>（6）cversion：znode 子节点变化号，znode子节点修改次数  </p>
<p>（7）dataversion：znode 数据变化号  </p>
<p>（8）aclVersion：znode 访问控制列表的变化号  </p>
<p>（9）ephemeralOwner：如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。  </p>
<p>（10）dataLength：znode 的数据长度  </p>
<p>（11）numChildren：znode 子节点数量  </p>
<h4 id="节点类型（持久-短暂-有序号-无序号）"><a href="#节点类型（持久-短暂-有序号-无序号）" class="headerlink" title="节点类型（持久&#x2F;短暂&#x2F;有序号&#x2F;无序号）"></a>节点类型（持久&#x2F;短暂&#x2F;有序号&#x2F;无序号）</h4><p><img src="/2023/08/06/hadoop/31.png" alt="Zookeeper节点类型">   </p>
<p>1）分别创建2个普通节点（永久节点 + 不带序号）   </p>
<pre><code>[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;diaochan&quot; 
注意：创建节点时，要赋值  
</code></pre>
<p>2）获得节点的值  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 5] get -s /sanguo  
</code></pre>
<p>3）创建带序号的节点（永久节点 + 带序号）  </p>
<p>（1）先创建一个普通的根节点&#x2F;sanguo&#x2F;weiguo    </p>
<pre><code>[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot;
</code></pre>
<p>（2）创建带序号的节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/zhangliao &quot;zhangliao&quot;  
</code></pre>
<p>如果原来没有序号节点，序号从 0 开始依次递增。如果原节点下已有 2 个节点，则再排序时从 2 开始，以此类推。    </p>
<p>4）创建短暂节点（短暂节点 + 不带序号 or 带序号）   </p>
<p>（1）创建短暂的不带序号的节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot;
</code></pre>
<p>（2）创建短暂的带序号的节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 2] create -e -s /sanguo/wuguo &quot;zhouyu&quot;    
</code></pre>
<p>（3）在当前客户端是能查看到的  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 3] ls /sanguo   
</code></pre>
<p>（4）修改节点数据值   </p>
<pre><code>[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo &quot;simayi&quot;  
</code></pre>
<h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><p>监听机制保证 ZooKeeper 保存的任何的数据的任何改变都能快速的响应到监听了该节点的应用程序。  </p>
<p><img src="/2023/08/06/hadoop/32.png" alt="监听器原理">      </p>
<h4 id="节点删除与查看"><a href="#节点删除与查看" class="headerlink" title="节点删除与查看"></a>节点删除与查看</h4><p>1）删除节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin
</code></pre>
<p>2）递归删除节点  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 15] deleteall /sanguo/shuguo
</code></pre>
<p>3）查看节点状态  </p>
<pre><code>[zk: localhost:2181(CONNECTED) 17] stat /sanguo
</code></pre>
<h4 id="客户端向服务端写数据流程"><a href="#客户端向服务端写数据流程" class="headerlink" title="客户端向服务端写数据流程"></a>客户端向服务端写数据流程</h4><p><img src="/2023/08/06/hadoop/33.png" alt="客户端向服务端写数据流程">  </p>
<h2 id="ZooKeeper-分布式锁"><a href="#ZooKeeper-分布式锁" class="headerlink" title="ZooKeeper 分布式锁"></a>ZooKeeper 分布式锁</h2><p>“进程1” 在使用该资源的时候，会先去获得锁，保持独占，这样其他进程就无法访问该资源,用完该资源以后就将锁释放掉,保证了分布式系统中多个进程能够有序的访问该临界资源。  </p>
<p><img src="/2023/08/06/hadoop/34.png" alt="Zookeeper分布式锁">      </p>
<h2 id="Zookeeper企业面试真题（面试重点）总结"><a href="#Zookeeper企业面试真题（面试重点）总结" class="headerlink" title="Zookeeper企业面试真题（面试重点）总结"></a>Zookeeper企业面试真题（面试重点）总结</h2><h3 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h3><p>半数机制，超过半数的投票通过，即通过。  </p>
<p>（1）第一次启动选举规则：  </p>
<pre><code>投票过半数时，服务器 id 大的胜出  
</code></pre>
<p>（2）第二次启动选举规则： </p>
<pre><code>①EPOCH 大的直接胜出  
②EPOCH 相同，事务 id 大的胜出  
③事务 id 相同，服务器 id 大的胜出  
</code></pre>
<p>生产集群安装多少 zk 合适？  </p>
<pre><code>安装奇数台  
</code></pre>
<p>生产经验：  </p>
<pre><code>10 台服务器：3 台 zk  
20 台服务器：5 台 zk  
100 台服务器：11 台 zk  
200 台服务器：11 台 zk    
</code></pre>
<p>服务器台数多：好处，提高可靠性；坏处：提高通信延时  </p>
<p>常用命令  </p>
<pre><code>ls、get、create、delete
</code></pre>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/04/JavaSE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/04/JavaSE/" class="post-title-link" itemprop="url">JavaSE</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-04 17:50:53" itemprop="dateCreated datePublished" datetime="2023-08-04T17:50:53+08:00">2023-08-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:02:10" itemprop="dateModified" datetime="2023-08-11T13:02:10+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>天行健，君子以自强不息~  </p>
<h2 id="Java-核心机制-Java-虚拟机-JVM-java-virtual-machine"><a href="#Java-核心机制-Java-虚拟机-JVM-java-virtual-machine" class="headerlink" title="Java 核心机制-Java 虚拟机 [JVM java virtual machine]"></a>Java 核心机制-Java 虚拟机 [JVM java virtual machine]</h2><p>JVM 是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器，包含在JDK中  </p>
<p>对于不同的平台，有不同的虚拟机    </p>
<h2 id="什么是-JDK，JRE"><a href="#什么是-JDK，JRE" class="headerlink" title="什么是 JDK，JRE"></a>什么是 JDK，JRE</h2><h3 id="JDK-JRE-java-的开发工具-java-javac-javadoc-javap-等"><a href="#JDK-JRE-java-的开发工具-java-javac-javadoc-javap-等" class="headerlink" title="JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等]"></a>JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等]</h3><p>JDK 是提供给 Java 开发人员使用的，其中包含了 java 的开发工具，也包括了 JRE。所以安装了 JDK，就不用在单独安装JRE了  </p>
<h3 id="JRE-JVM-Java-的核心类库-类"><a href="#JRE-JVM-Java-的核心类库-类" class="headerlink" title="JRE &#x3D; JVM + Java 的核心类库[类]"></a>JRE &#x3D; JVM + Java 的核心类库[类]</h3><p>包括 Java 虚拟机(JVM Java Virtual Machine)和 Java 程序所需的核心类库等，如果想要运行一个开发好的 Java 程序，计算机中只需要安装 JRE 即可    </p>
<h4 id="Java-转义字符"><a href="#Java-转义字符" class="headerlink" title="Java 转义字符"></a>Java 转义字符</h4><p>\t ：一个制表位，实现对齐的功能  </p>
<p>\n ：换行符  </p>
<p>\ ：一个\  </p>
<p>&quot; :一个”  </p>
<p>&#39; ：一个’ \r :一个回车 System.out.println(“韩顺平教育\r 北京”);  </p>
<h4 id="Java-中的注释类型"><a href="#Java-中的注释类型" class="headerlink" title="Java 中的注释类型"></a>Java 中的注释类型</h4><ol>
<li><p>单行注释 &#x2F;&#x2F;  </p>
</li>
<li><p>多行注释 &#x2F;* *&#x2F;    </p>
<p> 多行注释里面不允许有多行注释嵌套</p>
</li>
<li><p>文档注释 &#x2F;** *&#x2F;</p>
</li>
</ol>
<p><strong>文档注释：</strong>  </p>
<p>&#x2F;**  </p>
<ul>
<li>@author 韩顺平  </li>
<li>@version 1.0<br>*&#x2F;</li>
</ul>
<h4 id="Java-代码规范"><a href="#Java-代码规范" class="headerlink" title="Java 代码规范"></a>Java 代码规范</h4><p><img src="/2023/08/04/JavaSE/1.png" alt="Java代码规范">    </p>
<h3 id="DOS-介绍"><a href="#DOS-介绍" class="headerlink" title="DOS 介绍"></a>DOS 介绍</h3><p>Dos： Disk Operating System 磁盘操作系统  </p>
<h4 id="常用的-dos-命令"><a href="#常用的-dos-命令" class="headerlink" title="常用的 dos 命令"></a>常用的 dos 命令</h4><ol>
<li><p>查看当前目录是有什么内容 dir  </p>
<p> dir dir d:\abc2\test200</p>
</li>
<li><p>切换到其他盘下：盘符号 cd : change directory</p>
</li>
</ol>
<p>案例演示：切换到 c 盘 cd &#x2F;D c:  </p>
<ol start="3">
<li>切换到当前盘的其他目录下 (使用相对路径和绝对路径演示), ..\表示上一级目录</li>
</ol>
<p>案例演示： cd d:\abc2\test200 cd ....\abc2\test200  </p>
<ol start="4">
<li>切换到上一级：</li>
</ol>
<p>案例演示： cd .. 5) 切换到根目录：cd \  </p>
<p>案例演示：cd \  </p>
<ol start="6">
<li><p>查看指定的目录下所有的子级目录 tree  </p>
</li>
<li><p>清屏 cls [苍老师]  </p>
</li>
<li><p>退出 DOS exit</p>
</li>
</ol>
<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p><strong>变量三要素：变量名+值+数据类型</strong>  </p>
<p>变量相当于内存中一个数据存储空间的表示，你可以把变量看做是一个房间的门牌号，通过门牌号我们可以找到房间，而通过变量名可以访问到变量(值)    </p>
<ol>
<li><p>声明变量  </p>
<p> int a;  </p>
</li>
<li><p>赋值  </p>
<p> a &#x3D; 60; &#x2F;&#x2F;应该这么说: 把 60 赋给 a</p>
</li>
</ol>
<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>每一种数据都定义了明确的数据类型，在内存中分配了不同大小的内存空间(字节)。  </p>
<p><img src="/2023/08/04/JavaSE/2.png" alt="数据类型">    </p>
<h4 id="整型的类型"><a href="#整型的类型" class="headerlink" title="整型的类型"></a>整型的类型</h4><p><img src="/2023/08/04/JavaSE/3.png" alt="数据类型">     </p>
<p>&#x2F;&#x2F;Java 的整型常量（具体值）默认为 int 型，声明 long 型常量须后加‘l’或‘L’  </p>
<pre><code>int n1 = 1;//4 个字节
long n3 = 1L;//长整型  
</code></pre>
<h4 id="浮点类型"><a href="#浮点类型" class="headerlink" title="浮点类型"></a>浮点类型</h4><p><img src="/2023/08/04/JavaSE/4.png" alt="数据类型">    </p>
<p>&#x2F;&#x2F;Java 的浮点型常量(具体值)默认为 double 型，声明 float 型常量，须后加‘f’或‘F’  </p>
<pre><code>float num2 = 1.1F;    
double num3 = 1.1; 
double num4 = 1.1f; 
</code></pre>
<p>十进制数形式：如：5.12 512.0f .512 (必须有小数点）   </p>
<p>Java类的组织形式  </p>
<p><img src="/2023/08/04/JavaSE/5.png" alt="Java类的组织形式">   </p>
<h4 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h4><p><img src="/2023/08/04/JavaSE/6.png" alt="字符编码">  </p>
<h4 id="基本数据类型转换"><a href="#基本数据类型转换" class="headerlink" title="基本数据类型转换"></a>基本数据类型转换</h4><h5 id="自动类型转换"><a href="#自动类型转换" class="headerlink" title="自动类型转换"></a>自动类型转换</h5><p><img src="/2023/08/04/JavaSE/7.png" alt="自动类型转换">    </p>
<h5 id="强制类型转换"><a href="#强制类型转换" class="headerlink" title="强制类型转换"></a>强制类型转换</h5><p>自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符 ( )，但可能造成精度降低或溢出,格外要注意  </p>
<p><img src="/2023/08/04/JavaSE/8.png" alt="强制类型转换">  </p>
<h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><h3 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h3><p>说明逻辑运算规则：  </p>
<ol>
<li><p>a&amp;b : &amp; 叫逻辑与：规则：当 a 和 b 同时为 true ,则结果为 true, 否则为 false  </p>
</li>
<li><p>a&amp;&amp;b : &amp;&amp; 叫短路与：规则：当 a 和 b 同时为 true ,则结果为 true,否则为 false  </p>
</li>
<li><p>a|b : | 叫逻辑或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p>
</li>
<li><p>a||b : || 叫短路或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p>
</li>
<li><p>!a : 叫取反，或者非运算。当 a 为 true, 则结果为 false, 当 a 为 false 是，结果为 true  </p>
</li>
<li><p>a^b: 叫逻辑异或，当 a 和 b 不同时，则结果为 true, 否则为 false</p>
</li>
</ol>
<p><strong>&amp;&amp; 和 &amp; 使用区别</strong>  </p>
<ol>
<li><p>&amp;&amp;短路与：如果第一个条件为 false，则第二个条件不会判断，最终结果为 false，效率高  </p>
</li>
<li><p>&amp; 逻辑与：不管第一个条件是否为 false，第二个条件都要判断，效率低</p>
</li>
</ol>
<p><strong>|| 和 | 使用区别</strong>  </p>
<ol>
<li><p>||短路或：如果第一个条件为 true，则第二个条件不会判断，最终结果为 true，效率高  </p>
</li>
<li><p>| 逻辑或：不管第一个条件是否为 true，第二个条件都要判断，效率低</p>
</li>
</ol>
<h3 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h3><p>条件表达式 ? 表达式 1: 表达式 2;  </p>
<p>运算规则：  </p>
<p>1.如果条件表达式为 true，运算后的结果是表达式 1；  </p>
<p>2.如果条件表达式为 false，运算后的结果是表达式 2；  </p>
<p>口诀: [一灯大师：一真大师]  </p>
<h5 id="接收控制台输入Scanner"><a href="#接收控制台输入Scanner" class="headerlink" title="接收控制台输入Scanner"></a>接收控制台输入Scanner</h5><pre><code>import java.util.Scanner;    
Scanner myScanner = new Scanner(System.in);  
</code></pre>
<h5 id="原码、反码、补码-重点-难点"><a href="#原码、反码、补码-重点-难点" class="headerlink" title="原码、反码、补码(重点 难点)"></a>原码、反码、补码(重点 难点)</h5><p><img src="/2023/08/04/JavaSE/9.png">    </p>
<h2 id="程序控制结构"><a href="#程序控制结构" class="headerlink" title="程序控制结构"></a>程序控制结构</h2><p>主要有三大流程控制语句。  </p>
<ol>
<li><p>顺序控制  </p>
</li>
<li><p>分支控制    </p>
<ol>
<li>单分支 if  </li>
<li>双分支 if-else  </li>
<li>多分支 if-else if -….-else  </li>
<li>switch分支<br> switch(表达式){<br>         case xxx<br>                 }</li>
</ol>
</li>
<li><p>循环控制</p>
</li>
</ol>
<p><strong>for 循环控制</strong><br>    for(循环变量初始化;循环条件;循环变量迭代){<br>        循环操作(可以多条语句)	<br>            }  </p>
<pre><code>eg:  

for(int i = 1;i&lt;=10;i++)&#123;
    System.out.println(&quot;Hello World ~ ！&quot;)	
&#125;
</code></pre>
<p><strong>while 循环控制</strong>  </p>
<pre><code>循环变量初始化;  
while（循环条件）&#123;
    循环体（语句）；
    循环变量迭代；		
&#125;  

eg:		
int i = 1;
while (i &lt;= 10)&#123;
    System.out.println(&quot;Hello World ~ !&quot;)  
    i ++ 		
&#125;  
</code></pre>
<p><strong>do..while 循环控制</strong>  </p>
<pre><code>循环变量初始化;
do&#123;	
    循环体(语句);
    循环变量迭代;
&#125;while(循环条件);  
</code></pre>
<p>先执行，再判断，也就是说，一定会至少执行一次    </p>
<h2 id="跳转控制语句-break"><a href="#跳转控制语句-break" class="headerlink" title="跳转控制语句-break"></a>跳转控制语句-break</h2><p>break 语句用于终止某个语句块的执行，一般使用在 switch 或者循环[for , while , do-while]中  </p>
<h2 id="跳转控制语句-continue"><a href="#跳转控制语句-continue" class="headerlink" title="跳转控制语句-continue"></a>跳转控制语句-continue</h2><p>continue 语句用于结束本次循环，继续执行下一次循环  </p>
<h2 id="跳转控制语句-return"><a href="#跳转控制语句-return" class="headerlink" title="跳转控制语句-return"></a>跳转控制语句-return</h2><p>return 使用在方法，表示跳出所在的方法  </p>
<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>数组可以存放多个同一类型的数据。数组也是一种数据类型，是<strong>引用类型</strong>  </p>
<p><img src="/2023/08/04/JavaSE/10.png" alt="数组的使用">  </p>
<pre><code>方式一：int a[] = new Int[5]   

方式二：
       int[] a;
       a = new Int[10];   

方式三：
   	   int a[] = &#123;2,3,4,5,6&#125;  
</code></pre>
<h3 id="数组赋值机制"><a href="#数组赋值机制" class="headerlink" title="数组赋值机制"></a>数组赋值机制</h3><p><img src="/2023/08/04/JavaSE/11.png" alt="数组的使用">      </p>
<h1 id="面向对象编程-基础部分"><a href="#面向对象编程-基础部分" class="headerlink" title="面向对象编程(基础部分)"></a>面向对象编程(基础部分)</h1><p>类和对象的区别和联系</p>
<ol>
<li><p>类是抽象的，概念的，代表一类事物,比如人类,猫类.., 即它是数据类型  </p>
</li>
<li><p>对象是具体的，实际的，代表一个具体事物, 即 是实例   </p>
</li>
<li><p>类是对象的模板，对象是类的一个个体，对应一个实例</p>
</li>
</ol>
<h2 id="对象在内存中存在形式-重要的-必须搞清楚"><a href="#对象在内存中存在形式-重要的-必须搞清楚" class="headerlink" title="对象在内存中存在形式(重要的)必须搞清楚"></a>对象在内存中存在形式(重要的)必须搞清楚</h2><p><img src="/2023/08/04/JavaSE/12.png" alt="对象在内存中存在形式">     </p>
<h3 id="类和对象的内存分配机制"><a href="#类和对象的内存分配机制" class="headerlink" title="类和对象的内存分配机制"></a>类和对象的内存分配机制</h3><p>Java 内存的结构分析  </p>
<ol>
<li><p>栈： 一般存放基本数据类型(局部变量)  </p>
</li>
<li><p>堆： 存放对象(Cat cat , 数组等)  </p>
</li>
<li><p>方法区：常量池(常量，比如字符串)， 类加载信息</p>
</li>
</ol>
<h2 id="属性-成员变量-字段"><a href="#属性-成员变量-字段" class="headerlink" title="属性&#x2F;成员变量&#x2F;字段"></a>属性&#x2F;成员变量&#x2F;字段</h2><p>成员变量 &#x3D; 属性 &#x3D; field(字段)  </p>
<p>属性是类的一个组成部分，一般是基本数据类型,也可是引用类型(对象，数组)    </p>
<h2 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h2><p>Cat cat1 &#x3D; new Cat();  </p>
<h3 id="成员方法"><a href="#成员方法" class="headerlink" title="成员方法"></a>成员方法</h3><pre><code>class Person &#123;
    String name;
    int age;  
    //方法(成员方法)  
    public void speak() &#123;
        System.out.println(&quot;我是一个好人&quot;);
        &#125;
    &#125;  
</code></pre>
<h3 id="方法的调用机制原理"><a href="#方法的调用机制原理" class="headerlink" title="方法的调用机制原理"></a>方法的调用机制原理</h3><p><img src="/2023/08/04/JavaSE/13.png">  </p>
<h3 id="成员方法的好处"><a href="#成员方法的好处" class="headerlink" title="成员方法的好处"></a>成员方法的好处</h3><ol>
<li><p>提高代码的复用性  </p>
</li>
<li><p>可以将实现的细节封装起来，然后供其他用户来调用即可</p>
</li>
</ol>
<h3 id="成员方法的定义"><a href="#成员方法的定义" class="headerlink" title="成员方法的定义"></a>成员方法的定义</h3><pre><code>访问修饰符 返回数据类型 方法名（形参列表..） &#123;	
    //方法体语句；
    return 返回值;
&#125;
</code></pre>
<h3 id="传参"><a href="#传参" class="headerlink" title="传参"></a>传参</h3><p>引用类型传递的是地址（传递也是值，但是值是地址），可以通过形参影响实参！  </p>
<h3 id="方法递归调用-非常非常重要，比较难"><a href="#方法递归调用-非常非常重要，比较难" class="headerlink" title="方法递归调用(非常非常重要，比较难)"></a>方法递归调用(非常非常重要，比较难)</h3><p>递归就是方法自己调用自己  </p>
<p>递归重要规则  </p>
<p><img src="/2023/08/04/JavaSE/14.png"></p>
<h2 id="方法重载-OverLoad"><a href="#方法重载-OverLoad" class="headerlink" title="方法重载(OverLoad)"></a>方法重载(OverLoad)</h2><p>java 中允许同一个类中，多个同名方法的存在，但要求 形参列表不一致！  </p>
<p>案例：类：MyCalculator 方法：calculate  </p>
<ol>
<li>calculate(int n1, int n2) &#x2F;&#x2F;两个整数的和  </li>
<li>calculate(int n1, double n2) &#x2F;&#x2F;一个整数，一个 double 的和  </li>
<li>calculate(double n2, int n1)&#x2F;&#x2F;一个 double ,一个 Int 和  </li>
<li>calculate(int n1, int n2,int n3)&#x2F;&#x2F;三个 int 的和</li>
</ol>
<p><img src="/2023/08/04/JavaSE/15.png" alt="方法重载">  </p>
<h3 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h3><p>java 允许将同一个类中多个同名同功能但参数个数不同的方法，封装成一个方法。<br>就可以通过可变参数实现</p>
<p>eg:方法 sum 【可以计算 2 个数的和，3 个数的和 ， 4. 5， 。。】  </p>
<pre><code>//1. int... 表示接受的是可变参数，类型是 int ,即可以接收多个 int(0-多)
//2. 使用可变参数时，可以当做数组来使用 即 nums 可以当做数组
//3. 遍历 nums 求和即可
public int sum(int... nums) &#123;
//System.out.println(&quot;接收的参数个数=&quot; + nums.length);
int res = 0;
for(int i = 0; i &lt; nums.length; i++) &#123;
res += nums[i];
&#125;
return res;
&#125;
&#125;  
</code></pre>
<h3 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h3><p>变量：  </p>
<p>1.全局变量（属性）</p>
<p>2.局部变量（局部变量一般是指在成员方法中定义的变量）</p>
<p>全局变量和局部变量可以重名</p>
<p><img src="/2023/08/04/JavaSE/16.png" alt="变量作用域">  </p>
<p>全局变量和局部变量的区别  </p>
<p><img src="/2023/08/04/JavaSE/17.png" alt="全局变量和局部变量的区别">    </p>
<h3 id="构造方法-构造器"><a href="#构造方法-构造器" class="headerlink" title="构造方法&#x2F;构造器"></a>构造方法&#x2F;构造器</h3><p>在创建人类的对象时，就直接指定这个对象的年龄和姓名，该怎么做? 这时就可以使用构造器  </p>
<p>[修饰符] 方法名(形参列表){<br>方法体;<br>}   </p>
<ol>
<li><p>构造器的修饰符可以默认， 也可以是 public protected private  </p>
</li>
<li><p>构造器没有返回值   </p>
</li>
<li><p>方法名 和类名字必须一样  </p>
</li>
<li><p>参数列表 和 成员方法一样的规则  </p>
</li>
<li><p>构造器的调用, 由系统完成</p>
</li>
</ol>
<p>构造方法又叫构造器(constructor)，是类的一种特殊的方法，它的主要作用是完成对新对象的初始化  </p>
<p><img src="/2023/08/04/JavaSE/18.png" alt="构造器使用注意事项"></p>
<p><img src="/2023/08/04/JavaSE/19.png" alt="构造器使用注意事项"></p>
<h3 id="this-关键字"><a href="#this-关键字" class="headerlink" title="this 关键字"></a>this 关键字</h3><p><img src="/2023/08/04/JavaSE/20.png" alt="This关键字"></p>
<ol>
<li><p>this 关键字可以用来访问本类的属性、方法、构造器  </p>
</li>
<li><p>this 用于区分当前类的属性和局部变量  </p>
</li>
<li><p>访问成员方法的语法：this.方法名(参数列表);    </p>
</li>
<li><p>访问构造器语法：this(参数列表); 注意只能在构造器中使用(即只能在构造器中访问另外一个构造器, 必须放在第一条语句)  </p>
</li>
<li><p>this 不能在类定义的外部使用，只能在类定义的方法中使用。</p>
</li>
</ol>
<h1 id="面向对象编程-中级部分"><a href="#面向对象编程-中级部分" class="headerlink" title="面向对象编程(中级部分)"></a>面向对象编程(中级部分)</h1><p>IDEA 常用快捷键  </p>
<ol>
<li><p>删除当前行, 默认是 ctrl + Y 自己配置 ctrl + d  </p>
</li>
<li><p>复制当前行, 自己配置 ctrl + alt + 向下光标  </p>
</li>
<li><p>补全代码 alt + &#x2F;  </p>
</li>
<li><p>添加注释和取消注释 ctrl + &#x2F; 【第一次是添加注释，第二次是取消注释】  </p>
</li>
<li><p>导入该行需要的类 先配置 auto import , 然后使用 alt+enter 即可  </p>
</li>
<li><p>快速格式化代码 ctrl + alt + L  </p>
</li>
<li><p>快速运行程序 自己定义 alt + R  </p>
</li>
<li><p>生成构造器等 alt + insert [提高开发效率]  </p>
</li>
<li><p>查看一个类的层级关系 ctrl + H [学习继承后，非常有用]  </p>
</li>
<li><p>将光标放在一个方法上，输入 ctrl + B , 可以定位到方法 [学继承后，非常有用]  </p>
</li>
<li><p>自动的分配变量名 , 通过 在后面假 .var [老师最喜欢的]  </p>
</li>
<li><p>还有很多其它的快捷键</p>
</li>
</ol>
<h2 id="包"><a href="#包" class="headerlink" title="包"></a>包</h2><p><img src="/2023/08/04/JavaSE/21.png" alt="包">  </p>
<p>包的本质 </p>
<p><img src="/2023/08/04/JavaSE/22.png">  </p>
<p>包的命名：  </p>
<p>com.公司名.项目名.业务模块名  </p>
<h3 id="Java常用的包"><a href="#Java常用的包" class="headerlink" title="Java常用的包"></a>Java常用的包</h3><p>一个包下,包含很多的类,java 中常用的包有:  </p>
<ol>
<li><p>java.lang.* &#x2F;&#x2F;lang 包是基本包，默认引入，不需要再引入.  </p>
</li>
<li><p>java.util.* &#x2F;&#x2F;util 包，系统提供的工具包, 工具类，使用 Scanner  </p>
</li>
<li><p>java.net.* &#x2F;&#x2F;网络包，网络开发  </p>
</li>
<li><p>java.awt.* &#x2F;&#x2F;是做 java 的界面开发，GUI</p>
</li>
</ol>
<p>引入包的语法：  </p>
<p>import 包;  </p>
<p>eg:  import java.util.*  &#x2F;&#x2F;表示将java.util包所有都引入    </p>
<p>我们需要使用到哪个类，就导入哪个类即可，不建议使用*导入   </p>
<h2 id="访问修饰符"><a href="#访问修饰符" class="headerlink" title="访问修饰符"></a>访问修饰符</h2><p>java 提供四种访问控制修饰符号，用于控制方法和属性(成员变量)的访问权限（范围）   </p>
<ol>
<li><p>公开级别:用 public 修饰,对外公开  </p>
</li>
<li><p>受保护级别:用 protected 修饰,对子类和同一个包中的类公开  </p>
</li>
<li><p>默认级别:没有修饰符号,向同一个包的类公开.   </p>
</li>
<li><p>私有级别:用 private 修饰,只有类本身可以访问,不对外公开</p>
</li>
</ol>
<p><img src="/2023/08/04/JavaSE/23.png" alt="访问控制符">  </p>
<p><img src="/2023/08/04/JavaSE/24.png" alt="访问控制符使用说明">   </p>
<h2 id="面向对象编程三大特征"><a href="#面向对象编程三大特征" class="headerlink" title="面向对象编程三大特征"></a>面向对象编程三大特征</h2><p>封装、继承和多态    </p>
<h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><p><img src="/2023/08/04/JavaSE/25.png" alt="封装"></p>
<p>封装的实现步骤    </p>
<p><img src="/2023/08/04/JavaSE/26.png" alt="封装的实现步骤">    </p>
<h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>继承可以解决代码复用,让我们的编程更加靠近人类思维.当多个类存在相同的属性(变量)和方法时,可以从这些类中抽象出父类,在父类中定义这些相同的属性和方法，所有的子类不需要重新定义这些属性和方法，只需要通过 extends 来声明继承父类即可  </p>
<p><img src="/2023/08/04/JavaSE/27.png" alt="继承">  </p>
<p>继承的基本语法：  </p>
<p>class 子类 extends 父类 {</p>
<p>}<br>子类就会自动拥有父类定义的属性和方法<br>子类又叫超类，基类<br>子类又叫派生类  </p>
<p>继承给编程带来的便利  </p>
<ol>
<li><p>代码的复用性提高了  </p>
</li>
<li><p>代码的扩展性和维护性提高了</p>
</li>
</ol>
<p>**继承的细节问题： ** </p>
<ol>
<li><p>子类继承了所有的属性和方法，非私有的属性和方法可以在子类直接访问, 但是私有属性和方法不能在子类直接访问，要通过父类提供公共的方法去访问  </p>
</li>
<li><p>子类必须调用父类的构造器,完成父类的初始化  </p>
</li>
<li><p>当创建子类对象时，不管使用子类的哪个构造器，默认情况下总会去调用父类的无参构造器，如果父类没有提供无参构造器，则必须在子类的构造器中用 super 去指定使用父类的哪个构造器完成对父类的初始化工作，否则，编译不会通过(怎么理解。)   </p>
</li>
<li><p>如果希望指定去调用父类的某个构造器，则显式的调用一下 : super(参数列表)  </p>
</li>
<li><p>super 在使用时，必须放在构造器第一行(super 只能在构造器中使用)  </p>
</li>
<li><p>super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器  </p>
</li>
<li><p>java 所有类都是 Object 类的子类, Object 是所有类的基类.  </p>
</li>
<li><p>父类构造器的调用不限于直接父类！将一直往上追溯直到 Object 类(顶级父类)    </p>
</li>
<li><p>子类最多只能继承一个父类(指直接继承)，即 java 中是单继承机制。<br>思考：如何让 A 类继承 B 类和 C 类？ 【A 继承 B， B 继承 C】  </p>
</li>
<li><p>不能滥用继承，子类和父类之间必须满足 is-a 的逻辑关系</p>
</li>
</ol>
<p><strong>输入 ctrl + H 可以看到类的继承关系</strong></p>
<pre><code>public class Sub extends Base &#123; //子类
    public Sub(String name, int age) &#123;
    //1. 调用父类的无参构造器, 如下或者什么都不写,默认就是调用 super()
    //super();//父类的无参构造器
    //2. 调用父类的 Base(String name) 构造器
    //super(&quot;hsp&quot;);
    //调用父类的 Base(String name, int age) 构造器
    super(&quot;king&quot;, 20);
    //细节： super 在使用时，必须放在构造器第一行
    //细节: super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器
    //this() 不能再使用了
    System.out.println(&quot;子类 Sub(String name, int age)构造器被调用....&quot;);
    &#125;
</code></pre>
<p>子类创建的内存布局  </p>
<p><img src="/2023/08/04/JavaSE/28.png" alt="子类创建的内存布局">    </p>
<h3 id="super-关键字"><a href="#super-关键字" class="headerlink" title="super 关键字"></a>super 关键字</h3><p>super 代表父类的引用，用于访问父类的属性、方法、构造器  </p>
<p><img src="/2023/08/04/JavaSE/29.png" alt="super关键字">  </p>
<p>&#x2F;&#x2F; (1)先找本类，如果有，则调用  </p>
<p>&#x2F;&#x2F; (2)如果没有，则找父类(如果有，并可以调用，则调用)  </p>
<p>&#x2F;&#x2F; (3)如果父类没有，则继续找父类的父类,整个规则，就是一样的,直到 Object 类  </p>
<p>&#x2F;&#x2F; 提示：如果查找方法的过程中，找到了，但是不能访问， 则报错, cannot access  </p>
<p>&#x2F;&#x2F; 如果查找方法的过程中，没有找到，则提示方法不存在    </p>
<p><img src="/2023/08/04/JavaSE/30.png" alt="Super关键字的用法细节">      </p>
<p><strong>super 和 this 的比较</strong>    </p>
<p><img src="/2023/08/04/JavaSE/31.png" alt="super和this的比较">    </p>
<h2 id="方法重写-覆盖-override"><a href="#方法重写-覆盖-override" class="headerlink" title="方法重写&#x2F;覆盖(override)"></a>方法重写&#x2F;覆盖(override)</h2><p><img src="/2023/08/04/JavaSE/32.png" alt="方法重写">  </p>
<p><img src="/2023/08/04/JavaSE/33.png" alt="方法重写的注意事项">    </p>
<p><strong>方法重载和方法重写的区别</strong>  </p>
<p><img src="/2023/08/04/JavaSE/34.png" alt="方法重载和方法重写的区别">  </p>
<h2 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h2><p>多态是建立在封装和继承基础之上的<br><img src="/2023/08/04/JavaSE/35.png" alt="多态">     </p>
<p>多态的具体体现  </p>
<ol>
<li>方法的多态</li>
</ol>
<p>重写和重载就体现多态    </p>
<ol start="2">
<li>对象的多态 (核心，困难，重点)</li>
</ol>
<p><img src="/2023/08/04/JavaSE/36.png" alt="多态案例">     </p>
<p>多态的前提是：两个对象(类)存在继承关系  </p>
<p>多态的向上转型   </p>
<p><img src="/2023/08/04/JavaSE/37.png" alt="多态的向上转型">   </p>
<p>多态向下转型  </p>
<p><img src="/2023/08/04/JavaSE/38.png" alt="多态的向下转型">    </p>
<p>属性没有重写之说！属性的值看编译类型   </p>
<p>instanceOf 比较操作符，用于判断对象的运行类型是否为 XX 类型或 XX 类型的子类型  </p>
<p>java 的动态绑定机制(非常非常重要.)  page 365</p>
<p>JDBC page 1119  </p>
<p>正则表达式 page 1210</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/" class="post-title-link" itemprop="url">数仓建模</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-03 18:25:47" itemprop="dateCreated datePublished" datetime="2023-08-03T18:25:47+08:00">2023-08-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 12:58:35" itemprop="dateModified" datetime="2023-08-11T12:58:35+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="人生在勤，不索何获"><a href="#人生在勤，不索何获" class="headerlink" title="人生在勤，不索何获"></a>人生在勤，不索何获</h1><p><a target="_blank" rel="noopener" href="https://help.aliyun.com/zh/dataworks/user-guide/dataworks-data-modeling/?spm=a2c4g.11186623.0.0.6cbf2c36NLQ1IR">阿里数仓建模理论</a></p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/03/kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/03/kafka/" class="post-title-link" itemprop="url">kafka3.0.0学习记录</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-03 10:18:49" itemprop="dateCreated datePublished" datetime="2023-08-03T10:18:49+08:00">2023-08-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:00:47" itemprop="dateModified" datetime="2023-08-11T13:00:47+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>黄沙百战穿金甲，不破楼兰终不还！   </p>
<p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw">https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw</a><br>提取码：85vn   </p>
<h1 id="1-Kafka概述"><a href="#1-Kafka概述" class="headerlink" title="1.Kafka概述"></a>1.Kafka概述</h1><h2 id="1-1定义："><a href="#1-1定义：" class="headerlink" title="1.1定义："></a>1.1定义：</h2><p>Kafka传 统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（Message<br>Queue），主要应用于大数据实时处理领域。    </p>
<p>发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息<br>分为不同的类别，订阅者只接收感兴趣的消息。  </p>
<h2 id="1-2消息队列"><a href="#1-2消息队列" class="headerlink" title="1.2消息队列"></a>1.2消息队列</h2><p>大数据场景主要采用 Kafka 作为消息队列。  </p>
<h3 id="1-2-1传统消息队列的应用场景"><a href="#1-2-1传统消息队列的应用场景" class="headerlink" title="1.2.1传统消息队列的应用场景"></a>1.2.1传统消息队列的应用场景</h3><p>传统的消息队列的主要应用场景包括：缓存&#x2F;消峰、解耦和异步通信。</p>
<p>缓冲&#x2F;消峰：解决生产消息和消费消息的处理速度不一致的情况。 </p>
<p>解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p>
<p><img src="/2023/08/03/kafka/kafka1.png" alt="解耦"> </p>
<p>异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。  </p>
<h3 id="1-2-2消息队列的两种模式"><a href="#1-2-2消息队列的两种模式" class="headerlink" title="1.2.2消息队列的两种模式"></a>1.2.2消息队列的两种模式</h3><p><img src="/2023/08/03/kafka/2.png" alt="消息队列的两种模式">  </p>
<h2 id="1-3kafka基础架构"><a href="#1-3kafka基础架构" class="headerlink" title="1.3kafka基础架构"></a>1.3kafka基础架构</h2><p><img src="/2023/08/03/kafka/3.png" alt="kafka基础架构">   </p>
<p>（1）Producer：消息生产者，就是向 Kafka broker 发消息的客户端。  </p>
<p>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。  </p>
<p>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。消费者组内每个消<br>费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不<br>影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。  </p>
<p>（4）Broker：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个broker 可以容纳多个 topic。  </p>
<p>（5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个 topic。  </p>
<p>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。  </p>
<p>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个Follower。  </p>
<p>（8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数<br>据的对象都是 Leader。  </p>
<p>（9）Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和<br>Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Leader。    </p>
<h2 id="2kafka快速入门"><a href="#2kafka快速入门" class="headerlink" title="2kafka快速入门"></a>2kafka快速入门</h2><p>kafka命令行操作    </p>
<p>1）查看操作主题命令参数    </p>
<pre><code> bin/kafka-topics.sh    
</code></pre>
<p>2）查看当前服务器中的所有 topic  </p>
<pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list
</code></pre>
<p>3）创建 first topic  </p>
<pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 1 --replication-factor 3 --topic first  
</code></pre>
<p>选项说明：    </p>
<pre><code>--topic 定义 topic 名  

--replication-factor 定义副本数  

--partitions 定义分区数  
</code></pre>
<p>4）查看 first 主题的详情  </p>
<pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first
</code></pre>
<p>5）修改分区数（注意：分区数只能增加，不能减少）  </p>
<pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3 
</code></pre>
<p>6）删除 topic    </p>
<pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first
</code></pre>
<h2 id="生产者命令行操作"><a href="#生产者命令行操作" class="headerlink" title="生产者命令行操作"></a>生产者命令行操作</h2><p>发送消息</p>
<pre><code>bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first
</code></pre>
<h2 id="消费者命令行操作"><a href="#消费者命令行操作" class="headerlink" title="消费者命令行操作"></a>消费者命令行操作</h2><h3 id="消费-first-主题中的数据"><a href="#消费-first-主题中的数据" class="headerlink" title="消费 first 主题中的数据"></a>消费 first 主题中的数据</h3><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first
</code></pre>
<h3 id="把主题中所有的数据都读取出来（包括历史数据）"><a href="#把主题中所有的数据都读取出来（包括历史数据）" class="headerlink" title="把主题中所有的数据都读取出来（包括历史数据）"></a>把主题中所有的数据都读取出来（包括历史数据）</h3><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first  
</code></pre>
<h2 id="生产者重要参数"><a href="#生产者重要参数" class="headerlink" title="生产者重要参数"></a>生产者重要参数</h2><p>acks </p>
<pre><code>1）0：生产者发送过来的数据，不需要等数据落盘应答。  

2）1：生产者发送过来的数据，Leader 收到数据后应答。  

3）-1（all）：生产者发送过来的数据，Leader+和 isr 队列里面的所有节点收齐数据后应答。默认值是-1，-1 和all 是等价的。  
</code></pre>
<p>enable.idempotence  </p>
<p>是否开启幂等性，默认 true，开启幂等性。  </p>
<p>compression.type  </p>
<p>生产者发送的所有数据的压缩方式。默认是 none，也就是不压缩。  </p>
<p>支持压缩类型：none、gzip、snappy、lz4 和 zstd。    </p>
<h2 id="生产者分区"><a href="#生产者分区" class="headerlink" title="生产者分区"></a>生产者分区</h2><h3 id="1-分区好处"><a href="#1-分区好处" class="headerlink" title="1.分区好处"></a>1.分区好处</h3><p>（1）便于合理使用存储资源  </p>
<p>每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一<br>块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果   </p>
<p>（2）提高并行度  </p>
<p>生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。  </p>
<h2 id="生产经验——生产者如何提高吞吐量"><a href="#生产经验——生产者如何提高吞吐量" class="headerlink" title="生产经验——生产者如何提高吞吐量"></a>生产经验——生产者如何提高吞吐量</h2><p>batch.size：批次大小，默认16k   </p>
<p>linger.ms：等待时间，修改为5-100ms  </p>
<p>compression.type：压缩snappy   </p>
<p>RecordAccumulator：缓冲区大小，修改为64m  </p>
<h2 id="ack-应答原理"><a href="#ack-应答原理" class="headerlink" title="ack 应答原理"></a>ack 应答原理</h2><p><img src="/2023/08/03/kafka/4.png" alt="ack应答原理">  </p>
<p><img src="/2023/08/03/kafka/5.png" alt="ack应答原理"> </p>
<p>数据完全可靠条件 &#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2  </p>
<h3 id="可靠性总结："><a href="#可靠性总结：" class="headerlink" title="可靠性总结："></a>可靠性总结：</h3><p>acks&#x3D;0，生产者发送过来数据就不管了，可靠性差，效率高；  </p>
<p>acks&#x3D;1，生产者发送过来数据Leader应答，可靠性中等，效率中等；  </p>
<p>acks&#x3D;-1，生产者发送过来数据Leader和ISR队列里面所有Follwer应答，可靠性高，效率低；  </p>
<p>在生产环境中，acks&#x3D;0很少使用；acks&#x3D;1，一般用于传输普通日志，允许丢个别数据；acks&#x3D;-1，一般用于传输和钱相关的数据，对可靠性要求比较高的场景。    </p>
<h2 id="生产经验——数据去重"><a href="#生产经验——数据去重" class="headerlink" title="生产经验——数据去重"></a>生产经验——数据去重</h2><p>至少一次（At Least Once）&#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2  </p>
<p>最多一次（At Most Once）&#x3D; ACK级别设置为0  </p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>At Least Once可以保证数据不丢失，但是不能保证数据不重复；  </p>
<p>At Most Once可以保证数据不重复，但是不能保证数据不丢失。  </p>
<p>精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。  </p>
<p>Kafka 0.11版本以后，引入了一项重大特性：幂等性和事务。  </p>
<h3 id="幂等性原理"><a href="#幂等性原理" class="headerlink" title="幂等性原理"></a>幂等性原理</h3><p>幂等性就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。  </p>
<p>精确一次（Exactly Once） &#x3D; 幂等性 + 至少一次（ ack&#x3D;-1 + 分区副本数&gt;&#x3D;2 + ISR最小副本数量&gt;&#x3D;2）  </p>
<p>幂等性只能保证的是在单分区单会话内不重复。  </p>
<h4 id="如何使用幂等性"><a href="#如何使用幂等性" class="headerlink" title="如何使用幂等性"></a>如何使用幂等性</h4><p>开启参数 enable.idempotence 默认为 true，false 关闭。    </p>
<p>开启事务，必须开启幂等性。  </p>
<h2 id="生产经验——数据乱序"><a href="#生产经验——数据乱序" class="headerlink" title="生产经验——数据乱序"></a>生产经验——数据乱序</h2><p>）kafka在1.x版本之前保证数据单分区有序，条件如下：  </p>
<p>max.in.flight.requests.per.connection&#x3D;1（不需要考虑是否开启幂等性）。  </p>
<p>2）kafka在1.x及以后版本保证数据单分区有序，条件如下：    </p>
<p>（1）未开启幂等性  </p>
<p>max.in.flight.requests.per.connection需要设置为1。  </p>
<p>（2）开启幂等性   </p>
<p>max.in.flight.requests.per.connection需要设置小于等于5。    </p>
<h2 id="Kafka-副本"><a href="#Kafka-副本" class="headerlink" title="Kafka 副本"></a>Kafka 副本</h2><h3 id="副本基本信息"><a href="#副本基本信息" class="headerlink" title="副本基本信息"></a>副本基本信息</h3><p>（1）Kafka 副本作用：提高数据可靠性。  </p>
<p>（2）Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；  </p>
<p>（3）Kafka 中副本分为：Leader 和 Follower。    </p>
<p>（4）Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。AR &#x3D; ISR + OSR  </p>
<p>ISR，表示和 Leader 保持同步的 Follower 集合。如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值由 replica.lag.time.max.ms参数设定，默认 30s。Leader 发生故障之后，就会从 ISR 中选举新的 Leader    </p>
<p>OSR，表示 Follower 与 Leader 副本同步时，延迟过多的副本。    </p>
<h3 id="Leader-选举流程"><a href="#Leader-选举流程" class="headerlink" title="Leader 选举流程"></a>Leader 选举流程</h3><p>Kafka 集群中有一个 broker 的 Controller 会被选举为 Controller Leader，负责管理集群broker 的上下线，所有 topic 的分区副本分配和 Leader 选举等工作。  </p>
<p><img src="/2023/08/03/kafka/6.png" alt="Leader选举流程">  </p>
<h3 id="Leader-和-Follower-故障处理细节"><a href="#Leader-和-Follower-故障处理细节" class="headerlink" title="Leader 和 Follower 故障处理细节"></a>Leader 和 Follower 故障处理细节</h3><p><img src="/2023/08/03/kafka/7.png" alt="Follower故障处理细节">   </p>
<p><img src="/2023/08/03/kafka/8.png" alt="Leader故障处理细节"></p>
<h3 id="生产经验——Leader-Partition-负载平衡"><a href="#生产经验——Leader-Partition-负载平衡" class="headerlink" title="生产经验——Leader Partition 负载平衡"></a>生产经验——Leader Partition 负载平衡</h3><p>正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p>
<p>auto.leader.rebalance.enable 默认是 true。 自动 Leader Partition 平衡。生产环境中，leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭。   </p>
<p>leader.imbalance.per.broker.percentage 默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器<br>会触发 leader 的平衡。  </p>
<p>leader.imbalance.check.interval.seconds 默认值 300 秒。检查 leader 负载是否平衡的间隔时间。  </p>
<h3 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h3><p>1）Topic 数据的存储机制</p>
<p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。  </p>
<p><img src="/2023/08/03/kafka/9.png" alt="Kafka文件存储机制">   </p>
<p>Topic数据存储位置：每个broker节点的 kafka&#x2F;datas&#x2F;目录下</p>
<p>通过工具查看 index 和 log 信息<br>[atguigu@hadoop104 first-1]$ kafka-run-class.sh kafka.tools.DumpLogSegments –files .&#x2F;00000000000000000000.index   </p>
<pre><code>Dumping ./00000000000000000000.index  
offset: 3 position: 152 
</code></pre>
<h4 id="Log文件和Index文件详解"><a href="#Log文件和Index文件详解" class="headerlink" title="Log文件和Index文件详解"></a>Log文件和Index文件详解</h4><p><img src="/2023/08/03/kafka/10.png" alt="Log文件和Index文件详解">    </p>
<p>log.segment.bytes Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，默认值 1G。  </p>
<p>log.index.interval.bytes 默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。 稀疏索引。    </p>
<h3 id="文件清理策略"><a href="#文件清理策略" class="headerlink" title="文件清理策略"></a>文件清理策略</h3><p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。    </p>
<p>Kafka 中提供的日志清理策略有 delete 和 compact 两种。  </p>
<p>1）delete 日志删除：将过期数据删除<br> log.cleanup.policy &#x3D; delete 所有数据启用删除策略   </p>
<p>（1）基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳。  </p>
<p>（2）基于大小：默认关闭。超过设置的所有日志总大小，删除最早的 segment。log.retention.bytes，默认等于-1，表示无穷大。  </p>
<p>2）compact 日志压缩  </p>
<p>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。    </p>
<p>log.cleanup.policy &#x3D; compact 所有数据启用压缩策略</p>
<p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。  </p>
<p>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。  </p>
<p><img src="/2023/08/03/kafka/11.png" alt="compact日志压缩">  </p>
<h2 id="高效读写数据"><a href="#高效读写数据" class="headerlink" title="高效读写数据"></a>高效读写数据</h2><p>1）Kafka 本身是分布式集群，可以采用分区技术，并行度高  </p>
<p>2）读数据采用稀疏索引，可以快速定位要消费的数据  </p>
<p>3）顺序写磁盘  </p>
<p>   顺序写之所以快，是因为其省去了大量磁头寻址的时间。  </p>
<p>4）页缓存 + 零拷贝技术     </p>
<h2 id="Kafka-消费者"><a href="#Kafka-消费者" class="headerlink" title="Kafka 消费者"></a>Kafka 消费者</h2><h3 id="Kafka-消费方式"><a href="#Kafka-消费方式" class="headerlink" title="Kafka 消费方式"></a>Kafka 消费方式</h3><p>pull（拉）模 式：</p>
<p>consumer采用从broker中主动拉取数据。Kafka采用这种方式。</p>
<p>push（推）模式：  </p>
<p>Kafka没有采用这种方式，因为由broker决定消息发送速率，很难适应所有消费者的消费速率。    </p>
<p>pull模式不足之处是，如 果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据。  </p>
<p>Kafka 消费者总体工作流程  </p>
<p><img src="/2023/08/03/kafka/12.png" alt="kafka消费总体工作流程">    </p>
<p>消费者组原理    </p>
<p>Consumer Group（CG）：消费者组，由多个consumer组成。形成一个消费者组的条件，是所有消费者的groupid相同。  </p>
<p>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费。  </p>
<p>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。    </p>
<p><img src="/2023/08/03/kafka/13.png" alt="消费者组原理">  </p>
<h2 id="生产经验——分区的分配以及再平衡"><a href="#生产经验——分区的分配以及再平衡" class="headerlink" title="生产经验——分区的分配以及再平衡"></a>生产经验——分区的分配以及再平衡</h2><p>Kafka有四种主流的分区分配策略： Range、RoundRobin、Sticky、CooperativeSticky。可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略  </p>
<h3 id="Range-以及再平衡"><a href="#Range-以及再平衡" class="headerlink" title="Range 以及再平衡"></a>Range 以及再平衡</h3><p>Range 是对每个 topic 而言的。首先对同一个 topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。  </p>
<p>容易产生数据倾斜！    </p>
<p>Kafka 默认的分区分配策略就是 Range + CooperativeSticky   </p>
<h3 id="RoundRobin-以及再平衡"><a href="#RoundRobin-以及再平衡" class="headerlink" title="RoundRobin 以及再平衡"></a>RoundRobin 以及再平衡</h3><p>RoundRobin 轮询分区策略，是把所有的 partition 和所有的consumer 都列出来，然后按照 hashcode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。  </p>
<h3 id="Sticky-以及再平衡"><a href="#Sticky-以及再平衡" class="headerlink" title="Sticky 以及再平衡"></a>Sticky 以及再平衡</h3><p>粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，<br>考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。<br>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。   </p>
<h2 id="offset-位移"><a href="#offset-位移" class="headerlink" title="offset 位移"></a>offset 位移</h2><p>从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets</p>
<p>__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是group.id+topic+分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact，也就是每个 group.id+topic+分区号就保留最新数据。  </p>
<h3 id="自动提交-offset"><a href="#自动提交-offset" class="headerlink" title="自动提交 offset"></a>自动提交 offset</h3><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。  </p>
<p>自动提交offset的相关参数：  </p>
<p>enable.auto.commit：是否开启自动提交offset功能，默认是true  </p>
<p>auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s  </p>
<h3 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h3><p>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次提交的一批数据最高的偏移量提交；不同点是，同步提交阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故有可能提交失败。  </p>
<p>commitSync（同步提交）：必须等待offset提交完毕，再去消费下一批数据。  </p>
<p>commitAsync（异步提交） ：发送完提交offset请求后，就开始消费下一批数据了。  </p>
<h3 id="指定-Offset-消费"><a href="#指定-Offset-消费" class="headerlink" title="指定 Offset 消费"></a>指定 Offset 消费</h3><pre><code>auto.offset.reset = earliest | latest | none 默认是 latest
</code></pre>
<p>（1）earliest：自动将偏移量重置为最早的偏移量，–from-beginning。  </p>
<p>（2）latest（默认值）：自动将偏移量重置为最新偏移量。  </p>
<p>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。  </p>
<p>（4）任意指定 offset 位移开始消费    </p>
<h2 id="漏消费和重复消费"><a href="#漏消费和重复消费" class="headerlink" title="漏消费和重复消费"></a>漏消费和重复消费</h2><p>重复消费：已经消费了数据，但是 offset 没提交。 </p>
<p>漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。  </p>
<h3 id="生产经验——消费者事务"><a href="#生产经验——消费者事务" class="headerlink" title="生产经验——消费者事务"></a>生产经验——消费者事务</h3><p>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交<br>offset过程做原子绑定。  </p>
<h3 id="生产经验——数据积压（消费者如何提高吞吐量）"><a href="#生产经验——数据积压（消费者如何提高吞吐量）" class="headerlink" title="生产经验——数据积压（消费者如何提高吞吐量）"></a>生产经验——数据积压（消费者如何提高吞吐量）</h3><p><img src="/2023/08/03/kafka/14.png" alt="数据积压">    </p>
<h2 id="Kafka-Kraft-模式"><a href="#Kafka-Kraft-模式" class="headerlink" title="Kafka-Kraft 模式"></a>Kafka-Kraft 模式</h2><p><img src="/2023/08/03/kafka/15.png" alt="kafka-kraft架构">  </p>
<p>右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。  </p>
<p>这样做的好处有以下几个：  </p>
<p>Kafka 不再依赖外部框架，而是能够独立运行；    </p>
<p>controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升；  </p>
<p>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制；  </p>
<p>controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强  </p>
<p>controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。  </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/02/Spark-Core/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/02/Spark-Core/" class="post-title-link" itemprop="url">Spark学习笔记</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-02 10:51:06" itemprop="dateCreated datePublished" datetime="2023-08-02T10:51:06+08:00">2023-08-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 12:49:16" itemprop="dateModified" datetime="2023-08-11T12:49:16+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="纵浪大化中，不喜亦不惧"><a href="#纵浪大化中，不喜亦不惧" class="headerlink" title="纵浪大化中，不喜亦不惧 ~"></a>纵浪大化中，不喜亦不惧 ~</h1><p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g">https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g</a><br>提取码：mg5w   </p>
<h1 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h1><h2 id="Spark-是什么？"><a href="#Spark-是什么？" class="headerlink" title="Spark 是什么？"></a>Spark 是什么？</h2><p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎     </p>
<p>Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎  </p>
<p>Spark Core 中提供了 Spark 最基础与最核心的功能  </p>
<p>Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据   </p>
<p>Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API  </p>
<p>Spark 一直被认为是 Hadoop 框架的升级版。  </p>
<h2 id="Mapreduce是什么？"><a href="#Mapreduce是什么？" class="headerlink" title="Mapreduce是什么？"></a>Mapreduce是什么？</h2><p>MapReduce 是一种编程模型，作为 Hadoop 的分布式计算模型，是 Hadoop 的核心    </p>
<h2 id="HBase是什么？"><a href="#HBase是什么？" class="headerlink" title="HBase是什么？"></a>HBase是什么？</h2><p>HBase 是一个基于 HDFS 的分布式数据库，擅长实时地随机读&#x2F;写超大规模数据集  </p>
<h2 id="Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）"><a href="#Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）" class="headerlink" title="Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）"></a>Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）</h2><p>Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘  </p>
<p>Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互  </p>
<p>Spark 的缓存机制比 HDFS 的缓存机制高效  </p>
<h2 id="什么时候选用Spark什么时候选用Mapreduce？"><a href="#什么时候选用Spark什么时候选用Mapreduce？" class="headerlink" title="什么时候选用Spark什么时候选用Mapreduce？"></a>什么时候选用Spark什么时候选用Mapreduce？</h2><p>Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark并不能完全替代 MR。  </p>
<h2 id="提交Spark应用的代码示例"><a href="#提交Spark应用的代码示例" class="headerlink" title="提交Spark应用的代码示例"></a>提交Spark应用的代码示例</h2><h3 id="local模式"><a href="#local模式" class="headerlink" title="local模式"></a>local模式</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ./examples/jars/spark-examples_2.12-3.0.0.jar 10

1) --class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序  
2) --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟 CPU 核数量  
3) spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包  
4) 数字 10 表示程序的入口参数，用于设定当前应用的任务数量  
</code></pre>
<h3 id="Yarn模式-（生产：Cluster模式）"><a href="#Yarn模式-（生产：Cluster模式）" class="headerlink" title="Yarn模式  （生产：Cluster模式）"></a>Yarn模式  （生产：Cluster模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2.12-3.0.0.jar 10  
</code></pre>
<h3 id="Yarn模式-（测试：Client模式）"><a href="#Yarn模式-（测试：Client模式）" class="headerlink" title="Yarn模式  （测试：Client模式）"></a>Yarn模式  （测试：Client模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.12-3.0.0.jar 10  
</code></pre>
<h4 id="Spark的端口号"><a href="#Spark的端口号" class="headerlink" title="Spark的端口号"></a>Spark的端口号</h4><pre><code>➢ Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）  
➢ Spark Master 内部通信服务端口号：7077  
➢ Standalone 模式下，Spark Master Web 端口号：8080（资源）  
➢ Spark 历史服务器端口号：18080  
➢ Hadoop YARN 任务运行情况查看端口号：8088     
</code></pre>
<h2 id="Spark运行架构"><a href="#Spark运行架构" class="headerlink" title="Spark运行架构"></a>Spark运行架构</h2><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构  </p>
<p><img src="/2023/08/02/Spark-Core/1.png" alt="Spark运行架构">  </p>
<p>Driver表示Master，负责管理整个集群中的作业调度  </p>
<p>Executor表示slave，负责实际执行任务  </p>
<h3 id="Spark核心组件"><a href="#Spark核心组件" class="headerlink" title="Spark核心组件"></a>Spark核心组件</h3><p>Driver和Executor &amp; Master 和 Worker </p>
<h4 id="Driver的作用"><a href="#Driver的作用" class="headerlink" title="Driver的作用"></a>Driver的作用</h4><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。  </p>
<p>Driver 在 Spark 作业执行时主要负责：  </p>
<pre><code>➢ 将用户程序转化为作业（job）  
➢ 在 Executor 之间调度任务(task)  
➢ 跟踪 Executor 的执行情况  
➢ 通过 UI 展示查询运行情况      
</code></pre>
<p>所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。  </p>
<h4 id="Executor的作用"><a href="#Executor的作用" class="headerlink" title="Executor的作用"></a>Executor的作用</h4><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立  </p>
<p>Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。  </p>
<h5 id="Executor-有两个核心功能："><a href="#Executor-有两个核心功能：" class="headerlink" title="Executor 有两个核心功能："></a>Executor 有两个核心功能：</h5><pre><code>➢ 负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程
➢ 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。
  
</code></pre>
<h3 id="Master-和-Worker-Local模式时"><a href="#Master-和-Worker-Local模式时" class="headerlink" title="Master 和 Worker  (Local模式时)"></a>Master 和 Worker  (Local模式时)</h3><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker    </p>
<p>在Yarn模式时，Master 就是 RM ,Worker 就是 NM</p>
<h4 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h4><p>这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM  </p>
<h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>Worker也是进程，一个 Worker运行在集群中的一台服务器上，由 Master分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NM。    </p>
<h3 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h3><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。<br>说的简单点就是，ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是ApplicationMaster。    </p>
<h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="Executor-与-Core"><a href="#Executor-与-Core" class="headerlink" title="Executor 与 Core"></a>Executor 与 Core</h2><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。  </p>
<p>应用程序相关启动参数如下：<br>–num-executors 配置 Executor 的数量<br>–executor-memory 配置每个 Executor 的内存大小<br>–executor-cores 配置每个 Executor 的虚拟 CPU core 数量    </p>
<h2 id="并行度（Parallelism）"><a href="#并行度（Parallelism）" class="headerlink" title="并行度（Parallelism）"></a>并行度（Parallelism）</h2><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，是并行，而不是并发。  </p>
<p>将整个集群并行执行任务的数量称之为并行度。</p>
<p>一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。   </p>
<h2 id="有向无环图（DAG）"><a href="#有向无环图（DAG）" class="headerlink" title="有向无环图（DAG）"></a>有向无环图（DAG）</h2><p><img src="/2023/08/02/Spark-Core/2.png" alt="有向无环图">  </p>
<p>这里所谓的有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。    </p>
<p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。    </p>
<h2 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h2><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。  </p>
<p><img src="/2023/08/02/Spark-Core/3.png" alt="基于Yarn的Spark任务提交流程">   </p>
<p>Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client和 Cluster。两种模式主要区别在于：<strong>Driver 程序的运行节点位置</strong>。  </p>
<h3 id="Yarn-Client-模式"><a href="#Yarn-Client-模式" class="headerlink" title="Yarn Client 模式"></a>Yarn Client 模式</h3><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。  </p>
<pre><code>➢ Driver 在任务提交的本地机器上运行
➢ Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster
➢ ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存
➢ ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程  
➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数
➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。  
</code></pre>
<h3 id="Yarn-Cluster-模式"><a href="#Yarn-Cluster-模式" class="headerlink" title="Yarn Cluster 模式"></a>Yarn Cluster 模式</h3><p>Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。  </p>
<pre><code>➢ 在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster
➢ 随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。
➢ Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程  
➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数
➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行    
</code></pre>
<h1 id="Spark-核心编程"><a href="#Spark-核心编程" class="headerlink" title="Spark 核心编程"></a>Spark 核心编程</h1><p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。<strong>三大数据结构</strong>分别是：  </p>
<pre><code>➢ RDD : 弹性分布式数据集
➢ 累加器：分布式共享只写变量
➢ 广播变量：分布式共享只读变量
</code></pre>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合  </p>
<pre><code>➢ 弹性
    存储的弹性：内存与磁盘的自动切换；
    容错的弹性：数据丢失可以自动恢复；
    计算的弹性：计算出错重试机制；
    分片的弹性：可根据需要重新分片。
➢ 分布式：数据存储在大数据集群不同节点上
➢ 数据集：RDD 封装了计算逻辑，并不保存数据
➢ 数据抽象：RDD 是一个抽象类，需要子类具体实现
➢ 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑
➢ 可分区、并行计算  
</code></pre>
<h2 id="执行原理"><a href="#执行原理" class="headerlink" title="执行原理"></a>执行原理</h2><p>从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。  </p>
<p>执行时，需要将计算资源和计算模型进行协调和整合。  </p>
<p>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。  </p>
<p>在 Yarn 环境中，RDD的工作原理:  </p>
<p>1）启动Yarn集群环境  </p>
<p><img src="/2023/08/02/Spark-Core/5.png" alt="启动Yarn集群环境">   </p>
<p>2）Spark通过申请资源创建调度节点和计算节点   </p>
<p><img src="/2023/08/02/Spark-Core/6.png" alt="创建调度节点和计算节点">   </p>
<p>3）Spark框架根据需求将计算逻辑根据分区划分成不同的任务  </p>
<p><img src="/2023/08/02/Spark-Core/7.png" alt="根据分区划分成不同的任务">    </p>
<p>4）调度节点将任务根据计算节点状态发送到对应的计算节点进行计算  </p>
<p><img src="/2023/08/02/Spark-Core/8.png" alt="将任务分发给对应的计算节点进行计算"> </p>
<p>RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算  </p>
<h1 id="RDD算子总结"><a href="#RDD算子总结" class="headerlink" title="RDD算子总结"></a>RDD算子总结</h1><h2 id="Value类型总结"><a href="#Value类型总结" class="headerlink" title="Value类型总结"></a>Value类型总结</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><pre><code>｜ def map[U: ClassTag](f: T =&gt; U): RDD[U]
｜ 将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。
｜ val dataRDD1: RDD[Int] = dataRDD.map(
｜  num =&gt;
｜        &#123;   num * 2 &#125;
｜ )
｜ val dataRDD2: RDD[String] = dataRDD1.map(
｜  num =&gt; &#123;
｜  &quot;&quot; + num
｜  &#125;
｜ )
｜ ​
val mapRDD: RDD[Int] = rdd.map(_*2)
对传入的数据，一个一个的进行转换，再返回给结果集
</code></pre>
<h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><pre><code>｜ def mapPartitions[U: ClassTag](
｜  f: Iterator[T] =&gt; Iterator[U],
｜  preservesPartitioning: Boolean = false): RDD[U]
｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。
｜ val dataRDD1: RDD[Int] = dataRDD.mapPartitions(
｜  datas =&gt; &#123;
｜  datas.filter(_==2)
｜  &#125;
｜ )
｜ 
｜ 
val mpRDD: RDD[Int] = rdd.mapPartitions(
    iter =&gt; &#123;
println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;)
        iter.map(_ * 2) &#125;)
一个分区一个分区的数据进行转换，再返回给结果集
</code></pre>
<h3 id="map-和-mapPartitions-的区别："><a href="#map-和-mapPartitions-的区别：" class="headerlink" title="map 和 mapPartitions 的区别："></a>map 和 mapPartitions 的区别：</h3><pre><code>数据处理角度：
    Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作

功能的角度：
    Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。
    MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据

性能的角度：
    Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高
    但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作
</code></pre>
<h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h3><pre><code>｜ def mapPartitionsWithIndex[U: ClassTag](
｜  f: (Int, Iterator[T]) =&gt; Iterator[U],
｜  preservesPartitioning: Boolean = false): RDD[U]
｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引
｜ val dataRDD1 = dataRDD.mapPartitionsWithIndex(
｜  (index, datas) =&gt; &#123;
｜  datas.map(index, _)
｜  &#125;
｜ )
｜ 
｜ 
mapPartitionsWithIndex在mapPartitions基础上加上了分区index
val mpiRDD = rdd.mapPartitionsWithIndex(
  (index,iter) =&gt; &#123;
// 1 ,     2 ,     3 ,    4    // (0,1)   (2,2)   (4,3)  (6,4)    iter.map(
      num =&gt; &#123; (index,num) &#125;  ）&#125; )
</code></pre>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>｜ def flatMap[U: ClassTag](f: T &#x3D;&gt; TraversableOnce[U]): RDD[U]<br>｜ 将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射<br>｜ val dataRDD1 &#x3D; dataRDD.flatMap( list &#x3D;&gt; list)<br>｜<br>    val flatRDD:RDD[String] &#x3D; rdd.flatMap( s &#x3D;&gt; { s.split(“ “)  })<br>        Hello<br>        Scala<br>        Hello<br>        Spark</p>
<pre><code>如果使用rdd.map( s =&gt; &#123; s.split(&quot; &quot;)  &#125;)，会发现打印的结果是
    [Ljava.lang.String;@f1a45f8
    [Ljava.lang.String;@5edf2821

所以切割等扁平映射操作，选用flatMap
</code></pre>
<h3 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h3><p>｜ def glom(): RDD[Array[T]]<br>｜ 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变<br>｜ val dataRDD1:RDD[Array[Int]] &#x3D; dataRDD.glom()<br>    将同一个分区的数据直接转换为相同类型的内存数组进行处理<br>    List[Int] &#x3D;&gt; Array[Int]</p>
<h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><pre><code>｜ def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]
｜ 将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。极限情况下，数据可能被分在同一个分区中
｜ ​
val groupRDD:RDD[(Int,Iterable[Int])] = rdd.groupBy(num % 2)
    (0,CompactBuffer(2, 4))
    (1,CompactBuffer(1, 3))
    会输出一个RDD[(K, Iterable[T])]
</code></pre>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><pre><code>｜ 将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。
val dataRDD1 = dataRDD.filter(_%2 == 0)
</code></pre>
<h3 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h3><p>｜ def sample(<br>｜ withReplacement: Boolean,<br>｜  fraction: Double,<br>｜  seed: Long &#x3D; Utils.random.nextLong): RDD[T]<br>｜ 根据指定的规则从数据集中抽取数据<br>｜<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4<br>    ｜ ),1)<br>｜ &#x2F;&#x2F; 抽取数据不放回（伯努利算法）<br>｜ &#x2F;&#x2F; 伯努利算法：又叫 0、1 分布。例如扔硬币，要么正面，要么反面。<br>｜ &#x2F;&#x2F; 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不<br>｜ 要<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD1 &#x3D; dataRDD.sample(false, 0.5)<br>｜ &#x2F;&#x2F; 抽取数据放回（泊松算法）<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，true：放回；false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：重复数据的几率，范围大于等于 0.表示每一个元素被期望抽取到的次数<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD2 &#x3D; dataRDD.sample(true, 2)<br>｜ ​<br>｜<br>    相同的seed种子，多次运行依旧是相同的抽样结果,修改withReplacement也不会发生变化，&#x2F;&#x2F; 修改fraction后结果会发生改变<br>    rdd.sample(true,0.5,101)</p>
<h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>｜ def distinct()(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>｜ def distinct(numPartitions: Int)(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>    ｜ 将数据集中重复的数据去重<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4,1,2<br>    ｜ ),1)<br>    ｜ val dataRDD1 &#x3D; dataRDD.distinct()<br>    ｜ val dataRDD2 &#x3D; dataRDD.distinct(2)<br>｜ ​<br>｜ </p>
<h3 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h3><p>｜ def coalesce(numPartitions: Int, shuffle: Boolean &#x3D; false,<br>｜  partitionCoalescer: Option[PartitionCoalescer] &#x3D; Option.empty)<br>｜  (implicit ord: Ordering[T] &#x3D; null)<br>｜  : RDD[T]<br>｜ 根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率<br>｜ 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少<br>｜ 分区的个数，减小任务调度成本<br>｜ ​<br>｜<br>    &#x2F;&#x2F; coalesce 方法默认情况下不会将分区的数据打乱重新组合<br>    &#x2F;&#x2F; 这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜<br>    &#x2F;&#x2F; 如果想要让数据倾斜，可以进行shuffle处理<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2,shuffle &#x3D; false)<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2) 默认不进行shuffle<br>    &#x2F;&#x2F; 进行shuffle处理后会出现数据倾斜val newRDD &#x3D; rdd.coalesce(2, true)</p>
<h3 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h3><pre><code>｜ def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
repartition 操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true
</code></pre>
<h3 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h3><p>｜ def sortBy[K](<br>｜  f: (T) &#x3D;&gt; K,<br>｜ ascending: Boolean &#x3D; true,<br>｜  numPartitions: Int &#x3D; this.partitions.length)<br>｜  (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]<br>｜ 该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，之后按照 f 函数处理<br>｜ 的结果进行排序，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一<br>｜ 致。中间存在 shuffle 的过程<br>｜ ​<br>｜ ​<br>    默认升序排序<br>    val dataRDD1 &#x3D; dataRDD.sortBy(num&#x3D;&gt;num, false, 4)<br>    val newRDD &#x3D; rdd.sortBy(t &#x3D;&gt; t._1.toInt, true)</p>
<h2 id="双-Value-类型总结"><a href="#双-Value-类型总结" class="headerlink" title="双 Value 类型总结"></a>双 Value 类型总结</h2><h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><pre><code>def intersection(other: RDD[T]): RDD[T]
val dataRDD = dataRDD1.intersection(dataRDD2)
求交集
</code></pre>
<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><pre><code>def union(other: RDD[T]): RDD[T]
val dataRDD = dataRDD1.union(dataRDD2)
求并集
</code></pre>
<h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h3><pre><code>def subtract(other: RDD[T]): RDD[T]
val dataRDD = dataRDD1.subtract(dataRDD2)
求差集
</code></pre>
<h3 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h3><pre><code>def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]
val dataRDD = dataRDD1.zip(dataRDD2)
将两个 RDD 中的元素，以键值对的形式进行合并。其中，键值对中的 Key 为第 1 个 RDD中的元素，Value 为第 2 个 RDD 中的相同位置的元素
</code></pre>
<h2 id="Key-Value类型总结"><a href="#Key-Value类型总结" class="headerlink" title="Key-Value类型总结"></a>Key-Value类型总结</h2><h3 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h3><pre><code>def partitionBy(partitioner: Partitioner): RDD[(K, V)]
将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner
val rdd2: RDD[(Int, String)] = rdd.partitionBy(new HashPartitioner(2))
</code></pre>
<h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3><pre><code>def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]
def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]
可以将数据按照相同的 Key 对 Value 进行聚合
val dataRDD2 = dataRDD1.reduceByKey(_+_)
</code></pre>
<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><pre><code>def groupByKey(): RDD[(K, Iterable[V])]
def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
将数据源的数据根据 key 对 value 进行分组
val dataRDD2 = dataRDD1.groupByKey()
val dataRDD3 = dataRDD1.groupByKey(2)
val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2))
</code></pre>
<h3 id="reduceByKey-和-groupByKey-的区别："><a href="#reduceByKey-和-groupByKey-的区别：" class="headerlink" title="reduceByKey 和 groupByKey 的区别："></a>reduceByKey 和 groupByKey 的区别：</h3><pre><code>从 shuffle 的角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的
数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。
从功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey
</code></pre>
<h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h3><pre><code>def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): RDD[(K, U)]
将数据根据不同的规则进行分区内计算和分区间计算
dataRDD1.aggregateByKey(0)(_+_,_+_)
｜ // 1. 第一个参数列表中的参数表示初始值
｜ // 2. 第二个参数列表中含有两个参数
｜ // 2.1 第一个参数表示分区内的计算规则
｜ // 2.2 第二个参数表示分区间的计算规则
｜ 
</code></pre>
<h3 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h3><pre><code>def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]
当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey
</code></pre>
<h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h3><pre><code>def combineByKey[C](
createCombiner: V =&gt; C,
 mergeValue: (C, V) =&gt; C,
 mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]
最通用的对 key-value 型 rdd 进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致
val combineRdd: RDD[(String, (Int, Int))] = input.combineByKey(
 (_, 1),
 (acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1),
 (acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2))
</code></pre>
<h3 id="reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别："><a href="#reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别：" class="headerlink" title="reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别："></a>reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别：</h3><pre><code>｜ reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同
｜ FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同
｜ AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同
｜ CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同
｜ 
</code></pre>
<h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3><pre><code>def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length): RDD[(K, V)]
val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(true)
val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(false)
</code></pre>
<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><pre><code>def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的(K,(V,W))的 RDD
rdd.join(rdd1).collect().foreach(println)
</code></pre>
<h3 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h3><pre><code>def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]
类似于 SQL 语句的左外连接
val rdd: RDD[(String, (Int, Option[Int]))] = dataRDD1.leftOuterJoin(dataRDD2)
</code></pre>
<h3 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h3><pre><code>类似于 SQL 语句的右外连接
</code></pre>
<h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><pre><code>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))类型的 RDD
val value: RDD[(String, (Iterable[Int], Iterable[Int]))] = dataRDD1.cogroup(dataRDD2)
(a,(CompactBuffer(1),CompactBuffer(4)))
(b,(CompactBuffer(2),CompactBuffer(5)))
(c,(CompactBuffer(3),CompactBuffer(6, 7)))
</code></pre>
<h2 id="Spark行动算子"><a href="#Spark行动算子" class="headerlink" title="Spark行动算子"></a>Spark行动算子</h2><h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>在驱动程序中，以数组 Array 的形式返回数据集的所有元素  </p>
<pre><code>// 收集数据到 Driver
rdd.collect().foreach(println)  
</code></pre>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>返回 RDD 中元素的个数  </p>
<pre><code>// 返回 RDD 中元素的个数
val countResult: Long = rdd.count()  
</code></pre>
<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>返回 RDD 中的第一个元素  </p>
<pre><code>// 返回 RDD 中元素的第1个元素
val firstResult: Int = rdd.first()  
</code></pre>
<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>返回一个由 RDD 的前 n 个元素组成的数组  </p>
<pre><code>// 返回 RDD 中元素的个数
val takeResult: Array[Int] = rdd.take(2)
println(takeResult.mkString(&quot;,&quot;))  
</code></pre>
<h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>返回该 RDD 排序后的前 n 个元素组成的数组  </p>
<pre><code>// 返回 RDD 中元素的个数
val result: Array[Int] = rdd.takeOrdered(2)  
</code></pre>
<h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合  </p>
<pre><code>val result: Int = rdd.aggregate(10)(_ + _, _ + _)  
</code></pre>
<h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p>折叠操作，aggregate 的简化版操作  </p>
<pre><code>val foldResult: Int = rdd.fold(0)(_+_)  
</code></pre>
<h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>统计每种 key 的个数  </p>
<pre><code>// 统计每种 key 的个数
val result: collection.Map[Int, Long] = rdd.countByKey()  
</code></pre>
<h3 id="save-相关算子"><a href="#save-相关算子" class="headerlink" title="save 相关算子"></a>save 相关算子</h3><p>将数据保存到不同格式的文件中  </p>
<pre><code>// 保存成 Text 文件
rdd.saveAsTextFile(&quot;output&quot;)  
</code></pre>
<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><pre><code>// 收集后打印
rdd.map(num=&gt;num).collect().foreach(println)  
</code></pre>
<h2 id="RDD-序列化"><a href="#RDD-序列化" class="headerlink" title="RDD 序列化"></a>RDD 序列化</h2><h3 id="闭包检查"><a href="#闭包检查" class="headerlink" title="闭包检查"></a>闭包检查</h3><p>从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。</p>
<h1 id="RDD-依赖关系"><a href="#RDD-依赖关系" class="headerlink" title="RDD 依赖关系"></a>RDD 依赖关系</h1><h2 id="1）RDD血缘关系"><a href="#1）RDD血缘关系" class="headerlink" title="1）RDD血缘关系"></a>1）RDD血缘关系</h2><p>RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage（血统）记录下来，以便恢复丢失的分区。    </p>
<p>RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。   </p>
<pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)
println(fileRDD.toDebugString)
</code></pre>
<h2 id="2）RDD依赖关系"><a href="#2）RDD依赖关系" class="headerlink" title="2）RDD依赖关系"></a>2）RDD依赖关系</h2><p>所谓的依赖关系，其实就是两个相邻 RDD 之间的关系。  </p>
<pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)
println(fileRDD.dependencies)   
</code></pre>
<h2 id="3）RDD-窄依赖（没有Shuffle）"><a href="#3）RDD-窄依赖（没有Shuffle）" class="headerlink" title="3）RDD 窄依赖（没有Shuffle）"></a>3）RDD 窄依赖（没有Shuffle）</h2><p>窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，窄依赖我们形象的比喻为独生子女。</p>
<h2 id="4）RDD宽依赖-（有Shuffle）"><a href="#4）RDD宽依赖-（有Shuffle）" class="headerlink" title="4）RDD宽依赖 （有Shuffle）"></a>4）RDD宽依赖 （有Shuffle）</h2><p>宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。    </p>
<h2 id="RDD-任务划分"><a href="#RDD-任务划分" class="headerlink" title="RDD 任务划分"></a>RDD 任务划分</h2><p>RDD 任务切分中间分为：Application、Job、Stage 和 Task  </p>
<pre><code>Application：初始化一个 SparkContext 即生成一个 Application；
Job：一个 Action 算子就会生成一个 Job；
Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；
Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数。  
</code></pre>
<p>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系</p>
<p><img src="/2023/08/02/Spark-Core/9.png" alt="Spark任务划分流程">     </p>
<h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><h3 id="1-RDD-Cache-缓存"><a href="#1-RDD-Cache-缓存" class="headerlink" title="1) RDD Cache 缓存"></a>1) RDD Cache 缓存</h3><p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用。  </p>
<pre><code>// cache 操作会增加血缘关系，不改变原有的血缘关系  

// 可以更改存储级别
//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)  
</code></pre>
<h4 id="RDD持久化可选存储级别"><a href="#RDD持久化可选存储级别" class="headerlink" title="RDD持久化可选存储级别"></a>RDD持久化可选存储级别</h4><pre><code>object StorageLevel &#123;
 val NONE = new StorageLevel(false, false, false, false)
 val DISK_ONLY = new StorageLevel(true, false, false, false)
 val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)
 val MEMORY_ONLY = new StorageLevel(false, true, false, true)
 val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)
 val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)
 val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)
 val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)
 val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)
 val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)
 val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)
 val OFF_HEAP = new StorageLevel(true, true, true, false, 1)  
</code></pre>
<p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD 的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。  </p>
<p>Spark 会自动对一些 Shuffle 操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点 Shuffle 失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用 persist 或 cache。  </p>
<h3 id="2）RDD-CheckPoint-检查点"><a href="#2）RDD-CheckPoint-检查点" class="headerlink" title="2）RDD CheckPoint 检查点"></a>2）RDD CheckPoint 检查点</h3><p>所谓的检查点其实就是通过将 RDD 中间结果写入磁盘  </p>
<p>对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发。  </p>
<ol start="3">
<li><p>缓存和检查点区别  </p>
<p> 1）Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖。<br> 2）Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高。<br> 3）建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD</p>
</li>
</ol>
<h2 id="RDD-分区器"><a href="#RDD-分区器" class="headerlink" title="RDD 分区器"></a>RDD 分区器</h2><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认分区。   </p>
<p>分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。   </p>
<pre><code>➢ 只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None  
➢ 每个 RDD 的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。  
</code></pre>
<ol>
<li><p>Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余  </p>
</li>
<li><p>Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p>
</li>
</ol>
<h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge。  </p>
<h3 id="系统累加器"><a href="#系统累加器" class="headerlink" title="系统累加器"></a>系统累加器</h3><pre><code>val rdd = sc.makeRDD(List(1,2,3,4,5))
// 声明累加器
var sum = sc.longAccumulator(&quot;sum&quot;);
rdd.foreach(
 num =&gt; &#123;
 // 使用累加器
 sum.add(num)
 &#125;
)
// 获取累加器的值
println(&quot;sum = &quot; + sum.value)
</code></pre>
<h3 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h3><pre><code>// 自定义累加器
// 1. 继承 AccumulatorV2，并设定泛型
// 2. 重写累加器的抽象方法
class WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, 
Long]]&#123;
var map : mutable.Map[String, Long] = mutable.Map()
// 累加器是否为初始状态
override def isZero: Boolean = &#123;
 map.isEmpty
&#125;
// 复制累加器
override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = &#123;
 new WordCountAccumulator
&#125;
// 重置累加器
override def reset(): Unit = &#123;
 map.clear()
&#125;
// 向累加器中增加数据 (In)
override def add(word: String): Unit = &#123;
 // 查询 map 中是否存在相同的单词
 // 如果有相同的单词，那么单词的数量加 1
 // 如果没有相同的单词，那么在 map 中增加这个单词
 map(word) = map.getOrElse(word, 0L) + 1L
&#125;
// 合并累加器
override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): 
Unit = &#123;
 val map1 = map
 val map2 = other.value
 // 两个 Map 的合并
 map = map1.foldLeft(map2)(
 ( innerMap, kv ) =&gt; &#123;
 innerMap(kv._1) = innerMap.getOrElse(kv._1, 0L) + kv._2
 innerMap
 &#125;
 )
&#125;
// 返回累加器的结果 （Out）
override def value: mutable.Map[String, Long] = map
&#125;
</code></pre>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。  </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">SQLServer查询体系学习记录</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-01 11:07:31" itemprop="dateCreated datePublished" datetime="2023-08-01T11:07:31+08:00">2023-08-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 12:59:20" itemprop="dateModified" datetime="2023-08-11T12:59:20+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="SQLServer查询处理体系结构指南"><a href="#SQLServer查询处理体系结构指南" class="headerlink" title="SQLServer查询处理体系结构指南"></a>SQLServer查询处理体系结构指南</h1><h2 id="执行模式"><a href="#执行模式" class="headerlink" title="执行模式"></a>执行模式</h2><p>行执行模式<br>批执行模式 </p>
<h3 id="行执行模式"><a href="#行执行模式" class="headerlink" title="行执行模式"></a>行执行模式</h3><p>行模式执行是用于传统 RDBMS 表（其中数据以行格式存储）的查询处理方法</p>
<h3 id="批执行模式"><a href="#批执行模式" class="headerlink" title="批执行模式"></a>批执行模式</h3><p>批模式执行是一种查询处理方法，用于统一处理多个行（因此采用“批”一词）    </p>
<p>批中的每列都作为一个矢量存储在单独的内存区域中，因此批模式处理是基于矢量的</p>
<p>当在批模式下执行查询并且查询访问列存储索引中的数据时，执行树运算符和子运算符会一次读取列段中的多行。 SQL Server 仅读取结果所需的列，即 SELECT 语句、JOIN 谓词或筛选谓词引用的列</p>
<p><strong>批执行模式VS行执行模式的优势</strong>：一次读取多行，再筛选。避免了行执行模式的多次读取，提高运行效率  </p>
<h2 id="SQL-语句处理"><a href="#SQL-语句处理" class="headerlink" title="SQL 语句处理"></a>SQL 语句处理</h2><p>处理单个 Transact-SQL 语句是 SQL Server 执行 Transact-SQL 语句的最基本方法    </p>
<h3 id="逻辑运算符优先级"><a href="#逻辑运算符优先级" class="headerlink" title="逻辑运算符优先级"></a>逻辑运算符优先级</h3><p>计算顺序依次为：NOT、AND最后是 OR。算术运算符和位运算符优先于逻辑运算符处理  </p>
<h2 id="优化-SELECT-语句"><a href="#优化-SELECT-语句" class="headerlink" title="优化 SELECT 语句"></a>优化 SELECT 语句</h2><p>语句 SELECT 是非过程性的，数据库服务器必须分析语句，以决定提取所请求数据的最有效方法。  </p>
<p>执行此操作的组件称为查询优化器，可以使数据库服务器针对数据库内的更改情况进行动态调整，而无需程序员或数据库管理员输入  </p>
<p>order 排序字段上应该建索引  </p>
<p>从潜在的多个可能的计划中选择一个执行计划的过程称为“优化”  </p>
<p>SQL Server 查询优化器是基于成本的优化器（CBO）</p>
<p>当执行复杂的SQL语句时，不会去分析所有的执行计划成本，会根据算法选一个执行计划，其成本合理地接近最低可能成本的执行计划    </p>
<p><strong>除了CBO，还要考虑运行效率</strong>：SQL Server查询优化器不会仅选择资源成本最低的执行计划;它选择以合理的资源成本向用户返回结果的计划，并且以最快的速度返回结果  </p>
<p>SQL Server 查询优化器总能针对数据库的状态生成一个有效的执行计划    </p>
<p>SQL Server Management Studio 有三个选项可用于显示执行计划</p>
<pre><code>1.估计的执行计划    

2.实际执行计划    

3.实时查询统计信息，这与编译的计划及其执行上下文相同  

    这包括执行过程中的运行时信息，每秒更新一次
</code></pre>
<h2 id="密度"><a href="#密度" class="headerlink" title="密度"></a>密度</h2><p>密度定义数据中存在的唯一值的分布，或给定列的重复值平均数。 密度与值的选择性成反比，密度越小，值的选择性越大  </p>
<h2 id="处理-SELECT-语句"><a href="#处理-SELECT-语句" class="headerlink" title="处理 SELECT 语句"></a>处理 SELECT 语句</h2><p>SQL Server 处理单个 SELECT 语句的基本步骤包括如下内容：  </p>
<p>1.解析select语句</p>
<p>2.生成查询树</p>
<p>3.生成执行计划，并选择合理的执行计划</p>
<p>4.运行执行计划  </p>
<p>5.返回结果</p>
<h2 id="常量折叠和表达式计算"><a href="#常量折叠和表达式计算" class="headerlink" title="常量折叠和表达式计算"></a>常量折叠和表达式计算</h2><h3 id="可折叠表达式"><a href="#可折叠表达式" class="headerlink" title="可折叠表达式"></a>可折叠表达式</h3><p>仅包含常量的算术表达式  </p>
<p>仅包含常量的逻辑表达式  </p>
<p>被 SQL Server 认为可折叠的内置函数包括 CAST 和 CONVERT   </p>
<p>CLR 用户定义类型的确定性方法和确定性的标量值 CLR 用户定义函数  </p>
<h3 id="不可折叠的表达式"><a href="#不可折叠的表达式" class="headerlink" title="不可折叠的表达式"></a>不可折叠的表达式</h3><p>所有其他表达式类型都是不可折叠的。 特别是下列类型的表达式是不可折叠的：  </p>
<p>非常量表达式    </p>
<p>结果取决于局部变量或参数的表达式   </p>
<p>不确定性函数     </p>
<p>用户定义的 Transact-SQL 函数  </p>
<p>结果取决于语言设置的表达式   </p>
<p>结果取决于 SET 选项的表达式  </p>
<p>结果取决于服务器配置选项的表达式  </p>
<h2 id="常量折叠的优点"><a href="#常量折叠的优点" class="headerlink" title="常量折叠的优点"></a>常量折叠的优点</h2><p>表达式不必在运行时重复计算  </p>
<p>查询优化器可使用计算表达式后所得的值来估计 TotalDue &gt; 117.00 + 1000.00 查询部分的结果集的大小  </p>
<h2 id="工作表"><a href="#工作表" class="headerlink" title="工作表"></a>工作表</h2><p>工作表是用于保存中间结果的内部表。 某些 GROUP BY、 ORDER BY或 UNION 查询会生成工作表  </p>
<p>工作表在 tempdb 中生成，并在不再需要时自动删除  </p>
<p>SQL Server 查询处理器对索引视图和非索引视图将区别对待  </p>
<p>索引视图的行以表的格式存储在数据库中   </p>
<p>只有非索引视图的定义才存储，而不存储视图的行  </p>
<p> 如果索引视图中的数据包括所有或部分 Transact-SQL 语句，而且查询优化器确定视图的某个索引是低成本的访问路径，则不论查询中是否引用了该视图的名称，查询优化器都将选择此索引  </p>
<p>视图没有单独的执行计划</p>
<h2 id="存储过程和触发器执行"><a href="#存储过程和触发器执行" class="headerlink" title="存储过程和触发器执行"></a>存储过程和触发器执行</h2><p>SQL Server 仅存储存储过程和触发器的源  </p>
<p>第一次执行存储过程或触发器时，源被编译为执行计划，在内存中被释放后需要重新运行存储过程再次生成  </p>
<p>执行计划缓存和重用  </p>
<p>SQL Server 有一个用于存储执行计划和数据缓冲区的内存池，池内分配给执行计划或数据缓冲区的百分比随系统状态动态波动    </p>
<p>计划缓存有两个不用于存储计划的附加存储：  </p>
<p>“对象计划”缓存存储 (OBJCP)  </p>
<pre><code>用于与持久化对象（存储过程、函数和触发器）相关的计划   
</code></pre>
<p>“SQL 计划”缓存存储 (SQLCP)   </p>
<pre><code>用于与自动参数化、动态或已准备的查询相关的计划  
</code></pre>
<h2 id="从计划缓存中删除执行计划"><a href="#从计划缓存中删除执行计划" class="headerlink" title="从计划缓存中删除执行计划"></a>从计划缓存中删除执行计划</h2><p>只要计划缓存中有足够的存储空间，执行计划就会保留在其中   </p>
<p>当存在内存不足的情况时，SQL Server 数据库引擎将使用基于开销的方法来确定从计划缓存中删除哪些执行计划  </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/31/Hive-on-mr/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/31/Hive-on-mr/" class="post-title-link" itemprop="url">Hive on mr调优</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-31 15:57:39" itemprop="dateCreated datePublished" datetime="2023-07-31T15:57:39+08:00">2023-07-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:05:08" itemprop="dateModified" datetime="2023-08-11T13:05:08+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="我于人间全无敌，不与天战与shui战？"><a href="#我于人间全无敌，不与天战与shui战？" class="headerlink" title="我于人间全无敌，不与天战与shui战？"></a>我于人间全无敌，不与天战与shui战？</h1><p>本文测试数据和Explain可视化工具资料包<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1Q8Zt7gWBF6JkW_Kg6sIWzw">https://pan.baidu.com/s/1Q8Zt7gWBF6JkW_Kg6sIWzw</a><br>提取码：2khx     </p>
<p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1Wdc2f38csrGbC3GA1s0H_w">https://pan.baidu.com/s/1Wdc2f38csrGbC3GA1s0H_w</a><br>提取码：888i   </p>
<p>永久有效，失效来打我~<br><img src="/2023/07/31/Hive-on-mr/1.gif"></p>
<p>Hive on mr  </p>
<p>即Hive引擎选用mapreduce。（目前Hive引擎可选项为Mapreduce&#x2F;Tez&#x2F;Spark）  </p>
<p>调优主要分为下面三个方向 </p>
<p>1)：组件资源调优  </p>
<p>通过控制任务运行的组件资源，实现任务的高效运行</p>
<p>2)：Explain执行计划调优  </p>
<p>通过优化执行计划,保证相同资源配置的情况下，任务运行更流畅  </p>
<p>3): 常有调优参数设置  </p>
<p>开启Hive内置的一些有助于任务高效运行的设置,保障任务流畅运行  </p>
<h2 id="1-组件资源调优"><a href="#1-组件资源调优" class="headerlink" title="1:组件资源调优"></a>1:组件资源调优</h2><h3 id="Yarn资源配置"><a href="#Yarn资源配置" class="headerlink" title="Yarn资源配置"></a>Yarn资源配置</h3><p>需要调整的Yarn参数均与CPU、内存等资源有关，核心配置参数如下  </p>
<p>（1）yarn.nodemanager.resource.memory-mb  </p>
<p>该参数的含义是，一个NodeManager节点分配给Container使用的内存。该参数的配置，<strong>取决于NodeManager所在节点的总内存容量和该节点运行的其他服务的数量</strong>。  </p>
<p>（2）yarn.nodemanager.resource.cpu-vcores  </p>
<p>该参数的含义是，一个NodeManager节点分配给Container使用的CPU核数。该参数的配置，<strong>同样取决于NodeManager所在节点的总CPU核数和该节点运行的其他服务</strong>。</p>
<p>通常是一个核4个G  </p>
<pre><code>即（1）yarn.nodemanager.resource.memory-mb/（2）yarn.nodemanager.resource.cpu-vcores  = 4  
</code></pre>
<p>（3）yarn.scheduler.maximum-allocation-mb  </p>
<p>该参数的含义是，单个Container能够使用的最大内存</p>
<pre><code>（1）yarn.nodemanager.resource.memory-mb /（3）yarn.scheduler.maximum-allocation-mb  = 整数
</code></pre>
<p>（4）yarn.scheduler.minimum-allocation-mb  </p>
<p>该参数的含义是，单个Container能够使用的最小内存，推荐配置(512M)如下：  </p>
<pre><code>&lt;property&gt;
    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
    &lt;value&gt;512&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<h3 id="MapReduce资源配置"><a href="#MapReduce资源配置" class="headerlink" title="MapReduce资源配置"></a>MapReduce资源配置</h3><p>MapReduce资源配置主要包括Map Task的内存和CPU核数，以及Reduce Task的内存和CPU核数  </p>
<p>1）mapreduce.map.memory.mb	  </p>
<p>该参数的含义是，单个Map Task申请的container容器内存大小，其默认值为1024。该值不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围  </p>
<p>该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置： </p>
<pre><code>set  mapreduce.map.memory.mb=2048;  
</code></pre>
<p>2）mapreduce.map.cpu.vcores  </p>
<p>该参数的含义是，单个Map Task申请的container容器cpu核数，其默认值为1。该值一般无需调整</p>
<p>3）mapreduce.reduce.memory.mb	</p>
<p>该参数的含义是，单个Reduce Task申请的container容器内存大小，其默认值为1024。该值同样不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围  </p>
<p>该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置  </p>
<pre><code>set  mapreduce.reduce.memory.mb=2048;  
</code></pre>
<p>4）mapreduce.reduce.cpu.vcores	  </p>
<p>该参数的含义是，单个Reduce Task申请的container容器cpu核数，其默认值为1。该值一般无需调整  </p>
<h1 id="2-Explain执行计划调优"><a href="#2-Explain执行计划调优" class="headerlink" title="2.Explain执行计划调优"></a>2.Explain执行计划调优</h1><h2 id="测试用表"><a href="#测试用表" class="headerlink" title="测试用表"></a>测试用表</h2><h3 id="1-订单表-2000w条数据"><a href="#1-订单表-2000w条数据" class="headerlink" title="1.订单表(2000w条数据)"></a>1.订单表(2000w条数据)</h3><h4 id="建表语句"><a href="#建表语句" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt;
drop table if exists order_detail;
create table order_detail(
id           string comment &#39;订单id&#39;,
user_id      string comment &#39;用户id&#39;,
product_id   string comment &#39;商品id&#39;,
province_id  string comment &#39;省份id&#39;,
create_time  string comment &#39;下单时间&#39;,
product_num  int comment &#39;商品件数&#39;,
total_amount decimal(16, 2) comment &#39;下单金额&#39;
)
partitioned by (dt string)
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<h4 id="数据装载"><a href="#数据装载" class="headerlink" title="数据装载"></a>数据装载</h4><p>将order_detail.txt文件上传到hiveserver2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。</p>
<p>注：文件较大，请耐心等待。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/order_detail.txt&#39; overwrite into table order_detail partition(dt=&#39;2020-06-14&#39;); 
</code></pre>
<h3 id="2-支付表-600w条数据"><a href="#2-支付表-600w条数据" class="headerlink" title="2.支付表(600w条数据)"></a>2.支付表(600w条数据)</h3><h4 id="建表语句-1"><a href="#建表语句-1" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt;
drop table if exists payment_detail;
create table payment_detail(
id              string comment &#39;支付id&#39;,
order_detail_id string comment &#39;订单明细id&#39;,
user_id         string comment &#39;用户id&#39;,
payment_time    string comment &#39;支付时间&#39;,
total_amount    decimal(16, 2) comment &#39;支付金额&#39;
)
partitioned by (dt string)
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<h4 id="数据装载-1"><a href="#数据装载-1" class="headerlink" title="数据装载"></a>数据装载</h4><p>将payment_detail.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。</p>
<p>注：文件较大，请耐心等待。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/payment_detail.txt&#39; overwrite into table payment_detail partition(dt=&#39;2020-06-14&#39;);  
</code></pre>
<h3 id="3-商品信息表-100w条数据"><a href="#3-商品信息表-100w条数据" class="headerlink" title="3.商品信息表(100w条数据)"></a>3.商品信息表(100w条数据)</h3><h4 id="建表语句-2"><a href="#建表语句-2" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt; 
drop table if exists product_info;
create table product_info(
id           string comment &#39;商品id&#39;,
product_name string comment &#39;商品名称&#39;,
price        decimal(16, 2) comment &#39;价格&#39;,
category_id  string comment &#39;分类id&#39;
)
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<h4 id="数据装载-2"><a href="#数据装载-2" class="headerlink" title="数据装载"></a>数据装载</h4><p>将product_info.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/product_info.txt&#39; overwrite into table product_info;  
</code></pre>
<h3 id="4-省份信息表-34条数据"><a href="#4-省份信息表-34条数据" class="headerlink" title="4.省份信息表(34条数据)"></a>4.省份信息表(34条数据)</h3><h4 id="建表语句-3"><a href="#建表语句-3" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt; 
drop table if exists province_info;
create table province_info(
id            string comment &#39;省份id&#39;,
province_name string comment &#39;省份名称&#39;
)
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<h4 id="数据装载-3"><a href="#数据装载-3" class="headerlink" title="数据装载"></a>数据装载</h4><p>将province_info.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/province_info.txt&#39; overwrite into table province_info;  
</code></pre>
<h2 id="Explain查看执行计划（重点）"><a href="#Explain查看执行计划（重点）" class="headerlink" title="Explain查看执行计划（重点）"></a>Explain查看执行计划（重点）</h2><p>Explain呈现的执行计划，由一系列Stage组成，这一系列Stage具有依赖关系，每个Stage对应一个MapReduce Job，或者一个文件系统操作等。  </p>
<p>若某个Stage对应的一个MapReduce Job，其Map端和Reduce端的计算逻辑分别由Map Operator Tree和Reduce Operator Tree进行描述，Operator Tree由一系列的Operator组成，一个Operator代表在Map或Reduce阶段的一个单一的逻辑操作，例如TableScan Operator，Select Operator，Join Operator等</p>
<h3 id="常见的Operator及其作用如下："><a href="#常见的Operator及其作用如下：" class="headerlink" title="常见的Operator及其作用如下："></a>常见的Operator及其作用如下：</h3><p>TableScan：表扫描操作，通常map端第一个操作肯定是表扫描操作  </p>
<p>Select Operator：选取操作   </p>
<p>Group By Operator：分组聚合操作  </p>
<p>Reduce Output Operator：输出到 reduce 操作  </p>
<p>Filter Operator：过滤操作  </p>
<p>Join Operator：join 操作  </p>
<p>File Output Operator：文件输出操作  </p>
<p>Fetch Operator 客户端获取数据操作  </p>
<h3 id="Explain查看执行计划基本语法"><a href="#Explain查看执行计划基本语法" class="headerlink" title="Explain查看执行计划基本语法"></a>Explain查看执行计划基本语法</h3><p>EXPLAIN [FORMATTED | EXTENDED | DEPENDENCY] query-sql  </p>
<pre><code>FORMATTED：将执行计划以JSON字符串的形式输出  

EXTENDED：输出执行计划中的额外信息，通常是读写的文件名等信息  

DEPENDENCY：输出执行计划读取的表及分区  
</code></pre>
<h3 id="Explain执行计划可视化工具使用方法"><a href="#Explain执行计划可视化工具使用方法" class="headerlink" title="Explain执行计划可视化工具使用方法"></a>Explain执行计划可视化工具使用方法</h3><p>1.将本文开头百度网盘共享的dist文件压缩包上传至linux服务器中  </p>
<p>2.unzip dist.zip 解压  </p>
<p>3.进入解压后的dist文件夹下  </p>
<p>4.python -m SimpleHTTPServer 8901  启动可视化工具（此处的8901是我指定的可用端口号，可以按自己的想法设置）<br><img src="/2023/07/31/Hive-on-mr/1.png" alt="Explain可视化工具">  </p>
<p>5.将Explain执行计划制作成json格式粘贴到可视化工具里即可查看  </p>
<p><img src="/2023/07/31/Hive-on-mr/2.png" alt="Explain执行计划json格式">  </p>
<p>6.Explain可视化工具示例  </p>
<p><img src="/2023/07/31/Hive-on-mr/3.png" alt="示例">  </p>
<h2 id="HQL语法优化之分组聚合优化-（map-site）"><a href="#HQL语法优化之分组聚合优化-（map-site）" class="headerlink" title="HQL语法优化之分组聚合优化 （map-site）"></a>HQL语法优化之分组聚合优化 （map-site）</h2><p>Hive对分组聚合的优化主要围绕着减少Shuffle数据量进行，具体做法是map-side聚合  </p>
<p>在map端维护一个hash table进行预聚合，按照分组字段分区，发送至reduce端，完成最终的聚合。有效的减少shuffle操作的数量，达到提高运行效率的目的。  </p>
<h3 id="map-side-聚合相关的参数如下："><a href="#map-side-聚合相关的参数如下：" class="headerlink" title="map-side 聚合相关的参数如下："></a>map-side 聚合相关的参数如下：</h3><p>–启用map-side聚合 </p>
<pre><code>set hive.map.aggr=true;  
</code></pre>
<p>–用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。  </p>
<pre><code>set hive.map.aggr.hash.min.reduction=0.5;  
</code></pre>
<p>–用于检测源表是否适合map-side聚合的条数  </p>
<pre><code>set hive.groupby.mapaggr.checkinterval=100000;  
</code></pre>
<p>–map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。  </p>
<pre><code>set hive.map.aggr.hash.force.flush.memory.threshold=0.9;  
</code></pre>
<h3 id="HQL语法优化之分组聚合优化-（map-site）优化案例"><a href="#HQL语法优化之分组聚合优化-（map-site）优化案例" class="headerlink" title="HQL语法优化之分组聚合优化 （map-site）优化案例"></a>HQL语法优化之分组聚合优化 （map-site）优化案例</h3><pre><code>select
product_id,
count(*)
from order_detail
group by product_id;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/4.png" alt="启用map-site调优前后Explain执行计划对比">  </p>
<h4 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h4><p>开启map-side聚合，配置以下参数：  </p>
<pre><code>set hive.map.aggr=true;  

set hive.map.aggr.hash.min.reduction=0.5;   

set hive.groupby.mapaggr.checkinterval=100000;  

set hive.map.aggr.hash.force.flush.memory.threshold=0.9;  
</code></pre>
<h3 id="HQL语法优化之Join优化"><a href="#HQL语法优化之Join优化" class="headerlink" title="HQL语法优化之Join优化"></a>HQL语法优化之Join优化</h3><p>Join算法概述</p>
<p>Common Join : 常规join，不做优化    </p>
<p><img src="/2023/07/31/Hive-on-mr/5.png" alt="Common Join原理图">  </p>
<p>Map Join：<strong>适用于大表join小表</strong>，将小表数据缓存为hash table（内存表），然后扫描大表数据，这样在map端即可完成关联操作  </p>
<p><img src="/2023/07/31/Hive-on-mr/6.png" alt="Map Join原理图"></p>
<p>Bucket Map Join：<strong>适用于大表join大表</strong>  通过分桶对数据进行切分，让有限的内存缓存一部分分桶数据，再对另一个大表进行遍历操作     </p>
<pre><code> Bucket Map Join的使用要求：若能保证参与**join的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍**
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/7.png" alt="Bucket Map Join原理图"></p>
<p>SMB Map Join：<strong>适用于大表join大表</strong>，两个分桶之间的join实现原理为Sort Merge Join算法。前提条件是两个大表分桶数据都要排好序，这样就无需缓存在内存中，通过Sort Merge Join算法直接完成逐条遍历计算。  </p>
<pre><code>SMB Map Join的使用要求:参与join的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍    
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/8.png" alt="SMB Map Join原理图"></p>
<p>Bucket Map Join 和 SMB Map Join 的区别：  </p>
<p>1.SMB Map Join在Bucket Map join的基础上，要求分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段</p>
<p>2.两个分桶之间的join实现算法不一样  </p>
<pre><code>Bucket Map Join，两个分桶之间的join实现原理为Hash Join算法    

SMB Map Join，两个分桶之间的join实现原理为Sort Merge Join算法   
</code></pre>
<p>3.相较于Bucket Map Join，SMB Map Join对分桶大小是没有要求的,因为SMB Map Join不需要缓存数据  </p>
<h4 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h4><p>优化说明 </p>
<p><img src="/2023/07/31/Hive-on-mr/9.png" alt="Map join原理解析">   </p>
<p>寻找大表候选人：  </p>
<p>a inner join b时，a,b都可以作为大表候选人，只返回a,b都能连接上的数据  </p>
<p>A Left join  b时，只有b才可以作为大表候选人，这样才会遍历A表数据，输出A的所有数据  </p>
<p>A right join b时，只有A才可以作为大表候选人，这样才会遍历B表的数据，输出B的所有数据  </p>
<p>A full join b时，没有大表候选人，因为无论选a还是B作为大表候选人，都无法输出a和b的所有数据  </p>
<p>Conditionaltask：条件任务  </p>
<p>图中涉及到的参数如下：    </p>
<p>–启动Map Join自动转换  </p>
<pre><code>set hive.auto.convert.join=true;  
</code></pre>
<p>–一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的已知大小总和&lt;&#x3D;该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。 </p>
<pre><code>set hive.mapjoin.smalltable.filesize=250000;    
</code></pre>
<p>注意此处的内存大小参数与实际读取磁盘文件的大小是有差别的，考虑到磁盘文件解压缩，反序列化和对象信息，相同文件在内存中要比在磁盘中占用的空间放大大概10倍  </p>
<p>所以该参数设置为1G，就表明拿取磁盘中1G大小的文件，但内存需要占用10G  </p>
<p>实际生产中，将参数配置到hive-site等配置文件中。设置size参数时，通常配置为map端内存的1&#x2F;2 ~2&#x2F;3范围内作为缓存，记得size的值应该是map_memory*2&#x2F;3 的十分之一大小才行，否则磁盘文件读取到内存，会oom  </p>
<p>生产中，配置文件中的调优参数生效后，大部分sql语句性能提高了，如果极少部分任务还是慢sql，就需要单独调优，在语句中加入set参数的方式进行针对性局部调优  </p>
<p>–开启无条件转Map Join</p>
<pre><code>set hive.auto.convert.join.noconditionaltask=true;  
</code></pre>
<p>–无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和&lt;&#x3D;该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask.size=10000000;  
</code></pre>
<h5 id="Map-Join优化案例"><a href="#Map-Join优化案例" class="headerlink" title="Map Join优化案例"></a>Map Join优化案例</h5><pre><code>select  *
from order_detail od
join product_info product on od.product_id = product.id
join province_info province on od.province_id = province.id;  
</code></pre>
<p>优化前：设置 set hive.auto.convert.join&#x3D;true &#x3D; false  </p>
<p><img src="/2023/07/31/Hive-on-mr/10.png" alt="Map Join优化前">    </p>
<p>对参与关联的三张表进行分析，发现各自大小如下   </p>
<p><img src="/2023/07/31/Hive-on-mr/11.png" alt="参与关联的三张表大小"></p>
<h6 id="Map-Join方案一："><a href="#Map-Join方案一：" class="headerlink" title="Map Join方案一："></a>Map Join方案一：</h6><p>启用Map Join自动转换 </p>
<pre><code>set hive.auto.convert.join=true;  
</code></pre>
<p>不使用无条件转Map Join  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask=false;  
</code></pre>
<p>调整hive.mapjoin.smalltable.filesize参数，使其大于等于product_info  </p>
<pre><code>set hive.mapjoin.smalltable.filesize=25285707;  
</code></pre>
<p>这样可保证将两个Common Join operator均可转为Map Join operator，并保留Common Join作为后备计划，保证计算任务的稳定  </p>
<p><img src="/2023/07/31/Hive-on-mr/12.png" alt="Map Jion优化方案一">  </p>
<h6 id="Map-Join方案二："><a href="#Map-Join方案二：" class="headerlink" title="Map Join方案二："></a>Map Join方案二：</h6><p>启用Map Join自动转换  </p>
<pre><code>set hive.auto.convert.join=true;  
</code></pre>
<p>使用无条件转Map Join  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask=true;  
</code></pre>
<p>调整hive.auto.convert.join.noconditionaltask.size参数，使其大于等于product_info和province_info之和  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask.size=25286076;  
</code></pre>
<p>这样可直接将两个Common Join operator转为两个Map Join operator，并且由于两个Map Join operator的小表大小之和小于等于hive.auto.convert.join.noconditionaltask.size，故两个Map Join operator任务可合并为同一个。这个方案计算效率最高，但需要的内存也是最多的</p>
<p><img src="/2023/07/31/Hive-on-mr/13.png" alt="Map Jion优化方案二"> </p>
<h6 id="Map-Join方案三："><a href="#Map-Join方案三：" class="headerlink" title="Map Join方案三："></a>Map Join方案三：</h6><p>启用Map Join自动转换    </p>
<pre><code>set hive.auto.convert.join=true;   
</code></pre>
<p>使用无条件转Map Join  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask=true;    
</code></pre>
<p>调整hive.auto.convert.join.noconditionaltask.size参数，使其等于product_info</p>
<pre><code>set hive.auto.convert.join.noconditionaltask.size=25285707;  
</code></pre>
<p>这样可直接将两个Common Join operator转为Map Join operator，但不会将两个Map Join的任务合并。该方案计算效率比方案二低，但需要的内存也更少</p>
<p><img src="/2023/07/31/Hive-on-mr/14.png" alt="Map Join优化方案三"></p>
<h4 id="Bucket-Map-Join"><a href="#Bucket-Map-Join" class="headerlink" title="Bucket Map Join"></a>Bucket Map Join</h4><p>Bucket Map Join不支持自动转换，发须通过用户在SQL语句中提供如下Hint提示，并配置如下相关参数，方可使用  </p>
<pre><code>select /*+ mapjoin(ta) */
ta.id,
tb.id
from table_a ta
join table_b tb on ta.id=tb.id; 
</code></pre>
<p>相关参数  </p>
<p>–关闭cbo优化，cbo会导致hint信息被忽略  </p>
<pre><code>set hive.cbo.enable=false;
</code></pre>
<p>–map join hint默认会被忽略(因为已经过时)，需将如下参数设置为false  </p>
<pre><code>set hive.ignore.mapjoin.hint=false;  
</code></pre>
<p>–启用bucket map join优化功能  </p>
<pre><code>set hive.optimize.bucketmapjoin = true;    
</code></pre>
<h5 id="Bucket-Map-Join优化案例"><a href="#Bucket-Map-Join优化案例" class="headerlink" title="Bucket Map Join优化案例"></a>Bucket Map Join优化案例</h5><pre><code>select
    *
from(
    select
        *
    from order_detail
    where dt=&#39;2020-06-14&#39;
)od
join(
    select
        	*
    from payment_detail
    where dt=&#39;2020-06-14&#39;
)pd
on od.id=pd.order_detail_id;  
</code></pre>
<h6 id="Bucket-Map-Join优化前"><a href="#Bucket-Map-Join优化前" class="headerlink" title="Bucket Map Join优化前"></a>Bucket Map Join优化前</h6><pre><code>set hive.auto.convert.join=false;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/15.png" alt="Bucket Map Join优化前"></p>
<h6 id="Bucket-Map-Join优化思路"><a href="#Bucket-Map-Join优化思路" class="headerlink" title="Bucket Map Join优化思路"></a>Bucket Map Join优化思路</h6><p>经分析，参与join的两张表，数据量如下  </p>
<p>order_detail	  1176009934（约1122M）<br>payment_detail	  334198480（约319M）  </p>
<p>可以认为是大表join大表，可尝试采用Bucket Map Join优化方案  </p>
<p>首先需要依据源表创建两个分桶表，order_detail建议分16个bucket  </p>
<p>payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段  </p>
<p>–订单表 </p>
<pre><code>hive (default)&gt; 
drop table if exists order_detail_bucketed;
create table order_detail_bucketed(
id           string comment &#39;订单id&#39;,
user_id      string comment &#39;用户id&#39;,
product_id   string comment &#39;商品id&#39;,
province_id  string comment &#39;省份id&#39;,
create_time  string comment &#39;下单时间&#39;,
product_num  int comment &#39;商品件数&#39;,
total_amount decimal(16, 2) comment &#39;下单金额&#39;
)
clustered by (id) into 16 buckets
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<p>–支付表  </p>
<pre><code>hive (default)&gt; 
drop table if exists payment_detail_bucketed;
create table payment_detail_bucketed(
id              string comment &#39;支付id&#39;,
order_detail_id string comment &#39;订单明细id&#39;,
user_id         string comment &#39;用户id&#39;,
payment_time    string comment &#39;支付时间&#39;,
total_amount    decimal(16, 2) comment &#39;支付金额&#39;
)
clustered by (order_detail_id) into 8 buckets
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<p>然后向两个分桶表导入数据。  </p>
<p>–订单表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table order_detail_bucketed
select
id,
user_id,
product_id,
province_id,
create_time,
product_num,
total_amount   
from order_detail
where dt=&#39;2023-07-28&#39;;
</code></pre>
<p>–分桶表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table payment_detail_bucketed
select
id,
order_detail_id,
user_id,
payment_time,
total_amount
from payment_detail
where dt=&#39;2020-07-28&#39;;
</code></pre>
<p>然后设置以下参数：  </p>
<p>–关闭cbo优化，cbo会导致hint信息被忽略，需将如下参数修改为false  </p>
<pre><code>set hive.cbo.enable=false;  
</code></pre>
<p>–map join hint默认会被忽略(因为已经过时)，需将如下参数修改为false  </p>
<pre><code>set hive.ignore.mapjoin.hint=false;  
</code></pre>
<p>–启用bucket map join优化功能,默认不启用，需将如下参数修改为true </p>
<pre><code>set hive.optimize.bucketmapjoin = true;
</code></pre>
<p>最后在重写SQL语句，如下：  </p>
<pre><code>select /*+ mapjoin(pd) */
    *
from order_detail_bucketed od
join payment_detail_bucketed pd on od.id = pd.order_detail_id; 
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/16.png">  </p>
<p>由于bucket map join和map join的执行计划非常像，如何确定该执行计划是否属于bucket map join ? </p>
<p><img src="/2023/07/31/Hive-on-mr/17.png"></p>
<h4 id="Sort-Merge-Bucket-Map-Join"><a href="#Sort-Merge-Bucket-Map-Join" class="headerlink" title="Sort Merge Bucket Map Join"></a>Sort Merge Bucket Map Join</h4><p>优化说明  </p>
<p>Sort Merge Bucket Map Join有两种触发方式，包括Hint提示和自动转换。Hint提示已过时，不推荐使用。下面是自动转换的相关参数:  </p>
<p>–启动Sort Merge Bucket Map Join优化  </p>
<pre><code>set hive.optimize.bucketmapjoin.sortedmerge=true;  
</code></pre>
<p>–使用自动转换SMB Join  </p>
<pre><code>set hive.auto.convert.sortmerge.join=true;   
</code></pre>
<h5 id="Sort-Merge-Bucket-Map-Join优化案例"><a href="#Sort-Merge-Bucket-Map-Join优化案例" class="headerlink" title="Sort Merge Bucket Map Join优化案例"></a>Sort Merge Bucket Map Join优化案例</h5><pre><code>select
        *
from(
    select
            *
    from order_detail
    where dt=&#39;2020-06-14&#39;
)od
join(
    select
        *
    from payment_detail
    where dt=&#39;2020-06-14&#39;
)pd
on od.id=pd.order_detail_id;  
</code></pre>
<h5 id="Sort-Merge-Bucket-Map-Join优化思路"><a href="#Sort-Merge-Bucket-Map-Join优化思路" class="headerlink" title="Sort Merge Bucket Map Join优化思路"></a>Sort Merge Bucket Map Join优化思路</h5><p>order_detail	1176009934（约1122M）<br>payment_detail	334198480（约319M）</p>
<p>两张表都相对较大，除了可以考虑采用Bucket Map Join算法，还可以考虑SMB Join。相较于Bucket Map Join，SMB Map Join对分桶大小是没有要求的    </p>
<p>首先需要依据源表创建两个的有序的分桶表，order_detail建议分16个bucket，payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段和排序字段</p>
<p>–订单表  </p>
<pre><code>hive (default)&gt; 
drop table if exists order_detail_sorted_bucketed;
create table order_detail_sorted_bucketed(
id           string comment &#39;订单id&#39;,
user_id      string comment &#39;用户id&#39;,
product_id   string comment &#39;商品id&#39;,
province_id  string comment &#39;省份id&#39;,
create_time  string comment &#39;下单时间&#39;,
product_num  int comment &#39;商品件数&#39;,
total_amount decimal(16, 2) comment &#39;下单金额&#39;
)	
clustered by (id) sorted by(id) into 16 buckets
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<p>–支付表  </p>
<pre><code>hive (default)&gt; 
drop table if exists payment_detail_sorted_bucketed;
create table payment_detail_sorted_bucketed(
id              string comment &#39;支付id&#39;,
order_detail_id string comment &#39;订单明细id&#39;,
user_id         string comment &#39;用户id&#39;,
payment_time    string comment &#39;支付时间&#39;,
total_amount    decimal(16, 2) comment &#39;支付金额&#39;
)
clustered by (order_detail_id) sorted by(order_detail_id) into 8 buckets
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<p>然后向两个分桶表导入数据。  </p>
<p>–订单表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table order_detail_sorted_bucketed
select
id,
user_id,
product_id,
province_id,
create_time,
product_num,
total_amount   
from order_detail
where dt=&#39;2023-07-28&#39;;
</code></pre>
<p>–分桶表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table payment_detail_sorted_bucketed
select
id,
order_detail_id,
user_id,
payment_time,
total_amount
from payment_detail
where dt=&#39;2023-07-28&#39;;  
</code></pre>
<p>–启动Sort Merge Bucket Map Join优化  </p>
<pre><code>set hive.optimize.bucketmapjoin.sortedmerge=true;  
</code></pre>
<p>–使用自动转换SMB Join  </p>
<pre><code>set hive.auto.convert.sortmerge.join=true; 
</code></pre>
<p>最后在重写SQL语句，如下：  </p>
<pre><code>hive (default)&gt; 
select
    *
from order_detail_sorted_bucketed od
join payment_detail_sorted_bucketed pd
on od.id = pd.order_detail_id;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/18.png">  </p>
<h3 id="HQL语法优化之数据倾斜"><a href="#HQL语法优化之数据倾斜" class="headerlink" title="HQL语法优化之数据倾斜"></a>HQL语法优化之数据倾斜</h3><p>数据倾斜概述</p>
<p>数据倾斜问题，通常是指参与计算的数据分布不均，即某个key或者某些key的数据量远超其他key，导致在shuffle阶段，大量相同key的数据被发往同一个Reduce，进而导致该Reduce所需的时间远超其他Reduce，成为整个任务的瓶颈</p>
<p>Hive中的数据倾斜常出现在<strong>分组聚合</strong>和<strong>join操作</strong>的场景中  </p>
<h4 id="分组聚合导致的数据倾斜-（Map-Side聚合-Skew-GroupBy优化）"><a href="#分组聚合导致的数据倾斜-（Map-Side聚合-Skew-GroupBy优化）" class="headerlink" title="分组聚合导致的数据倾斜 （Map-Side聚合&#x2F;Skew-GroupBy优化）"></a>分组聚合导致的数据倾斜 （Map-Side聚合&#x2F;Skew-GroupBy优化）</h4><p>如果group by分组字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题   </p>
<p>由分组聚合导致的数据倾斜问题，有以下两种解决思路    </p>
<h5 id="Map-Side聚合"><a href="#Map-Side聚合" class="headerlink" title="Map-Side聚合"></a>Map-Side聚合</h5><p>开启Map-Side聚合后，数据会现在Map端完成部分聚合工作。这样一来即便原始数据是倾斜的，经过Map端的初步聚合后，发往Reduce的数据也就不再倾斜了，最佳状态下，Map-端聚合能完全屏蔽数据倾斜问题。  </p>
<p>相关参数如下：  </p>
<pre><code>set hive.map.aggr=true;

set hive.map.aggr.hash.min.reduction=0.5;  

set hive.groupby.mapaggr.checkinterval=100000;  

set hive.map.aggr.hash.force.flush.memory.threshold=0.9;   
</code></pre>
<h5 id="Skew-GroupBy优化"><a href="#Skew-GroupBy优化" class="headerlink" title="Skew-GroupBy优化"></a>Skew-GroupBy优化</h5><p>Skew-GroupBy的原理是启动两个MR任务，第一个MR按照随机数分区，将数据分散发送到Reduce，完成部分聚合，第二个MR按照分组字段分区，完成最终聚合  </p>
<p>–启用分组聚合数据倾斜优化  </p>
<pre><code>set hive.groupby.skewindata=true;  
</code></pre>
<p>–关闭map-side聚合  </p>
<pre><code>set hive.map.aggr=false;    
</code></pre>
<h4 id="Join导致的数据倾斜"><a href="#Join导致的数据倾斜" class="headerlink" title="Join导致的数据倾斜"></a>Join导致的数据倾斜</h4><p>如果关联字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题。  </p>
<p>由join导致的数据倾斜问题，有如下三种解决方案：<br>map join<br>skew join<br>调整sql，通过sql语句将倾斜数据打散成更小的块  </p>
<h5 id="map-join（适用于大表join小表时发生数据倾斜的场景）"><a href="#map-join（适用于大表join小表时发生数据倾斜的场景）" class="headerlink" title="map join（适用于大表join小表时发生数据倾斜的场景）"></a>map join（<strong>适用于大表join小表时发生数据倾斜的场景</strong>）</h5><p> 使用map join算法，join操作仅在map端就能完成，没有shuffle操作，没有reduce阶段，自然不会产生reduce端的数据倾斜     </p>
<p>相关参数参照上文中map join部分内容  </p>
<pre><code>set hive.auto.convert.join=true;  
set hive.mapjoin.smalltable.filesize=250000;  
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000000;
</code></pre>
<h5 id="skew-join（对两表中倾斜的key的数据量有要求）"><a href="#skew-join（对两表中倾斜的key的数据量有要求）" class="headerlink" title="skew join（对两表中倾斜的key的数据量有要求）"></a>skew join（<strong>对两表中倾斜的key的数据量有要求</strong>）</h5><p>skew join的原理是，为倾斜的大key单独启动一个map join任务进行计算，其余key进行正常的common join  </p>
<p><img src="/2023/07/31/Hive-on-mr/19.png" alt="Skew Join原理图">  </p>
<p>相关参数如下：  </p>
<p>–启用skew join优化  </p>
<pre><code>set hive.optimize.skewjoin=true;  
</code></pre>
<p>–触发skew join的阈值，若某个key的行数超过该参数值，则触发  </p>
<pre><code>set hive.skewjoin.key=100000;    
</code></pre>
<p>对两表中倾斜的key的数据量有要求，要求一张表中的倾斜key的数据量比较小（方便走mapjoin）  </p>
<h5 id="SQL打散"><a href="#SQL打散" class="headerlink" title="SQL打散"></a>SQL打散</h5><pre><code>select
    *
from(
    select --打散操作
    concat(id,&#39;_&#39;,cast(rand()*2 as int)) id,
    value
from A
)ta
join(
    select --扩容操作
        concat(id,&#39;_&#39;,0) id,
        value
    from B
    union all
    select
        concat(id,&#39;_&#39;,1) id,
           value
    from B
)tb
on ta.id=tb.id;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/20.png" alt="SQL打散"></p>
<h4 id="HQL语法优化之任务并行度"><a href="#HQL语法优化之任务并行度" class="headerlink" title="HQL语法优化之任务并行度"></a>HQL语法优化之任务并行度</h4><p>对于一个分布式的计算任务而言，设置一个合适的并行度十分重要。Hive的计算任务由MapReduce完成，故并行度的调整需要分为Map端和Reduce端</p>
<h5 id="Map端并行度"><a href="#Map端并行度" class="headerlink" title="Map端并行度"></a>Map端并行度</h5><p>Map端的并行度，也就是Map的个数。是由输入文件的切片数决定的。一般情况下，Map端的并行度无需手动调整  </p>
<p>以下特殊情况可考虑调整map端并行度：  </p>
<p>1）查询的表中存在大量小文件    </p>
<pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  
</code></pre>
<p>2）map端有复杂的查询逻辑  </p>
<p>在计算资源充足的情况下，可考虑增大map端的并行度，令map task多一些，每个map task计算的数据少一些  </p>
<p>–一个切片的最大值  </p>
<pre><code>set mapreduce.input.fileinputformat.split.maxsize=256000000;  
</code></pre>
<h5 id="Reduce端并行度"><a href="#Reduce端并行度" class="headerlink" title="Reduce端并行度"></a>Reduce端并行度</h5><p>Reduce端的并行度，也就是Reduce个数。相对来说，更需要关注。Reduce端的并行度，可由用户自己指定，也可由Hive自行根据该MR Job输入的文件大小进行估算    </p>
<p>Reduce端的并行度的相关参数如下：  </p>
<p>–指定Reduce端并行度，默认值为-1，表示用户未指定  </p>
<pre><code>set mapreduce.job.reduces;  
</code></pre>
<p>–Reduce端并行度最大值  </p>
<pre><code>set hive.exec.reducers.max;  
</code></pre>
<p>–单个Reduce Task计算的数据量，用于估算Reduce并行度  </p>
<pre><code>set hive.exec.reducers.bytes.per.reducer;
</code></pre>
<h6 id="估算逻辑"><a href="#估算逻辑" class="headerlink" title="估算逻辑"></a>估算逻辑</h6><p>假设Job输入的文件大小为totalInputBytes  </p>
<p>参数hive.exec.reducers.bytes.per.reducer的值为bytesPerReducer。  </p>
<p>参数hive.exec.reducers.max的值为maxReducers。  </p>
<p>则Reduce端的并行度为：  </p>
<pre><code>min(ceil(totalInputBytes/bytesPerReducer),maxReducers)  
</code></pre>
<h4 id="HQL语法优化之小文件合并"><a href="#HQL语法优化之小文件合并" class="headerlink" title="HQL语法优化之小文件合并"></a>HQL语法优化之小文件合并</h4><p>Map端输入文件合并   </p>
<p>合并Map端输入的小文件，是指将多个小文件划分到一个切片中，进而由一个Map Task去处理。目的是防止为单个小文件启动一个Map Task，浪费计算资源  </p>
<p>–可将多个小文件切片，合并为一个切片，进而由一个map任务处理  </p>
<pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  
</code></pre>
<p>Reduce输出文件合并  </p>
<p>合并Reduce端输出的小文件，是指将多个小文件合并成大文件。目的是减少HDFS小文件数量。其原理是根据计算任务输出文件的平均大小进行判断，若符合条件，则单独启动一个额外的任务进行合并  </p>
<p>–开启合并map only任务输出的小文件  </p>
<pre><code>set hive.merge.mapfiles=true;
</code></pre>
<p>–开启合并map reduce任务输出的小文件  </p>
<pre><code>set hive.merge.mapredfiles=true;
</code></pre>
<p>–合并后的文件大小</p>
<pre><code>set hive.merge.size.per.task=256000000;
</code></pre>
<p>–触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并 </p>
<pre><code>set hive.merge.smallfiles.avgsize=16000000;  
</code></pre>
<h3 id="其他优化"><a href="#其他优化" class="headerlink" title="其他优化"></a>其他优化</h3><h4 id="1-CBO优化"><a href="#1-CBO优化" class="headerlink" title="1.CBO优化"></a>1.CBO优化</h4><p>CBO是指Cost based Optimizer，即基于计算成本的优化</p>
<p>在Hive中，计算成本模型考虑到了：数据的行数、CPU、本地IO、HDFS IO、网络IO等方面  </p>
<p>目前CBO在hive的MR引擎下主要用于join的优化，例如多表join的join顺序  </p>
<p>–是否启用cbo优化   </p>
<pre><code>set hive.cbo.enable=true;    
</code></pre>
<h4 id="2-谓词下推"><a href="#2-谓词下推" class="headerlink" title="2.谓词下推"></a>2.谓词下推</h4><p>谓词下推（predicate pushdown）是指，尽量将过滤操作前移，以减少后续计算步骤的数据量</p>
<p>–是否启动谓词下推（predicate pushdown）优化  </p>
<pre><code>set hive.optimize.ppd = true;  
</code></pre>
<p>CBO优化也会完成一部分的谓词下推优化工作，因为在执行计划中，谓词越靠前，整个计划的计算成本就会越低</p>
<h4 id="3-矢量化查询"><a href="#3-矢量化查询" class="headerlink" title="3.矢量化查询"></a>3.矢量化查询</h4><p>Hive的矢量化查询优化，依赖于CPU的矢量化计算，CPU的矢量化计算的基本原理如下图  </p>
<p><img src="/2023/07/31/Hive-on-mr/21.png" alt="矢量化计算原理">  </p>
<pre><code>set hive.vectorized.execution.enabled=true;  
</code></pre>
<p>若执行计划中，出现“Execution mode: vectorized”字样，即表明使用了矢量化计算。  </p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution#VectorizedQueryExecution-Limitations">矢量化计算官方文档</a> </p>
<h4 id="4-Fetch抓取"><a href="#4-Fetch抓取" class="headerlink" title="4.Fetch抓取"></a>4.Fetch抓取</h4><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算  </p>
<p>Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果到控制台</p>
<p>–是否在特定场景转换为fetch 任务  </p>
<p>–设置为none表示不转换  </p>
<p>–设置为minimal表示支持select *，分区字段过滤，Limit等  </p>
<p>–设置为more表示支持select 任意字段,包括函数，过滤，和limit等  </p>
<pre><code>set hive.fetch.task.conversion=more;  
</code></pre>
<h4 id="5-本地模式（不上yarn）"><a href="#5-本地模式（不上yarn）" class="headerlink" title="5.本地模式（不上yarn）"></a>5.本地模式（不上yarn）</h4><p>Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短</p>
<p>–开启自动转换为本地模式  </p>
<pre><code>set hive.exec.mode.local.auto=true;  
</code></pre>
<p>–设置local MapReduce的最大输入数据量，当输入数据量小于这个值时采用local  MapReduce的方式，默认为134217728，即128M  </p>
<pre><code>set hive.exec.mode.local.auto.inputbytes.max=50000000;
</code></pre>
<p>–设置local MapReduce的最大输入文件个数，当输入文件个数小于这个值时采用local MapReduce的方式，默认为4  </p>
<pre><code>set hive.exec.mode.local.auto.input.files.max=10;
</code></pre>
<h4 id="6-并行执行"><a href="#6-并行执行" class="headerlink" title="6.并行执行"></a>6.并行执行</h4><p>Hive会将一个SQL语句转化成一个或者多个Stage，每个Stage对应一个MR Job。默认情况下，Hive同时只会执行一个Stage。但是某SQL语句可能会包含多个Stage，但这多个Stage可能并非完全互相依赖，也就是说有些Stage是可以并行执行的。此处提到的并行执行就是指这些Stage的并行执行   </p>
<p>–启用并行执行优化  </p>
<pre><code>set hive.exec.parallel=true;       
</code></pre>
<p>–同一个sql允许最大并行度，默认为8  </p>
<pre><code>set hive.exec.parallel.thread.number=8;   
</code></pre>
<h4 id="7-严格模式"><a href="#7-严格模式" class="headerlink" title="7.严格模式"></a>7.严格模式</h4><p>Hive可以通过设置某些参数防止危险操作：   </p>
<p>1）分区表不使用分区过滤  </p>
<p>将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行  </p>
<p>2）使用order by没有limit过滤   </p>
<p>将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句    </p>
<p>3）笛卡尔积    </p>
<p>将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=2042878838&auto=1&height=66"></iframe>

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张宴银"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">张宴银</p>
  <div class="site-description" itemprop="description">初级以内我无敌，中级以上我一换一</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; Sat Jul 29 2023 08:00:00 GMT+0800 (中国标准时间) – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张宴银</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>



    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共47.2k字</span>
</div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
