<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="初级以内我无敌，中级以上我一换一">
<meta property="og:type" content="website">
<meta property="og:title" content="第五门徒">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="第五门徒">
<meta property="og:description" content="初级以内我无敌，中级以上我一换一">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张宴银">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>第五门徒</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="第五门徒" type="application/rss+xml">
</head>




<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">第五门徒</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="heartbeat fa-fw"></i>公益 404</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/02/Spark-Core/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/02/Spark-Core/" class="post-title-link" itemprop="url">Spark学习笔记</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-02 10:51:06" itemprop="dateCreated datePublished" datetime="2023-08-02T10:51:06+08:00">2023-08-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 12:49:16" itemprop="dateModified" datetime="2023-08-11T12:49:16+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="纵浪大化中，不喜亦不惧"><a href="#纵浪大化中，不喜亦不惧" class="headerlink" title="纵浪大化中，不喜亦不惧 ~"></a>纵浪大化中，不喜亦不惧 ~</h1><p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g">https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g</a><br>提取码：mg5w   </p>
<h1 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h1><h2 id="Spark-是什么？"><a href="#Spark-是什么？" class="headerlink" title="Spark 是什么？"></a>Spark 是什么？</h2><p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎     </p>
<p>Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎  </p>
<p>Spark Core 中提供了 Spark 最基础与最核心的功能  </p>
<p>Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据   </p>
<p>Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API  </p>
<p>Spark 一直被认为是 Hadoop 框架的升级版。  </p>
<h2 id="Mapreduce是什么？"><a href="#Mapreduce是什么？" class="headerlink" title="Mapreduce是什么？"></a>Mapreduce是什么？</h2><p>MapReduce 是一种编程模型，作为 Hadoop 的分布式计算模型，是 Hadoop 的核心    </p>
<h2 id="HBase是什么？"><a href="#HBase是什么？" class="headerlink" title="HBase是什么？"></a>HBase是什么？</h2><p>HBase 是一个基于 HDFS 的分布式数据库，擅长实时地随机读&#x2F;写超大规模数据集  </p>
<h2 id="Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）"><a href="#Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）" class="headerlink" title="Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）"></a>Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）</h2><p>Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘  </p>
<p>Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互  </p>
<p>Spark 的缓存机制比 HDFS 的缓存机制高效  </p>
<h2 id="什么时候选用Spark什么时候选用Mapreduce？"><a href="#什么时候选用Spark什么时候选用Mapreduce？" class="headerlink" title="什么时候选用Spark什么时候选用Mapreduce？"></a>什么时候选用Spark什么时候选用Mapreduce？</h2><p>Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark并不能完全替代 MR。  </p>
<h2 id="提交Spark应用的代码示例"><a href="#提交Spark应用的代码示例" class="headerlink" title="提交Spark应用的代码示例"></a>提交Spark应用的代码示例</h2><h3 id="local模式"><a href="#local模式" class="headerlink" title="local模式"></a>local模式</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ./examples/jars/spark-examples_2.12-3.0.0.jar 10

1) --class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序  
2) --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟 CPU 核数量  
3) spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包  
4) 数字 10 表示程序的入口参数，用于设定当前应用的任务数量  
</code></pre>
<h3 id="Yarn模式-（生产：Cluster模式）"><a href="#Yarn模式-（生产：Cluster模式）" class="headerlink" title="Yarn模式  （生产：Cluster模式）"></a>Yarn模式  （生产：Cluster模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2.12-3.0.0.jar 10  
</code></pre>
<h3 id="Yarn模式-（测试：Client模式）"><a href="#Yarn模式-（测试：Client模式）" class="headerlink" title="Yarn模式  （测试：Client模式）"></a>Yarn模式  （测试：Client模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.12-3.0.0.jar 10  
</code></pre>
<h4 id="Spark的端口号"><a href="#Spark的端口号" class="headerlink" title="Spark的端口号"></a>Spark的端口号</h4><pre><code>➢ Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）  
➢ Spark Master 内部通信服务端口号：7077  
➢ Standalone 模式下，Spark Master Web 端口号：8080（资源）  
➢ Spark 历史服务器端口号：18080  
➢ Hadoop YARN 任务运行情况查看端口号：8088     
</code></pre>
<h2 id="Spark运行架构"><a href="#Spark运行架构" class="headerlink" title="Spark运行架构"></a>Spark运行架构</h2><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构  </p>
<p><img src="/2023/08/02/Spark-Core/1.png" alt="Spark运行架构">  </p>
<p>Driver表示Master，负责管理整个集群中的作业调度  </p>
<p>Executor表示slave，负责实际执行任务  </p>
<h3 id="Spark核心组件"><a href="#Spark核心组件" class="headerlink" title="Spark核心组件"></a>Spark核心组件</h3><p>Driver和Executor &amp; Master 和 Worker </p>
<h4 id="Driver的作用"><a href="#Driver的作用" class="headerlink" title="Driver的作用"></a>Driver的作用</h4><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。  </p>
<p>Driver 在 Spark 作业执行时主要负责：  </p>
<pre><code>➢ 将用户程序转化为作业（job）  
➢ 在 Executor 之间调度任务(task)  
➢ 跟踪 Executor 的执行情况  
➢ 通过 UI 展示查询运行情况      
</code></pre>
<p>所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。  </p>
<h4 id="Executor的作用"><a href="#Executor的作用" class="headerlink" title="Executor的作用"></a>Executor的作用</h4><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立  </p>
<p>Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。  </p>
<h5 id="Executor-有两个核心功能："><a href="#Executor-有两个核心功能：" class="headerlink" title="Executor 有两个核心功能："></a>Executor 有两个核心功能：</h5><pre><code>➢ 负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程
➢ 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。
  
</code></pre>
<h3 id="Master-和-Worker-Local模式时"><a href="#Master-和-Worker-Local模式时" class="headerlink" title="Master 和 Worker  (Local模式时)"></a>Master 和 Worker  (Local模式时)</h3><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker    </p>
<p>在Yarn模式时，Master 就是 RM ,Worker 就是 NM</p>
<h4 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h4><p>这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM  </p>
<h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>Worker也是进程，一个 Worker运行在集群中的一台服务器上，由 Master分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NM。    </p>
<h3 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h3><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。<br>说的简单点就是，ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是ApplicationMaster。    </p>
<h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="Executor-与-Core"><a href="#Executor-与-Core" class="headerlink" title="Executor 与 Core"></a>Executor 与 Core</h2><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。  </p>
<p>应用程序相关启动参数如下：<br>–num-executors 配置 Executor 的数量<br>–executor-memory 配置每个 Executor 的内存大小<br>–executor-cores 配置每个 Executor 的虚拟 CPU core 数量    </p>
<h2 id="并行度（Parallelism）"><a href="#并行度（Parallelism）" class="headerlink" title="并行度（Parallelism）"></a>并行度（Parallelism）</h2><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，是并行，而不是并发。  </p>
<p>将整个集群并行执行任务的数量称之为并行度。</p>
<p>一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。   </p>
<h2 id="有向无环图（DAG）"><a href="#有向无环图（DAG）" class="headerlink" title="有向无环图（DAG）"></a>有向无环图（DAG）</h2><p><img src="/2023/08/02/Spark-Core/2.png" alt="有向无环图">  </p>
<p>这里所谓的有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。    </p>
<p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。    </p>
<h2 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h2><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。  </p>
<p><img src="/2023/08/02/Spark-Core/3.png" alt="基于Yarn的Spark任务提交流程">   </p>
<p>Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client和 Cluster。两种模式主要区别在于：<strong>Driver 程序的运行节点位置</strong>。  </p>
<h3 id="Yarn-Client-模式"><a href="#Yarn-Client-模式" class="headerlink" title="Yarn Client 模式"></a>Yarn Client 模式</h3><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。  </p>
<pre><code>➢ Driver 在任务提交的本地机器上运行
➢ Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster
➢ ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存
➢ ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程  
➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数
➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。  
</code></pre>
<h3 id="Yarn-Cluster-模式"><a href="#Yarn-Cluster-模式" class="headerlink" title="Yarn Cluster 模式"></a>Yarn Cluster 模式</h3><p>Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。  </p>
<pre><code>➢ 在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster
➢ 随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。
➢ Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程  
➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数
➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行    
</code></pre>
<h1 id="Spark-核心编程"><a href="#Spark-核心编程" class="headerlink" title="Spark 核心编程"></a>Spark 核心编程</h1><p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。<strong>三大数据结构</strong>分别是：  </p>
<pre><code>➢ RDD : 弹性分布式数据集
➢ 累加器：分布式共享只写变量
➢ 广播变量：分布式共享只读变量
</code></pre>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合  </p>
<pre><code>➢ 弹性
    存储的弹性：内存与磁盘的自动切换；
    容错的弹性：数据丢失可以自动恢复；
    计算的弹性：计算出错重试机制；
    分片的弹性：可根据需要重新分片。
➢ 分布式：数据存储在大数据集群不同节点上
➢ 数据集：RDD 封装了计算逻辑，并不保存数据
➢ 数据抽象：RDD 是一个抽象类，需要子类具体实现
➢ 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑
➢ 可分区、并行计算  
</code></pre>
<h2 id="执行原理"><a href="#执行原理" class="headerlink" title="执行原理"></a>执行原理</h2><p>从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。  </p>
<p>执行时，需要将计算资源和计算模型进行协调和整合。  </p>
<p>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。  </p>
<p>在 Yarn 环境中，RDD的工作原理:  </p>
<p>1）启动Yarn集群环境  </p>
<p><img src="/2023/08/02/Spark-Core/5.png" alt="启动Yarn集群环境">   </p>
<p>2）Spark通过申请资源创建调度节点和计算节点   </p>
<p><img src="/2023/08/02/Spark-Core/6.png" alt="创建调度节点和计算节点">   </p>
<p>3）Spark框架根据需求将计算逻辑根据分区划分成不同的任务  </p>
<p><img src="/2023/08/02/Spark-Core/7.png" alt="根据分区划分成不同的任务">    </p>
<p>4）调度节点将任务根据计算节点状态发送到对应的计算节点进行计算  </p>
<p><img src="/2023/08/02/Spark-Core/8.png" alt="将任务分发给对应的计算节点进行计算"> </p>
<p>RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算  </p>
<h1 id="RDD算子总结"><a href="#RDD算子总结" class="headerlink" title="RDD算子总结"></a>RDD算子总结</h1><h2 id="Value类型总结"><a href="#Value类型总结" class="headerlink" title="Value类型总结"></a>Value类型总结</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><pre><code>｜ def map[U: ClassTag](f: T =&gt; U): RDD[U]
｜ 将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。
｜ val dataRDD1: RDD[Int] = dataRDD.map(
｜  num =&gt;
｜        &#123;   num * 2 &#125;
｜ )
｜ val dataRDD2: RDD[String] = dataRDD1.map(
｜  num =&gt; &#123;
｜  &quot;&quot; + num
｜  &#125;
｜ )
｜ ​
val mapRDD: RDD[Int] = rdd.map(_*2)
对传入的数据，一个一个的进行转换，再返回给结果集
</code></pre>
<h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><pre><code>｜ def mapPartitions[U: ClassTag](
｜  f: Iterator[T] =&gt; Iterator[U],
｜  preservesPartitioning: Boolean = false): RDD[U]
｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。
｜ val dataRDD1: RDD[Int] = dataRDD.mapPartitions(
｜  datas =&gt; &#123;
｜  datas.filter(_==2)
｜  &#125;
｜ )
｜ 
｜ 
val mpRDD: RDD[Int] = rdd.mapPartitions(
    iter =&gt; &#123;
println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;)
        iter.map(_ * 2) &#125;)
一个分区一个分区的数据进行转换，再返回给结果集
</code></pre>
<h3 id="map-和-mapPartitions-的区别："><a href="#map-和-mapPartitions-的区别：" class="headerlink" title="map 和 mapPartitions 的区别："></a>map 和 mapPartitions 的区别：</h3><pre><code>数据处理角度：
    Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作

功能的角度：
    Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。
    MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据

性能的角度：
    Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高
    但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作
</code></pre>
<h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h3><pre><code>｜ def mapPartitionsWithIndex[U: ClassTag](
｜  f: (Int, Iterator[T]) =&gt; Iterator[U],
｜  preservesPartitioning: Boolean = false): RDD[U]
｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引
｜ val dataRDD1 = dataRDD.mapPartitionsWithIndex(
｜  (index, datas) =&gt; &#123;
｜  datas.map(index, _)
｜  &#125;
｜ )
｜ 
｜ 
mapPartitionsWithIndex在mapPartitions基础上加上了分区index
val mpiRDD = rdd.mapPartitionsWithIndex(
  (index,iter) =&gt; &#123;
// 1 ,     2 ,     3 ,    4    // (0,1)   (2,2)   (4,3)  (6,4)    iter.map(
      num =&gt; &#123; (index,num) &#125;  ）&#125; )
</code></pre>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>｜ def flatMap[U: ClassTag](f: T &#x3D;&gt; TraversableOnce[U]): RDD[U]<br>｜ 将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射<br>｜ val dataRDD1 &#x3D; dataRDD.flatMap( list &#x3D;&gt; list)<br>｜<br>    val flatRDD:RDD[String] &#x3D; rdd.flatMap( s &#x3D;&gt; { s.split(“ “)  })<br>        Hello<br>        Scala<br>        Hello<br>        Spark</p>
<pre><code>如果使用rdd.map( s =&gt; &#123; s.split(&quot; &quot;)  &#125;)，会发现打印的结果是
    [Ljava.lang.String;@f1a45f8
    [Ljava.lang.String;@5edf2821

所以切割等扁平映射操作，选用flatMap
</code></pre>
<h3 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h3><p>｜ def glom(): RDD[Array[T]]<br>｜ 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变<br>｜ val dataRDD1:RDD[Array[Int]] &#x3D; dataRDD.glom()<br>    将同一个分区的数据直接转换为相同类型的内存数组进行处理<br>    List[Int] &#x3D;&gt; Array[Int]</p>
<h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><pre><code>｜ def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]
｜ 将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。极限情况下，数据可能被分在同一个分区中
｜ ​
val groupRDD:RDD[(Int,Iterable[Int])] = rdd.groupBy(num % 2)
    (0,CompactBuffer(2, 4))
    (1,CompactBuffer(1, 3))
    会输出一个RDD[(K, Iterable[T])]
</code></pre>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><pre><code>｜ 将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。
val dataRDD1 = dataRDD.filter(_%2 == 0)
</code></pre>
<h3 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h3><p>｜ def sample(<br>｜ withReplacement: Boolean,<br>｜  fraction: Double,<br>｜  seed: Long &#x3D; Utils.random.nextLong): RDD[T]<br>｜ 根据指定的规则从数据集中抽取数据<br>｜<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4<br>    ｜ ),1)<br>｜ &#x2F;&#x2F; 抽取数据不放回（伯努利算法）<br>｜ &#x2F;&#x2F; 伯努利算法：又叫 0、1 分布。例如扔硬币，要么正面，要么反面。<br>｜ &#x2F;&#x2F; 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不<br>｜ 要<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD1 &#x3D; dataRDD.sample(false, 0.5)<br>｜ &#x2F;&#x2F; 抽取数据放回（泊松算法）<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，true：放回；false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：重复数据的几率，范围大于等于 0.表示每一个元素被期望抽取到的次数<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD2 &#x3D; dataRDD.sample(true, 2)<br>｜ ​<br>｜<br>    相同的seed种子，多次运行依旧是相同的抽样结果,修改withReplacement也不会发生变化，&#x2F;&#x2F; 修改fraction后结果会发生改变<br>    rdd.sample(true,0.5,101)</p>
<h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>｜ def distinct()(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>｜ def distinct(numPartitions: Int)(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>    ｜ 将数据集中重复的数据去重<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4,1,2<br>    ｜ ),1)<br>    ｜ val dataRDD1 &#x3D; dataRDD.distinct()<br>    ｜ val dataRDD2 &#x3D; dataRDD.distinct(2)<br>｜ ​<br>｜ </p>
<h3 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h3><p>｜ def coalesce(numPartitions: Int, shuffle: Boolean &#x3D; false,<br>｜  partitionCoalescer: Option[PartitionCoalescer] &#x3D; Option.empty)<br>｜  (implicit ord: Ordering[T] &#x3D; null)<br>｜  : RDD[T]<br>｜ 根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率<br>｜ 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少<br>｜ 分区的个数，减小任务调度成本<br>｜ ​<br>｜<br>    &#x2F;&#x2F; coalesce 方法默认情况下不会将分区的数据打乱重新组合<br>    &#x2F;&#x2F; 这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜<br>    &#x2F;&#x2F; 如果想要让数据倾斜，可以进行shuffle处理<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2,shuffle &#x3D; false)<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2) 默认不进行shuffle<br>    &#x2F;&#x2F; 进行shuffle处理后会出现数据倾斜val newRDD &#x3D; rdd.coalesce(2, true)</p>
<h3 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h3><pre><code>｜ def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
repartition 操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true
</code></pre>
<h3 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h3><p>｜ def sortBy[K](<br>｜  f: (T) &#x3D;&gt; K,<br>｜ ascending: Boolean &#x3D; true,<br>｜  numPartitions: Int &#x3D; this.partitions.length)<br>｜  (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]<br>｜ 该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，之后按照 f 函数处理<br>｜ 的结果进行排序，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一<br>｜ 致。中间存在 shuffle 的过程<br>｜ ​<br>｜ ​<br>    默认升序排序<br>    val dataRDD1 &#x3D; dataRDD.sortBy(num&#x3D;&gt;num, false, 4)<br>    val newRDD &#x3D; rdd.sortBy(t &#x3D;&gt; t._1.toInt, true)</p>
<h2 id="双-Value-类型总结"><a href="#双-Value-类型总结" class="headerlink" title="双 Value 类型总结"></a>双 Value 类型总结</h2><h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><pre><code>def intersection(other: RDD[T]): RDD[T]
val dataRDD = dataRDD1.intersection(dataRDD2)
求交集
</code></pre>
<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><pre><code>def union(other: RDD[T]): RDD[T]
val dataRDD = dataRDD1.union(dataRDD2)
求并集
</code></pre>
<h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h3><pre><code>def subtract(other: RDD[T]): RDD[T]
val dataRDD = dataRDD1.subtract(dataRDD2)
求差集
</code></pre>
<h3 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h3><pre><code>def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]
val dataRDD = dataRDD1.zip(dataRDD2)
将两个 RDD 中的元素，以键值对的形式进行合并。其中，键值对中的 Key 为第 1 个 RDD中的元素，Value 为第 2 个 RDD 中的相同位置的元素
</code></pre>
<h2 id="Key-Value类型总结"><a href="#Key-Value类型总结" class="headerlink" title="Key-Value类型总结"></a>Key-Value类型总结</h2><h3 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h3><pre><code>def partitionBy(partitioner: Partitioner): RDD[(K, V)]
将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner
val rdd2: RDD[(Int, String)] = rdd.partitionBy(new HashPartitioner(2))
</code></pre>
<h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3><pre><code>def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]
def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]
可以将数据按照相同的 Key 对 Value 进行聚合
val dataRDD2 = dataRDD1.reduceByKey(_+_)
</code></pre>
<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><pre><code>def groupByKey(): RDD[(K, Iterable[V])]
def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
将数据源的数据根据 key 对 value 进行分组
val dataRDD2 = dataRDD1.groupByKey()
val dataRDD3 = dataRDD1.groupByKey(2)
val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2))
</code></pre>
<h3 id="reduceByKey-和-groupByKey-的区别："><a href="#reduceByKey-和-groupByKey-的区别：" class="headerlink" title="reduceByKey 和 groupByKey 的区别："></a>reduceByKey 和 groupByKey 的区别：</h3><pre><code>从 shuffle 的角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的
数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。
从功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey
</code></pre>
<h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h3><pre><code>def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): RDD[(K, U)]
将数据根据不同的规则进行分区内计算和分区间计算
dataRDD1.aggregateByKey(0)(_+_,_+_)
｜ // 1. 第一个参数列表中的参数表示初始值
｜ // 2. 第二个参数列表中含有两个参数
｜ // 2.1 第一个参数表示分区内的计算规则
｜ // 2.2 第二个参数表示分区间的计算规则
｜ 
</code></pre>
<h3 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h3><pre><code>def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]
当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey
</code></pre>
<h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h3><pre><code>def combineByKey[C](
createCombiner: V =&gt; C,
 mergeValue: (C, V) =&gt; C,
 mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]
最通用的对 key-value 型 rdd 进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致
val combineRdd: RDD[(String, (Int, Int))] = input.combineByKey(
 (_, 1),
 (acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1),
 (acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2))
</code></pre>
<h3 id="reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别："><a href="#reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别：" class="headerlink" title="reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别："></a>reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别：</h3><pre><code>｜ reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同
｜ FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同
｜ AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同
｜ CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同
｜ 
</code></pre>
<h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3><pre><code>def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length): RDD[(K, V)]
val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(true)
val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(false)
</code></pre>
<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><pre><code>def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的(K,(V,W))的 RDD
rdd.join(rdd1).collect().foreach(println)
</code></pre>
<h3 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h3><pre><code>def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]
类似于 SQL 语句的左外连接
val rdd: RDD[(String, (Int, Option[Int]))] = dataRDD1.leftOuterJoin(dataRDD2)
</code></pre>
<h3 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h3><pre><code>类似于 SQL 语句的右外连接
</code></pre>
<h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><pre><code>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))类型的 RDD
val value: RDD[(String, (Iterable[Int], Iterable[Int]))] = dataRDD1.cogroup(dataRDD2)
(a,(CompactBuffer(1),CompactBuffer(4)))
(b,(CompactBuffer(2),CompactBuffer(5)))
(c,(CompactBuffer(3),CompactBuffer(6, 7)))
</code></pre>
<h2 id="Spark行动算子"><a href="#Spark行动算子" class="headerlink" title="Spark行动算子"></a>Spark行动算子</h2><h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>在驱动程序中，以数组 Array 的形式返回数据集的所有元素  </p>
<pre><code>// 收集数据到 Driver
rdd.collect().foreach(println)  
</code></pre>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>返回 RDD 中元素的个数  </p>
<pre><code>// 返回 RDD 中元素的个数
val countResult: Long = rdd.count()  
</code></pre>
<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>返回 RDD 中的第一个元素  </p>
<pre><code>// 返回 RDD 中元素的第1个元素
val firstResult: Int = rdd.first()  
</code></pre>
<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>返回一个由 RDD 的前 n 个元素组成的数组  </p>
<pre><code>// 返回 RDD 中元素的个数
val takeResult: Array[Int] = rdd.take(2)
println(takeResult.mkString(&quot;,&quot;))  
</code></pre>
<h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>返回该 RDD 排序后的前 n 个元素组成的数组  </p>
<pre><code>// 返回 RDD 中元素的个数
val result: Array[Int] = rdd.takeOrdered(2)  
</code></pre>
<h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合  </p>
<pre><code>val result: Int = rdd.aggregate(10)(_ + _, _ + _)  
</code></pre>
<h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p>折叠操作，aggregate 的简化版操作  </p>
<pre><code>val foldResult: Int = rdd.fold(0)(_+_)  
</code></pre>
<h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>统计每种 key 的个数  </p>
<pre><code>// 统计每种 key 的个数
val result: collection.Map[Int, Long] = rdd.countByKey()  
</code></pre>
<h3 id="save-相关算子"><a href="#save-相关算子" class="headerlink" title="save 相关算子"></a>save 相关算子</h3><p>将数据保存到不同格式的文件中  </p>
<pre><code>// 保存成 Text 文件
rdd.saveAsTextFile(&quot;output&quot;)  
</code></pre>
<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><pre><code>// 收集后打印
rdd.map(num=&gt;num).collect().foreach(println)  
</code></pre>
<h2 id="RDD-序列化"><a href="#RDD-序列化" class="headerlink" title="RDD 序列化"></a>RDD 序列化</h2><h3 id="闭包检查"><a href="#闭包检查" class="headerlink" title="闭包检查"></a>闭包检查</h3><p>从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。</p>
<h1 id="RDD-依赖关系"><a href="#RDD-依赖关系" class="headerlink" title="RDD 依赖关系"></a>RDD 依赖关系</h1><h2 id="1）RDD血缘关系"><a href="#1）RDD血缘关系" class="headerlink" title="1）RDD血缘关系"></a>1）RDD血缘关系</h2><p>RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage（血统）记录下来，以便恢复丢失的分区。    </p>
<p>RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。   </p>
<pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)
println(fileRDD.toDebugString)
</code></pre>
<h2 id="2）RDD依赖关系"><a href="#2）RDD依赖关系" class="headerlink" title="2）RDD依赖关系"></a>2）RDD依赖关系</h2><p>所谓的依赖关系，其实就是两个相邻 RDD 之间的关系。  </p>
<pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)
println(fileRDD.dependencies)   
</code></pre>
<h2 id="3）RDD-窄依赖（没有Shuffle）"><a href="#3）RDD-窄依赖（没有Shuffle）" class="headerlink" title="3）RDD 窄依赖（没有Shuffle）"></a>3）RDD 窄依赖（没有Shuffle）</h2><p>窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，窄依赖我们形象的比喻为独生子女。</p>
<h2 id="4）RDD宽依赖-（有Shuffle）"><a href="#4）RDD宽依赖-（有Shuffle）" class="headerlink" title="4）RDD宽依赖 （有Shuffle）"></a>4）RDD宽依赖 （有Shuffle）</h2><p>宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。    </p>
<h2 id="RDD-任务划分"><a href="#RDD-任务划分" class="headerlink" title="RDD 任务划分"></a>RDD 任务划分</h2><p>RDD 任务切分中间分为：Application、Job、Stage 和 Task  </p>
<pre><code>Application：初始化一个 SparkContext 即生成一个 Application；
Job：一个 Action 算子就会生成一个 Job；
Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；
Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数。  
</code></pre>
<p>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系</p>
<p><img src="/2023/08/02/Spark-Core/9.png" alt="Spark任务划分流程">     </p>
<h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><h3 id="1-RDD-Cache-缓存"><a href="#1-RDD-Cache-缓存" class="headerlink" title="1) RDD Cache 缓存"></a>1) RDD Cache 缓存</h3><p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用。  </p>
<pre><code>// cache 操作会增加血缘关系，不改变原有的血缘关系  

// 可以更改存储级别
//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)  
</code></pre>
<h4 id="RDD持久化可选存储级别"><a href="#RDD持久化可选存储级别" class="headerlink" title="RDD持久化可选存储级别"></a>RDD持久化可选存储级别</h4><pre><code>object StorageLevel &#123;
 val NONE = new StorageLevel(false, false, false, false)
 val DISK_ONLY = new StorageLevel(true, false, false, false)
 val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)
 val MEMORY_ONLY = new StorageLevel(false, true, false, true)
 val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)
 val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)
 val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)
 val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)
 val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)
 val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)
 val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)
 val OFF_HEAP = new StorageLevel(true, true, true, false, 1)  
</code></pre>
<p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD 的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。  </p>
<p>Spark 会自动对一些 Shuffle 操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点 Shuffle 失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用 persist 或 cache。  </p>
<h3 id="2）RDD-CheckPoint-检查点"><a href="#2）RDD-CheckPoint-检查点" class="headerlink" title="2）RDD CheckPoint 检查点"></a>2）RDD CheckPoint 检查点</h3><p>所谓的检查点其实就是通过将 RDD 中间结果写入磁盘  </p>
<p>对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发。  </p>
<ol start="3">
<li><p>缓存和检查点区别  </p>
<p> 1）Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖。<br> 2）Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高。<br> 3）建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD</p>
</li>
</ol>
<h2 id="RDD-分区器"><a href="#RDD-分区器" class="headerlink" title="RDD 分区器"></a>RDD 分区器</h2><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认分区。   </p>
<p>分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。   </p>
<pre><code>➢ 只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None  
➢ 每个 RDD 的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。  
</code></pre>
<ol>
<li><p>Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余  </p>
</li>
<li><p>Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p>
</li>
</ol>
<h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge。  </p>
<h3 id="系统累加器"><a href="#系统累加器" class="headerlink" title="系统累加器"></a>系统累加器</h3><pre><code>val rdd = sc.makeRDD(List(1,2,3,4,5))
// 声明累加器
var sum = sc.longAccumulator(&quot;sum&quot;);
rdd.foreach(
 num =&gt; &#123;
 // 使用累加器
 sum.add(num)
 &#125;
)
// 获取累加器的值
println(&quot;sum = &quot; + sum.value)
</code></pre>
<h3 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h3><pre><code>// 自定义累加器
// 1. 继承 AccumulatorV2，并设定泛型
// 2. 重写累加器的抽象方法
class WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, 
Long]]&#123;
var map : mutable.Map[String, Long] = mutable.Map()
// 累加器是否为初始状态
override def isZero: Boolean = &#123;
 map.isEmpty
&#125;
// 复制累加器
override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = &#123;
 new WordCountAccumulator
&#125;
// 重置累加器
override def reset(): Unit = &#123;
 map.clear()
&#125;
// 向累加器中增加数据 (In)
override def add(word: String): Unit = &#123;
 // 查询 map 中是否存在相同的单词
 // 如果有相同的单词，那么单词的数量加 1
 // 如果没有相同的单词，那么在 map 中增加这个单词
 map(word) = map.getOrElse(word, 0L) + 1L
&#125;
// 合并累加器
override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): 
Unit = &#123;
 val map1 = map
 val map2 = other.value
 // 两个 Map 的合并
 map = map1.foldLeft(map2)(
 ( innerMap, kv ) =&gt; &#123;
 innerMap(kv._1) = innerMap.getOrElse(kv._1, 0L) + kv._2
 innerMap
 &#125;
 )
&#125;
// 返回累加器的结果 （Out）
override def value: mutable.Map[String, Long] = map
&#125;
</code></pre>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。  </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">SQLServer查询体系学习记录</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-01 11:07:31" itemprop="dateCreated datePublished" datetime="2023-08-01T11:07:31+08:00">2023-08-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 12:59:20" itemprop="dateModified" datetime="2023-08-11T12:59:20+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="SQLServer查询处理体系结构指南"><a href="#SQLServer查询处理体系结构指南" class="headerlink" title="SQLServer查询处理体系结构指南"></a>SQLServer查询处理体系结构指南</h1><h2 id="执行模式"><a href="#执行模式" class="headerlink" title="执行模式"></a>执行模式</h2><p>行执行模式<br>批执行模式 </p>
<h3 id="行执行模式"><a href="#行执行模式" class="headerlink" title="行执行模式"></a>行执行模式</h3><p>行模式执行是用于传统 RDBMS 表（其中数据以行格式存储）的查询处理方法</p>
<h3 id="批执行模式"><a href="#批执行模式" class="headerlink" title="批执行模式"></a>批执行模式</h3><p>批模式执行是一种查询处理方法，用于统一处理多个行（因此采用“批”一词）    </p>
<p>批中的每列都作为一个矢量存储在单独的内存区域中，因此批模式处理是基于矢量的</p>
<p>当在批模式下执行查询并且查询访问列存储索引中的数据时，执行树运算符和子运算符会一次读取列段中的多行。 SQL Server 仅读取结果所需的列，即 SELECT 语句、JOIN 谓词或筛选谓词引用的列</p>
<p><strong>批执行模式VS行执行模式的优势</strong>：一次读取多行，再筛选。避免了行执行模式的多次读取，提高运行效率  </p>
<h2 id="SQL-语句处理"><a href="#SQL-语句处理" class="headerlink" title="SQL 语句处理"></a>SQL 语句处理</h2><p>处理单个 Transact-SQL 语句是 SQL Server 执行 Transact-SQL 语句的最基本方法    </p>
<h3 id="逻辑运算符优先级"><a href="#逻辑运算符优先级" class="headerlink" title="逻辑运算符优先级"></a>逻辑运算符优先级</h3><p>计算顺序依次为：NOT、AND最后是 OR。算术运算符和位运算符优先于逻辑运算符处理  </p>
<h2 id="优化-SELECT-语句"><a href="#优化-SELECT-语句" class="headerlink" title="优化 SELECT 语句"></a>优化 SELECT 语句</h2><p>语句 SELECT 是非过程性的，数据库服务器必须分析语句，以决定提取所请求数据的最有效方法。  </p>
<p>执行此操作的组件称为查询优化器，可以使数据库服务器针对数据库内的更改情况进行动态调整，而无需程序员或数据库管理员输入  </p>
<p>order 排序字段上应该建索引  </p>
<p>从潜在的多个可能的计划中选择一个执行计划的过程称为“优化”  </p>
<p>SQL Server 查询优化器是基于成本的优化器（CBO）</p>
<p>当执行复杂的SQL语句时，不会去分析所有的执行计划成本，会根据算法选一个执行计划，其成本合理地接近最低可能成本的执行计划    </p>
<p><strong>除了CBO，还要考虑运行效率</strong>：SQL Server查询优化器不会仅选择资源成本最低的执行计划;它选择以合理的资源成本向用户返回结果的计划，并且以最快的速度返回结果  </p>
<p>SQL Server 查询优化器总能针对数据库的状态生成一个有效的执行计划    </p>
<p>SQL Server Management Studio 有三个选项可用于显示执行计划</p>
<pre><code>1.估计的执行计划    

2.实际执行计划    

3.实时查询统计信息，这与编译的计划及其执行上下文相同  

    这包括执行过程中的运行时信息，每秒更新一次
</code></pre>
<h2 id="密度"><a href="#密度" class="headerlink" title="密度"></a>密度</h2><p>密度定义数据中存在的唯一值的分布，或给定列的重复值平均数。 密度与值的选择性成反比，密度越小，值的选择性越大  </p>
<h2 id="处理-SELECT-语句"><a href="#处理-SELECT-语句" class="headerlink" title="处理 SELECT 语句"></a>处理 SELECT 语句</h2><p>SQL Server 处理单个 SELECT 语句的基本步骤包括如下内容：  </p>
<p>1.解析select语句</p>
<p>2.生成查询树</p>
<p>3.生成执行计划，并选择合理的执行计划</p>
<p>4.运行执行计划  </p>
<p>5.返回结果</p>
<h2 id="常量折叠和表达式计算"><a href="#常量折叠和表达式计算" class="headerlink" title="常量折叠和表达式计算"></a>常量折叠和表达式计算</h2><h3 id="可折叠表达式"><a href="#可折叠表达式" class="headerlink" title="可折叠表达式"></a>可折叠表达式</h3><p>仅包含常量的算术表达式  </p>
<p>仅包含常量的逻辑表达式  </p>
<p>被 SQL Server 认为可折叠的内置函数包括 CAST 和 CONVERT   </p>
<p>CLR 用户定义类型的确定性方法和确定性的标量值 CLR 用户定义函数  </p>
<h3 id="不可折叠的表达式"><a href="#不可折叠的表达式" class="headerlink" title="不可折叠的表达式"></a>不可折叠的表达式</h3><p>所有其他表达式类型都是不可折叠的。 特别是下列类型的表达式是不可折叠的：  </p>
<p>非常量表达式    </p>
<p>结果取决于局部变量或参数的表达式   </p>
<p>不确定性函数     </p>
<p>用户定义的 Transact-SQL 函数  </p>
<p>结果取决于语言设置的表达式   </p>
<p>结果取决于 SET 选项的表达式  </p>
<p>结果取决于服务器配置选项的表达式  </p>
<h2 id="常量折叠的优点"><a href="#常量折叠的优点" class="headerlink" title="常量折叠的优点"></a>常量折叠的优点</h2><p>表达式不必在运行时重复计算  </p>
<p>查询优化器可使用计算表达式后所得的值来估计 TotalDue &gt; 117.00 + 1000.00 查询部分的结果集的大小  </p>
<h2 id="工作表"><a href="#工作表" class="headerlink" title="工作表"></a>工作表</h2><p>工作表是用于保存中间结果的内部表。 某些 GROUP BY、 ORDER BY或 UNION 查询会生成工作表  </p>
<p>工作表在 tempdb 中生成，并在不再需要时自动删除  </p>
<p>SQL Server 查询处理器对索引视图和非索引视图将区别对待  </p>
<p>索引视图的行以表的格式存储在数据库中   </p>
<p>只有非索引视图的定义才存储，而不存储视图的行  </p>
<p> 如果索引视图中的数据包括所有或部分 Transact-SQL 语句，而且查询优化器确定视图的某个索引是低成本的访问路径，则不论查询中是否引用了该视图的名称，查询优化器都将选择此索引  </p>
<p>视图没有单独的执行计划</p>
<h2 id="存储过程和触发器执行"><a href="#存储过程和触发器执行" class="headerlink" title="存储过程和触发器执行"></a>存储过程和触发器执行</h2><p>SQL Server 仅存储存储过程和触发器的源  </p>
<p>第一次执行存储过程或触发器时，源被编译为执行计划，在内存中被释放后需要重新运行存储过程再次生成  </p>
<p>执行计划缓存和重用  </p>
<p>SQL Server 有一个用于存储执行计划和数据缓冲区的内存池，池内分配给执行计划或数据缓冲区的百分比随系统状态动态波动    </p>
<p>计划缓存有两个不用于存储计划的附加存储：  </p>
<p>“对象计划”缓存存储 (OBJCP)  </p>
<pre><code>用于与持久化对象（存储过程、函数和触发器）相关的计划   
</code></pre>
<p>“SQL 计划”缓存存储 (SQLCP)   </p>
<pre><code>用于与自动参数化、动态或已准备的查询相关的计划  
</code></pre>
<h2 id="从计划缓存中删除执行计划"><a href="#从计划缓存中删除执行计划" class="headerlink" title="从计划缓存中删除执行计划"></a>从计划缓存中删除执行计划</h2><p>只要计划缓存中有足够的存储空间，执行计划就会保留在其中   </p>
<p>当存在内存不足的情况时，SQL Server 数据库引擎将使用基于开销的方法来确定从计划缓存中删除哪些执行计划  </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/31/Hive-on-mr/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/31/Hive-on-mr/" class="post-title-link" itemprop="url">Hive on mr调优</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-31 15:57:39" itemprop="dateCreated datePublished" datetime="2023-07-31T15:57:39+08:00">2023-07-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:05:08" itemprop="dateModified" datetime="2023-08-11T13:05:08+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="我于人间全无敌，不与天战与shui战？"><a href="#我于人间全无敌，不与天战与shui战？" class="headerlink" title="我于人间全无敌，不与天战与shui战？"></a>我于人间全无敌，不与天战与shui战？</h1><p>本文测试数据和Explain可视化工具资料包<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1Q8Zt7gWBF6JkW_Kg6sIWzw">https://pan.baidu.com/s/1Q8Zt7gWBF6JkW_Kg6sIWzw</a><br>提取码：2khx     </p>
<p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1Wdc2f38csrGbC3GA1s0H_w">https://pan.baidu.com/s/1Wdc2f38csrGbC3GA1s0H_w</a><br>提取码：888i   </p>
<p>永久有效，失效来打我~<br><img src="/2023/07/31/Hive-on-mr/1.gif"></p>
<p>Hive on mr  </p>
<p>即Hive引擎选用mapreduce。（目前Hive引擎可选项为Mapreduce&#x2F;Tez&#x2F;Spark）  </p>
<p>调优主要分为下面三个方向 </p>
<p>1)：组件资源调优  </p>
<p>通过控制任务运行的组件资源，实现任务的高效运行</p>
<p>2)：Explain执行计划调优  </p>
<p>通过优化执行计划,保证相同资源配置的情况下，任务运行更流畅  </p>
<p>3): 常有调优参数设置  </p>
<p>开启Hive内置的一些有助于任务高效运行的设置,保障任务流畅运行  </p>
<h2 id="1-组件资源调优"><a href="#1-组件资源调优" class="headerlink" title="1:组件资源调优"></a>1:组件资源调优</h2><h3 id="Yarn资源配置"><a href="#Yarn资源配置" class="headerlink" title="Yarn资源配置"></a>Yarn资源配置</h3><p>需要调整的Yarn参数均与CPU、内存等资源有关，核心配置参数如下  </p>
<p>（1）yarn.nodemanager.resource.memory-mb  </p>
<p>该参数的含义是，一个NodeManager节点分配给Container使用的内存。该参数的配置，<strong>取决于NodeManager所在节点的总内存容量和该节点运行的其他服务的数量</strong>。  </p>
<p>（2）yarn.nodemanager.resource.cpu-vcores  </p>
<p>该参数的含义是，一个NodeManager节点分配给Container使用的CPU核数。该参数的配置，<strong>同样取决于NodeManager所在节点的总CPU核数和该节点运行的其他服务</strong>。</p>
<p>通常是一个核4个G  </p>
<pre><code>即（1）yarn.nodemanager.resource.memory-mb/（2）yarn.nodemanager.resource.cpu-vcores  = 4  
</code></pre>
<p>（3）yarn.scheduler.maximum-allocation-mb  </p>
<p>该参数的含义是，单个Container能够使用的最大内存</p>
<pre><code>（1）yarn.nodemanager.resource.memory-mb /（3）yarn.scheduler.maximum-allocation-mb  = 整数
</code></pre>
<p>（4）yarn.scheduler.minimum-allocation-mb  </p>
<p>该参数的含义是，单个Container能够使用的最小内存，推荐配置(512M)如下：  </p>
<pre><code>&lt;property&gt;
    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
    &lt;value&gt;512&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<h3 id="MapReduce资源配置"><a href="#MapReduce资源配置" class="headerlink" title="MapReduce资源配置"></a>MapReduce资源配置</h3><p>MapReduce资源配置主要包括Map Task的内存和CPU核数，以及Reduce Task的内存和CPU核数  </p>
<p>1）mapreduce.map.memory.mb	  </p>
<p>该参数的含义是，单个Map Task申请的container容器内存大小，其默认值为1024。该值不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围  </p>
<p>该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置： </p>
<pre><code>set  mapreduce.map.memory.mb=2048;  
</code></pre>
<p>2）mapreduce.map.cpu.vcores  </p>
<p>该参数的含义是，单个Map Task申请的container容器cpu核数，其默认值为1。该值一般无需调整</p>
<p>3）mapreduce.reduce.memory.mb	</p>
<p>该参数的含义是，单个Reduce Task申请的container容器内存大小，其默认值为1024。该值同样不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围  </p>
<p>该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置  </p>
<pre><code>set  mapreduce.reduce.memory.mb=2048;  
</code></pre>
<p>4）mapreduce.reduce.cpu.vcores	  </p>
<p>该参数的含义是，单个Reduce Task申请的container容器cpu核数，其默认值为1。该值一般无需调整  </p>
<h1 id="2-Explain执行计划调优"><a href="#2-Explain执行计划调优" class="headerlink" title="2.Explain执行计划调优"></a>2.Explain执行计划调优</h1><h2 id="测试用表"><a href="#测试用表" class="headerlink" title="测试用表"></a>测试用表</h2><h3 id="1-订单表-2000w条数据"><a href="#1-订单表-2000w条数据" class="headerlink" title="1.订单表(2000w条数据)"></a>1.订单表(2000w条数据)</h3><h4 id="建表语句"><a href="#建表语句" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt;
drop table if exists order_detail;
create table order_detail(
id           string comment &#39;订单id&#39;,
user_id      string comment &#39;用户id&#39;,
product_id   string comment &#39;商品id&#39;,
province_id  string comment &#39;省份id&#39;,
create_time  string comment &#39;下单时间&#39;,
product_num  int comment &#39;商品件数&#39;,
total_amount decimal(16, 2) comment &#39;下单金额&#39;
)
partitioned by (dt string)
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<h4 id="数据装载"><a href="#数据装载" class="headerlink" title="数据装载"></a>数据装载</h4><p>将order_detail.txt文件上传到hiveserver2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。</p>
<p>注：文件较大，请耐心等待。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/order_detail.txt&#39; overwrite into table order_detail partition(dt=&#39;2020-06-14&#39;); 
</code></pre>
<h3 id="2-支付表-600w条数据"><a href="#2-支付表-600w条数据" class="headerlink" title="2.支付表(600w条数据)"></a>2.支付表(600w条数据)</h3><h4 id="建表语句-1"><a href="#建表语句-1" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt;
drop table if exists payment_detail;
create table payment_detail(
id              string comment &#39;支付id&#39;,
order_detail_id string comment &#39;订单明细id&#39;,
user_id         string comment &#39;用户id&#39;,
payment_time    string comment &#39;支付时间&#39;,
total_amount    decimal(16, 2) comment &#39;支付金额&#39;
)
partitioned by (dt string)
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<h4 id="数据装载-1"><a href="#数据装载-1" class="headerlink" title="数据装载"></a>数据装载</h4><p>将payment_detail.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。</p>
<p>注：文件较大，请耐心等待。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/payment_detail.txt&#39; overwrite into table payment_detail partition(dt=&#39;2020-06-14&#39;);  
</code></pre>
<h3 id="3-商品信息表-100w条数据"><a href="#3-商品信息表-100w条数据" class="headerlink" title="3.商品信息表(100w条数据)"></a>3.商品信息表(100w条数据)</h3><h4 id="建表语句-2"><a href="#建表语句-2" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt; 
drop table if exists product_info;
create table product_info(
id           string comment &#39;商品id&#39;,
product_name string comment &#39;商品名称&#39;,
price        decimal(16, 2) comment &#39;价格&#39;,
category_id  string comment &#39;分类id&#39;
)
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<h4 id="数据装载-2"><a href="#数据装载-2" class="headerlink" title="数据装载"></a>数据装载</h4><p>将product_info.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/product_info.txt&#39; overwrite into table product_info;  
</code></pre>
<h3 id="4-省份信息表-34条数据"><a href="#4-省份信息表-34条数据" class="headerlink" title="4.省份信息表(34条数据)"></a>4.省份信息表(34条数据)</h3><h4 id="建表语句-3"><a href="#建表语句-3" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt; 
drop table if exists province_info;
create table province_info(
id            string comment &#39;省份id&#39;,
province_name string comment &#39;省份名称&#39;
)
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<h4 id="数据装载-3"><a href="#数据装载-3" class="headerlink" title="数据装载"></a>数据装载</h4><p>将province_info.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/province_info.txt&#39; overwrite into table province_info;  
</code></pre>
<h2 id="Explain查看执行计划（重点）"><a href="#Explain查看执行计划（重点）" class="headerlink" title="Explain查看执行计划（重点）"></a>Explain查看执行计划（重点）</h2><p>Explain呈现的执行计划，由一系列Stage组成，这一系列Stage具有依赖关系，每个Stage对应一个MapReduce Job，或者一个文件系统操作等。  </p>
<p>若某个Stage对应的一个MapReduce Job，其Map端和Reduce端的计算逻辑分别由Map Operator Tree和Reduce Operator Tree进行描述，Operator Tree由一系列的Operator组成，一个Operator代表在Map或Reduce阶段的一个单一的逻辑操作，例如TableScan Operator，Select Operator，Join Operator等</p>
<h3 id="常见的Operator及其作用如下："><a href="#常见的Operator及其作用如下：" class="headerlink" title="常见的Operator及其作用如下："></a>常见的Operator及其作用如下：</h3><p>TableScan：表扫描操作，通常map端第一个操作肯定是表扫描操作  </p>
<p>Select Operator：选取操作   </p>
<p>Group By Operator：分组聚合操作  </p>
<p>Reduce Output Operator：输出到 reduce 操作  </p>
<p>Filter Operator：过滤操作  </p>
<p>Join Operator：join 操作  </p>
<p>File Output Operator：文件输出操作  </p>
<p>Fetch Operator 客户端获取数据操作  </p>
<h3 id="Explain查看执行计划基本语法"><a href="#Explain查看执行计划基本语法" class="headerlink" title="Explain查看执行计划基本语法"></a>Explain查看执行计划基本语法</h3><p>EXPLAIN [FORMATTED | EXTENDED | DEPENDENCY] query-sql  </p>
<pre><code>FORMATTED：将执行计划以JSON字符串的形式输出  

EXTENDED：输出执行计划中的额外信息，通常是读写的文件名等信息  

DEPENDENCY：输出执行计划读取的表及分区  
</code></pre>
<h3 id="Explain执行计划可视化工具使用方法"><a href="#Explain执行计划可视化工具使用方法" class="headerlink" title="Explain执行计划可视化工具使用方法"></a>Explain执行计划可视化工具使用方法</h3><p>1.将本文开头百度网盘共享的dist文件压缩包上传至linux服务器中  </p>
<p>2.unzip dist.zip 解压  </p>
<p>3.进入解压后的dist文件夹下  </p>
<p>4.python -m SimpleHTTPServer 8901  启动可视化工具（此处的8901是我指定的可用端口号，可以按自己的想法设置）<br><img src="/2023/07/31/Hive-on-mr/1.png" alt="Explain可视化工具">  </p>
<p>5.将Explain执行计划制作成json格式粘贴到可视化工具里即可查看  </p>
<p><img src="/2023/07/31/Hive-on-mr/2.png" alt="Explain执行计划json格式">  </p>
<p>6.Explain可视化工具示例  </p>
<p><img src="/2023/07/31/Hive-on-mr/3.png" alt="示例">  </p>
<h2 id="HQL语法优化之分组聚合优化-（map-site）"><a href="#HQL语法优化之分组聚合优化-（map-site）" class="headerlink" title="HQL语法优化之分组聚合优化 （map-site）"></a>HQL语法优化之分组聚合优化 （map-site）</h2><p>Hive对分组聚合的优化主要围绕着减少Shuffle数据量进行，具体做法是map-side聚合  </p>
<p>在map端维护一个hash table进行预聚合，按照分组字段分区，发送至reduce端，完成最终的聚合。有效的减少shuffle操作的数量，达到提高运行效率的目的。  </p>
<h3 id="map-side-聚合相关的参数如下："><a href="#map-side-聚合相关的参数如下：" class="headerlink" title="map-side 聚合相关的参数如下："></a>map-side 聚合相关的参数如下：</h3><p>–启用map-side聚合 </p>
<pre><code>set hive.map.aggr=true;  
</code></pre>
<p>–用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。  </p>
<pre><code>set hive.map.aggr.hash.min.reduction=0.5;  
</code></pre>
<p>–用于检测源表是否适合map-side聚合的条数  </p>
<pre><code>set hive.groupby.mapaggr.checkinterval=100000;  
</code></pre>
<p>–map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。  </p>
<pre><code>set hive.map.aggr.hash.force.flush.memory.threshold=0.9;  
</code></pre>
<h3 id="HQL语法优化之分组聚合优化-（map-site）优化案例"><a href="#HQL语法优化之分组聚合优化-（map-site）优化案例" class="headerlink" title="HQL语法优化之分组聚合优化 （map-site）优化案例"></a>HQL语法优化之分组聚合优化 （map-site）优化案例</h3><pre><code>select
product_id,
count(*)
from order_detail
group by product_id;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/4.png" alt="启用map-site调优前后Explain执行计划对比">  </p>
<h4 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h4><p>开启map-side聚合，配置以下参数：  </p>
<pre><code>set hive.map.aggr=true;  

set hive.map.aggr.hash.min.reduction=0.5;   

set hive.groupby.mapaggr.checkinterval=100000;  

set hive.map.aggr.hash.force.flush.memory.threshold=0.9;  
</code></pre>
<h3 id="HQL语法优化之Join优化"><a href="#HQL语法优化之Join优化" class="headerlink" title="HQL语法优化之Join优化"></a>HQL语法优化之Join优化</h3><p>Join算法概述</p>
<p>Common Join : 常规join，不做优化    </p>
<p><img src="/2023/07/31/Hive-on-mr/5.png" alt="Common Join原理图">  </p>
<p>Map Join：<strong>适用于大表join小表</strong>，将小表数据缓存为hash table（内存表），然后扫描大表数据，这样在map端即可完成关联操作  </p>
<p><img src="/2023/07/31/Hive-on-mr/6.png" alt="Map Join原理图"></p>
<p>Bucket Map Join：<strong>适用于大表join大表</strong>  通过分桶对数据进行切分，让有限的内存缓存一部分分桶数据，再对另一个大表进行遍历操作     </p>
<pre><code> Bucket Map Join的使用要求：若能保证参与**join的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍**
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/7.png" alt="Bucket Map Join原理图"></p>
<p>SMB Map Join：<strong>适用于大表join大表</strong>，两个分桶之间的join实现原理为Sort Merge Join算法。前提条件是两个大表分桶数据都要排好序，这样就无需缓存在内存中，通过Sort Merge Join算法直接完成逐条遍历计算。  </p>
<pre><code>SMB Map Join的使用要求:参与join的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍    
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/8.png" alt="SMB Map Join原理图"></p>
<p>Bucket Map Join 和 SMB Map Join 的区别：  </p>
<p>1.SMB Map Join在Bucket Map join的基础上，要求分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段</p>
<p>2.两个分桶之间的join实现算法不一样  </p>
<pre><code>Bucket Map Join，两个分桶之间的join实现原理为Hash Join算法    

SMB Map Join，两个分桶之间的join实现原理为Sort Merge Join算法   
</code></pre>
<p>3.相较于Bucket Map Join，SMB Map Join对分桶大小是没有要求的,因为SMB Map Join不需要缓存数据  </p>
<h4 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h4><p>优化说明 </p>
<p><img src="/2023/07/31/Hive-on-mr/9.png" alt="Map join原理解析">   </p>
<p>寻找大表候选人：  </p>
<p>a inner join b时，a,b都可以作为大表候选人，只返回a,b都能连接上的数据  </p>
<p>A Left join  b时，只有b才可以作为大表候选人，这样才会遍历A表数据，输出A的所有数据  </p>
<p>A right join b时，只有A才可以作为大表候选人，这样才会遍历B表的数据，输出B的所有数据  </p>
<p>A full join b时，没有大表候选人，因为无论选a还是B作为大表候选人，都无法输出a和b的所有数据  </p>
<p>Conditionaltask：条件任务  </p>
<p>图中涉及到的参数如下：    </p>
<p>–启动Map Join自动转换  </p>
<pre><code>set hive.auto.convert.join=true;  
</code></pre>
<p>–一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的已知大小总和&lt;&#x3D;该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。 </p>
<pre><code>set hive.mapjoin.smalltable.filesize=250000;    
</code></pre>
<p>注意此处的内存大小参数与实际读取磁盘文件的大小是有差别的，考虑到磁盘文件解压缩，反序列化和对象信息，相同文件在内存中要比在磁盘中占用的空间放大大概10倍  </p>
<p>所以该参数设置为1G，就表明拿取磁盘中1G大小的文件，但内存需要占用10G  </p>
<p>实际生产中，将参数配置到hive-site等配置文件中。设置size参数时，通常配置为map端内存的1&#x2F;2 ~2&#x2F;3范围内作为缓存，记得size的值应该是map_memory*2&#x2F;3 的十分之一大小才行，否则磁盘文件读取到内存，会oom  </p>
<p>生产中，配置文件中的调优参数生效后，大部分sql语句性能提高了，如果极少部分任务还是慢sql，就需要单独调优，在语句中加入set参数的方式进行针对性局部调优  </p>
<p>–开启无条件转Map Join</p>
<pre><code>set hive.auto.convert.join.noconditionaltask=true;  
</code></pre>
<p>–无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和&lt;&#x3D;该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask.size=10000000;  
</code></pre>
<h5 id="Map-Join优化案例"><a href="#Map-Join优化案例" class="headerlink" title="Map Join优化案例"></a>Map Join优化案例</h5><pre><code>select  *
from order_detail od
join product_info product on od.product_id = product.id
join province_info province on od.province_id = province.id;  
</code></pre>
<p>优化前：设置 set hive.auto.convert.join&#x3D;true &#x3D; false  </p>
<p><img src="/2023/07/31/Hive-on-mr/10.png" alt="Map Join优化前">    </p>
<p>对参与关联的三张表进行分析，发现各自大小如下   </p>
<p><img src="/2023/07/31/Hive-on-mr/11.png" alt="参与关联的三张表大小"></p>
<h6 id="Map-Join方案一："><a href="#Map-Join方案一：" class="headerlink" title="Map Join方案一："></a>Map Join方案一：</h6><p>启用Map Join自动转换 </p>
<pre><code>set hive.auto.convert.join=true;  
</code></pre>
<p>不使用无条件转Map Join  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask=false;  
</code></pre>
<p>调整hive.mapjoin.smalltable.filesize参数，使其大于等于product_info  </p>
<pre><code>set hive.mapjoin.smalltable.filesize=25285707;  
</code></pre>
<p>这样可保证将两个Common Join operator均可转为Map Join operator，并保留Common Join作为后备计划，保证计算任务的稳定  </p>
<p><img src="/2023/07/31/Hive-on-mr/12.png" alt="Map Jion优化方案一">  </p>
<h6 id="Map-Join方案二："><a href="#Map-Join方案二：" class="headerlink" title="Map Join方案二："></a>Map Join方案二：</h6><p>启用Map Join自动转换  </p>
<pre><code>set hive.auto.convert.join=true;  
</code></pre>
<p>使用无条件转Map Join  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask=true;  
</code></pre>
<p>调整hive.auto.convert.join.noconditionaltask.size参数，使其大于等于product_info和province_info之和  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask.size=25286076;  
</code></pre>
<p>这样可直接将两个Common Join operator转为两个Map Join operator，并且由于两个Map Join operator的小表大小之和小于等于hive.auto.convert.join.noconditionaltask.size，故两个Map Join operator任务可合并为同一个。这个方案计算效率最高，但需要的内存也是最多的</p>
<p><img src="/2023/07/31/Hive-on-mr/13.png" alt="Map Jion优化方案二"> </p>
<h6 id="Map-Join方案三："><a href="#Map-Join方案三：" class="headerlink" title="Map Join方案三："></a>Map Join方案三：</h6><p>启用Map Join自动转换    </p>
<pre><code>set hive.auto.convert.join=true;   
</code></pre>
<p>使用无条件转Map Join  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask=true;    
</code></pre>
<p>调整hive.auto.convert.join.noconditionaltask.size参数，使其等于product_info</p>
<pre><code>set hive.auto.convert.join.noconditionaltask.size=25285707;  
</code></pre>
<p>这样可直接将两个Common Join operator转为Map Join operator，但不会将两个Map Join的任务合并。该方案计算效率比方案二低，但需要的内存也更少</p>
<p><img src="/2023/07/31/Hive-on-mr/14.png" alt="Map Join优化方案三"></p>
<h4 id="Bucket-Map-Join"><a href="#Bucket-Map-Join" class="headerlink" title="Bucket Map Join"></a>Bucket Map Join</h4><p>Bucket Map Join不支持自动转换，发须通过用户在SQL语句中提供如下Hint提示，并配置如下相关参数，方可使用  </p>
<pre><code>select /*+ mapjoin(ta) */
ta.id,
tb.id
from table_a ta
join table_b tb on ta.id=tb.id; 
</code></pre>
<p>相关参数  </p>
<p>–关闭cbo优化，cbo会导致hint信息被忽略  </p>
<pre><code>set hive.cbo.enable=false;
</code></pre>
<p>–map join hint默认会被忽略(因为已经过时)，需将如下参数设置为false  </p>
<pre><code>set hive.ignore.mapjoin.hint=false;  
</code></pre>
<p>–启用bucket map join优化功能  </p>
<pre><code>set hive.optimize.bucketmapjoin = true;    
</code></pre>
<h5 id="Bucket-Map-Join优化案例"><a href="#Bucket-Map-Join优化案例" class="headerlink" title="Bucket Map Join优化案例"></a>Bucket Map Join优化案例</h5><pre><code>select
    *
from(
    select
        *
    from order_detail
    where dt=&#39;2020-06-14&#39;
)od
join(
    select
        	*
    from payment_detail
    where dt=&#39;2020-06-14&#39;
)pd
on od.id=pd.order_detail_id;  
</code></pre>
<h6 id="Bucket-Map-Join优化前"><a href="#Bucket-Map-Join优化前" class="headerlink" title="Bucket Map Join优化前"></a>Bucket Map Join优化前</h6><pre><code>set hive.auto.convert.join=false;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/15.png" alt="Bucket Map Join优化前"></p>
<h6 id="Bucket-Map-Join优化思路"><a href="#Bucket-Map-Join优化思路" class="headerlink" title="Bucket Map Join优化思路"></a>Bucket Map Join优化思路</h6><p>经分析，参与join的两张表，数据量如下  </p>
<p>order_detail	  1176009934（约1122M）<br>payment_detail	  334198480（约319M）  </p>
<p>可以认为是大表join大表，可尝试采用Bucket Map Join优化方案  </p>
<p>首先需要依据源表创建两个分桶表，order_detail建议分16个bucket  </p>
<p>payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段  </p>
<p>–订单表 </p>
<pre><code>hive (default)&gt; 
drop table if exists order_detail_bucketed;
create table order_detail_bucketed(
id           string comment &#39;订单id&#39;,
user_id      string comment &#39;用户id&#39;,
product_id   string comment &#39;商品id&#39;,
province_id  string comment &#39;省份id&#39;,
create_time  string comment &#39;下单时间&#39;,
product_num  int comment &#39;商品件数&#39;,
total_amount decimal(16, 2) comment &#39;下单金额&#39;
)
clustered by (id) into 16 buckets
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<p>–支付表  </p>
<pre><code>hive (default)&gt; 
drop table if exists payment_detail_bucketed;
create table payment_detail_bucketed(
id              string comment &#39;支付id&#39;,
order_detail_id string comment &#39;订单明细id&#39;,
user_id         string comment &#39;用户id&#39;,
payment_time    string comment &#39;支付时间&#39;,
total_amount    decimal(16, 2) comment &#39;支付金额&#39;
)
clustered by (order_detail_id) into 8 buckets
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<p>然后向两个分桶表导入数据。  </p>
<p>–订单表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table order_detail_bucketed
select
id,
user_id,
product_id,
province_id,
create_time,
product_num,
total_amount   
from order_detail
where dt=&#39;2023-07-28&#39;;
</code></pre>
<p>–分桶表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table payment_detail_bucketed
select
id,
order_detail_id,
user_id,
payment_time,
total_amount
from payment_detail
where dt=&#39;2020-07-28&#39;;
</code></pre>
<p>然后设置以下参数：  </p>
<p>–关闭cbo优化，cbo会导致hint信息被忽略，需将如下参数修改为false  </p>
<pre><code>set hive.cbo.enable=false;  
</code></pre>
<p>–map join hint默认会被忽略(因为已经过时)，需将如下参数修改为false  </p>
<pre><code>set hive.ignore.mapjoin.hint=false;  
</code></pre>
<p>–启用bucket map join优化功能,默认不启用，需将如下参数修改为true </p>
<pre><code>set hive.optimize.bucketmapjoin = true;
</code></pre>
<p>最后在重写SQL语句，如下：  </p>
<pre><code>select /*+ mapjoin(pd) */
    *
from order_detail_bucketed od
join payment_detail_bucketed pd on od.id = pd.order_detail_id; 
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/16.png">  </p>
<p>由于bucket map join和map join的执行计划非常像，如何确定该执行计划是否属于bucket map join ? </p>
<p><img src="/2023/07/31/Hive-on-mr/17.png"></p>
<h4 id="Sort-Merge-Bucket-Map-Join"><a href="#Sort-Merge-Bucket-Map-Join" class="headerlink" title="Sort Merge Bucket Map Join"></a>Sort Merge Bucket Map Join</h4><p>优化说明  </p>
<p>Sort Merge Bucket Map Join有两种触发方式，包括Hint提示和自动转换。Hint提示已过时，不推荐使用。下面是自动转换的相关参数:  </p>
<p>–启动Sort Merge Bucket Map Join优化  </p>
<pre><code>set hive.optimize.bucketmapjoin.sortedmerge=true;  
</code></pre>
<p>–使用自动转换SMB Join  </p>
<pre><code>set hive.auto.convert.sortmerge.join=true;   
</code></pre>
<h5 id="Sort-Merge-Bucket-Map-Join优化案例"><a href="#Sort-Merge-Bucket-Map-Join优化案例" class="headerlink" title="Sort Merge Bucket Map Join优化案例"></a>Sort Merge Bucket Map Join优化案例</h5><pre><code>select
        *
from(
    select
            *
    from order_detail
    where dt=&#39;2020-06-14&#39;
)od
join(
    select
        *
    from payment_detail
    where dt=&#39;2020-06-14&#39;
)pd
on od.id=pd.order_detail_id;  
</code></pre>
<h5 id="Sort-Merge-Bucket-Map-Join优化思路"><a href="#Sort-Merge-Bucket-Map-Join优化思路" class="headerlink" title="Sort Merge Bucket Map Join优化思路"></a>Sort Merge Bucket Map Join优化思路</h5><p>order_detail	1176009934（约1122M）<br>payment_detail	334198480（约319M）</p>
<p>两张表都相对较大，除了可以考虑采用Bucket Map Join算法，还可以考虑SMB Join。相较于Bucket Map Join，SMB Map Join对分桶大小是没有要求的    </p>
<p>首先需要依据源表创建两个的有序的分桶表，order_detail建议分16个bucket，payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段和排序字段</p>
<p>–订单表  </p>
<pre><code>hive (default)&gt; 
drop table if exists order_detail_sorted_bucketed;
create table order_detail_sorted_bucketed(
id           string comment &#39;订单id&#39;,
user_id      string comment &#39;用户id&#39;,
product_id   string comment &#39;商品id&#39;,
province_id  string comment &#39;省份id&#39;,
create_time  string comment &#39;下单时间&#39;,
product_num  int comment &#39;商品件数&#39;,
total_amount decimal(16, 2) comment &#39;下单金额&#39;
)	
clustered by (id) sorted by(id) into 16 buckets
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<p>–支付表  </p>
<pre><code>hive (default)&gt; 
drop table if exists payment_detail_sorted_bucketed;
create table payment_detail_sorted_bucketed(
id              string comment &#39;支付id&#39;,
order_detail_id string comment &#39;订单明细id&#39;,
user_id         string comment &#39;用户id&#39;,
payment_time    string comment &#39;支付时间&#39;,
total_amount    decimal(16, 2) comment &#39;支付金额&#39;
)
clustered by (order_detail_id) sorted by(order_detail_id) into 8 buckets
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<p>然后向两个分桶表导入数据。  </p>
<p>–订单表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table order_detail_sorted_bucketed
select
id,
user_id,
product_id,
province_id,
create_time,
product_num,
total_amount   
from order_detail
where dt=&#39;2023-07-28&#39;;
</code></pre>
<p>–分桶表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table payment_detail_sorted_bucketed
select
id,
order_detail_id,
user_id,
payment_time,
total_amount
from payment_detail
where dt=&#39;2023-07-28&#39;;  
</code></pre>
<p>–启动Sort Merge Bucket Map Join优化  </p>
<pre><code>set hive.optimize.bucketmapjoin.sortedmerge=true;  
</code></pre>
<p>–使用自动转换SMB Join  </p>
<pre><code>set hive.auto.convert.sortmerge.join=true; 
</code></pre>
<p>最后在重写SQL语句，如下：  </p>
<pre><code>hive (default)&gt; 
select
    *
from order_detail_sorted_bucketed od
join payment_detail_sorted_bucketed pd
on od.id = pd.order_detail_id;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/18.png">  </p>
<h3 id="HQL语法优化之数据倾斜"><a href="#HQL语法优化之数据倾斜" class="headerlink" title="HQL语法优化之数据倾斜"></a>HQL语法优化之数据倾斜</h3><p>数据倾斜概述</p>
<p>数据倾斜问题，通常是指参与计算的数据分布不均，即某个key或者某些key的数据量远超其他key，导致在shuffle阶段，大量相同key的数据被发往同一个Reduce，进而导致该Reduce所需的时间远超其他Reduce，成为整个任务的瓶颈</p>
<p>Hive中的数据倾斜常出现在<strong>分组聚合</strong>和<strong>join操作</strong>的场景中  </p>
<h4 id="分组聚合导致的数据倾斜-（Map-Side聚合-Skew-GroupBy优化）"><a href="#分组聚合导致的数据倾斜-（Map-Side聚合-Skew-GroupBy优化）" class="headerlink" title="分组聚合导致的数据倾斜 （Map-Side聚合&#x2F;Skew-GroupBy优化）"></a>分组聚合导致的数据倾斜 （Map-Side聚合&#x2F;Skew-GroupBy优化）</h4><p>如果group by分组字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题   </p>
<p>由分组聚合导致的数据倾斜问题，有以下两种解决思路    </p>
<h5 id="Map-Side聚合"><a href="#Map-Side聚合" class="headerlink" title="Map-Side聚合"></a>Map-Side聚合</h5><p>开启Map-Side聚合后，数据会现在Map端完成部分聚合工作。这样一来即便原始数据是倾斜的，经过Map端的初步聚合后，发往Reduce的数据也就不再倾斜了，最佳状态下，Map-端聚合能完全屏蔽数据倾斜问题。  </p>
<p>相关参数如下：  </p>
<pre><code>set hive.map.aggr=true;

set hive.map.aggr.hash.min.reduction=0.5;  

set hive.groupby.mapaggr.checkinterval=100000;  

set hive.map.aggr.hash.force.flush.memory.threshold=0.9;   
</code></pre>
<h5 id="Skew-GroupBy优化"><a href="#Skew-GroupBy优化" class="headerlink" title="Skew-GroupBy优化"></a>Skew-GroupBy优化</h5><p>Skew-GroupBy的原理是启动两个MR任务，第一个MR按照随机数分区，将数据分散发送到Reduce，完成部分聚合，第二个MR按照分组字段分区，完成最终聚合  </p>
<p>–启用分组聚合数据倾斜优化  </p>
<pre><code>set hive.groupby.skewindata=true;  
</code></pre>
<p>–关闭map-side聚合  </p>
<pre><code>set hive.map.aggr=false;    
</code></pre>
<h4 id="Join导致的数据倾斜"><a href="#Join导致的数据倾斜" class="headerlink" title="Join导致的数据倾斜"></a>Join导致的数据倾斜</h4><p>如果关联字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题。  </p>
<p>由join导致的数据倾斜问题，有如下三种解决方案：<br>map join<br>skew join<br>调整sql，通过sql语句将倾斜数据打散成更小的块  </p>
<h5 id="map-join（适用于大表join小表时发生数据倾斜的场景）"><a href="#map-join（适用于大表join小表时发生数据倾斜的场景）" class="headerlink" title="map join（适用于大表join小表时发生数据倾斜的场景）"></a>map join（<strong>适用于大表join小表时发生数据倾斜的场景</strong>）</h5><p> 使用map join算法，join操作仅在map端就能完成，没有shuffle操作，没有reduce阶段，自然不会产生reduce端的数据倾斜     </p>
<p>相关参数参照上文中map join部分内容  </p>
<pre><code>set hive.auto.convert.join=true;  
set hive.mapjoin.smalltable.filesize=250000;  
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000000;
</code></pre>
<h5 id="skew-join（对两表中倾斜的key的数据量有要求）"><a href="#skew-join（对两表中倾斜的key的数据量有要求）" class="headerlink" title="skew join（对两表中倾斜的key的数据量有要求）"></a>skew join（<strong>对两表中倾斜的key的数据量有要求</strong>）</h5><p>skew join的原理是，为倾斜的大key单独启动一个map join任务进行计算，其余key进行正常的common join  </p>
<p><img src="/2023/07/31/Hive-on-mr/19.png" alt="Skew Join原理图">  </p>
<p>相关参数如下：  </p>
<p>–启用skew join优化  </p>
<pre><code>set hive.optimize.skewjoin=true;  
</code></pre>
<p>–触发skew join的阈值，若某个key的行数超过该参数值，则触发  </p>
<pre><code>set hive.skewjoin.key=100000;    
</code></pre>
<p>对两表中倾斜的key的数据量有要求，要求一张表中的倾斜key的数据量比较小（方便走mapjoin）  </p>
<h5 id="SQL打散"><a href="#SQL打散" class="headerlink" title="SQL打散"></a>SQL打散</h5><pre><code>select
    *
from(
    select --打散操作
    concat(id,&#39;_&#39;,cast(rand()*2 as int)) id,
    value
from A
)ta
join(
    select --扩容操作
        concat(id,&#39;_&#39;,0) id,
        value
    from B
    union all
    select
        concat(id,&#39;_&#39;,1) id,
           value
    from B
)tb
on ta.id=tb.id;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/20.png" alt="SQL打散"></p>
<h4 id="HQL语法优化之任务并行度"><a href="#HQL语法优化之任务并行度" class="headerlink" title="HQL语法优化之任务并行度"></a>HQL语法优化之任务并行度</h4><p>对于一个分布式的计算任务而言，设置一个合适的并行度十分重要。Hive的计算任务由MapReduce完成，故并行度的调整需要分为Map端和Reduce端</p>
<h5 id="Map端并行度"><a href="#Map端并行度" class="headerlink" title="Map端并行度"></a>Map端并行度</h5><p>Map端的并行度，也就是Map的个数。是由输入文件的切片数决定的。一般情况下，Map端的并行度无需手动调整  </p>
<p>以下特殊情况可考虑调整map端并行度：  </p>
<p>1）查询的表中存在大量小文件    </p>
<pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  
</code></pre>
<p>2）map端有复杂的查询逻辑  </p>
<p>在计算资源充足的情况下，可考虑增大map端的并行度，令map task多一些，每个map task计算的数据少一些  </p>
<p>–一个切片的最大值  </p>
<pre><code>set mapreduce.input.fileinputformat.split.maxsize=256000000;  
</code></pre>
<h5 id="Reduce端并行度"><a href="#Reduce端并行度" class="headerlink" title="Reduce端并行度"></a>Reduce端并行度</h5><p>Reduce端的并行度，也就是Reduce个数。相对来说，更需要关注。Reduce端的并行度，可由用户自己指定，也可由Hive自行根据该MR Job输入的文件大小进行估算    </p>
<p>Reduce端的并行度的相关参数如下：  </p>
<p>–指定Reduce端并行度，默认值为-1，表示用户未指定  </p>
<pre><code>set mapreduce.job.reduces;  
</code></pre>
<p>–Reduce端并行度最大值  </p>
<pre><code>set hive.exec.reducers.max;  
</code></pre>
<p>–单个Reduce Task计算的数据量，用于估算Reduce并行度  </p>
<pre><code>set hive.exec.reducers.bytes.per.reducer;
</code></pre>
<h6 id="估算逻辑"><a href="#估算逻辑" class="headerlink" title="估算逻辑"></a>估算逻辑</h6><p>假设Job输入的文件大小为totalInputBytes  </p>
<p>参数hive.exec.reducers.bytes.per.reducer的值为bytesPerReducer。  </p>
<p>参数hive.exec.reducers.max的值为maxReducers。  </p>
<p>则Reduce端的并行度为：  </p>
<pre><code>min(ceil(totalInputBytes/bytesPerReducer),maxReducers)  
</code></pre>
<h4 id="HQL语法优化之小文件合并"><a href="#HQL语法优化之小文件合并" class="headerlink" title="HQL语法优化之小文件合并"></a>HQL语法优化之小文件合并</h4><p>Map端输入文件合并   </p>
<p>合并Map端输入的小文件，是指将多个小文件划分到一个切片中，进而由一个Map Task去处理。目的是防止为单个小文件启动一个Map Task，浪费计算资源  </p>
<p>–可将多个小文件切片，合并为一个切片，进而由一个map任务处理  </p>
<pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  
</code></pre>
<p>Reduce输出文件合并  </p>
<p>合并Reduce端输出的小文件，是指将多个小文件合并成大文件。目的是减少HDFS小文件数量。其原理是根据计算任务输出文件的平均大小进行判断，若符合条件，则单独启动一个额外的任务进行合并  </p>
<p>–开启合并map only任务输出的小文件  </p>
<pre><code>set hive.merge.mapfiles=true;
</code></pre>
<p>–开启合并map reduce任务输出的小文件  </p>
<pre><code>set hive.merge.mapredfiles=true;
</code></pre>
<p>–合并后的文件大小</p>
<pre><code>set hive.merge.size.per.task=256000000;
</code></pre>
<p>–触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并 </p>
<pre><code>set hive.merge.smallfiles.avgsize=16000000;  
</code></pre>
<h3 id="其他优化"><a href="#其他优化" class="headerlink" title="其他优化"></a>其他优化</h3><h4 id="1-CBO优化"><a href="#1-CBO优化" class="headerlink" title="1.CBO优化"></a>1.CBO优化</h4><p>CBO是指Cost based Optimizer，即基于计算成本的优化</p>
<p>在Hive中，计算成本模型考虑到了：数据的行数、CPU、本地IO、HDFS IO、网络IO等方面  </p>
<p>目前CBO在hive的MR引擎下主要用于join的优化，例如多表join的join顺序  </p>
<p>–是否启用cbo优化   </p>
<pre><code>set hive.cbo.enable=true;    
</code></pre>
<h4 id="2-谓词下推"><a href="#2-谓词下推" class="headerlink" title="2.谓词下推"></a>2.谓词下推</h4><p>谓词下推（predicate pushdown）是指，尽量将过滤操作前移，以减少后续计算步骤的数据量</p>
<p>–是否启动谓词下推（predicate pushdown）优化  </p>
<pre><code>set hive.optimize.ppd = true;  
</code></pre>
<p>CBO优化也会完成一部分的谓词下推优化工作，因为在执行计划中，谓词越靠前，整个计划的计算成本就会越低</p>
<h4 id="3-矢量化查询"><a href="#3-矢量化查询" class="headerlink" title="3.矢量化查询"></a>3.矢量化查询</h4><p>Hive的矢量化查询优化，依赖于CPU的矢量化计算，CPU的矢量化计算的基本原理如下图  </p>
<p><img src="/2023/07/31/Hive-on-mr/21.png" alt="矢量化计算原理">  </p>
<pre><code>set hive.vectorized.execution.enabled=true;  
</code></pre>
<p>若执行计划中，出现“Execution mode: vectorized”字样，即表明使用了矢量化计算。  </p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution#VectorizedQueryExecution-Limitations">矢量化计算官方文档</a> </p>
<h4 id="4-Fetch抓取"><a href="#4-Fetch抓取" class="headerlink" title="4.Fetch抓取"></a>4.Fetch抓取</h4><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算  </p>
<p>Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果到控制台</p>
<p>–是否在特定场景转换为fetch 任务  </p>
<p>–设置为none表示不转换  </p>
<p>–设置为minimal表示支持select *，分区字段过滤，Limit等  </p>
<p>–设置为more表示支持select 任意字段,包括函数，过滤，和limit等  </p>
<pre><code>set hive.fetch.task.conversion=more;  
</code></pre>
<h4 id="5-本地模式（不上yarn）"><a href="#5-本地模式（不上yarn）" class="headerlink" title="5.本地模式（不上yarn）"></a>5.本地模式（不上yarn）</h4><p>Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短</p>
<p>–开启自动转换为本地模式  </p>
<pre><code>set hive.exec.mode.local.auto=true;  
</code></pre>
<p>–设置local MapReduce的最大输入数据量，当输入数据量小于这个值时采用local  MapReduce的方式，默认为134217728，即128M  </p>
<pre><code>set hive.exec.mode.local.auto.inputbytes.max=50000000;
</code></pre>
<p>–设置local MapReduce的最大输入文件个数，当输入文件个数小于这个值时采用local MapReduce的方式，默认为4  </p>
<pre><code>set hive.exec.mode.local.auto.input.files.max=10;
</code></pre>
<h4 id="6-并行执行"><a href="#6-并行执行" class="headerlink" title="6.并行执行"></a>6.并行执行</h4><p>Hive会将一个SQL语句转化成一个或者多个Stage，每个Stage对应一个MR Job。默认情况下，Hive同时只会执行一个Stage。但是某SQL语句可能会包含多个Stage，但这多个Stage可能并非完全互相依赖，也就是说有些Stage是可以并行执行的。此处提到的并行执行就是指这些Stage的并行执行   </p>
<p>–启用并行执行优化  </p>
<pre><code>set hive.exec.parallel=true;       
</code></pre>
<p>–同一个sql允许最大并行度，默认为8  </p>
<pre><code>set hive.exec.parallel.thread.number=8;   
</code></pre>
<h4 id="7-严格模式"><a href="#7-严格模式" class="headerlink" title="7.严格模式"></a>7.严格模式</h4><p>Hive可以通过设置某些参数防止危险操作：   </p>
<p>1）分区表不使用分区过滤  </p>
<p>将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行  </p>
<p>2）使用order by没有limit过滤   </p>
<p>将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句    </p>
<p>3）笛卡尔积    </p>
<p>将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/31/hive-udf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/31/hive-udf/" class="post-title-link" itemprop="url">hive-udf</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-31 11:40:34" itemprop="dateCreated datePublished" datetime="2023-07-31T11:40:34+08:00">2023-07-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:03:22" itemprop="dateModified" datetime="2023-08-11T13:03:22+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="看似寻常最奇崛，成如容易却艰辛"><a href="#看似寻常最奇崛，成如容易却艰辛" class="headerlink" title="看似寻常最奇崛，成如容易却艰辛"></a>看似寻常最奇崛，成如容易却艰辛</h1><p><img src="/2023/07/31/hive-udf/2.png"></p>
<h1 id="Hive自定义UDF函数案例"><a href="#Hive自定义UDF函数案例" class="headerlink" title="Hive自定义UDF函数案例"></a>Hive自定义UDF函数案例</h1><h2 id="0）需求"><a href="#0）需求" class="headerlink" title="0）需求"></a>0）需求</h2><p>自定义一个UDF实现计算给定基本数据类型的长度，例如：  </p>
<pre><code>hive(default)&gt; select my_len(&quot;abcd&quot;);  
4  
</code></pre>
<h2 id="1）创建一个Maven工程Hive"><a href="#1）创建一个Maven工程Hive" class="headerlink" title="1）创建一个Maven工程Hive"></a>1）创建一个Maven工程Hive</h2><h2 id="2）导入依赖"><a href="#2）导入依赖" class="headerlink" title="2）导入依赖"></a>2）导入依赖</h2><pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
        &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
        &lt;version&gt;3.1.3&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;  
</code></pre>
<h2 id="3）创建一个类"><a href="#3）创建一个类" class="headerlink" title="3）创建一个类"></a>3）创建一个类</h2><pre><code>package com.atguigu.hive.udf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

/* 我们需计算一个要给定基本数据类型的长度 */	
public class MyUDF extends GenericUDF &#123;
    /*** 判断传进来的参数的类型和长度* 约定返回的数据类型*/  
    @Override
    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123;

    if (arguments.length !=1) &#123;
        throw  new UDFArgumentLengthException(&quot;please give me  only one arg&quot;);
    &#125;

    if (!arguments[0].getCategory().equals(ObjectInspector.Category.PRIMITIVE))&#123;
        throw  new UDFArgumentTypeException(1, &quot;i need primitive type arg&quot;);
    &#125;

    return PrimitiveObjectInspectorFactory.javaIntObjectInspector;
&#125;

/**
 * 解决具体逻辑的
 */
@Override
public Object evaluate(DeferredObject[] arguments) throws HiveException &#123;

    Object o = arguments[0].get();
    if(o==null)&#123;
        return 0;
    &#125;

    return o.toString().length();
&#125;

@Override
// 用于获取解释的字符串
public String getDisplayString(String[] children) &#123;
    return &quot;&quot;;
&#125;
&#125;
</code></pre>
<h2 id="4）创建临时函数"><a href="#4）创建临时函数" class="headerlink" title="4）创建临时函数"></a>4）创建临时函数</h2><h3 id="（1）打成jar包上传到服务器-opt-module-hive-datas-myudf-jar"><a href="#（1）打成jar包上传到服务器-opt-module-hive-datas-myudf-jar" class="headerlink" title="（1）打成jar包上传到服务器&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;myudf.jar"></a>（1）打成jar包上传到服务器&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;myudf.jar</h3><h3 id="（2）将jar包添加到hive的classpath，临时生效"><a href="#（2）将jar包添加到hive的classpath，临时生效" class="headerlink" title="（2）将jar包添加到hive的classpath，临时生效"></a>（2）将jar包添加到hive的classpath，临时生效</h3><pre><code>hive (default)&gt; add jar /opt/module/hive/datas/myudf.jar;
</code></pre>
<h3 id="（3）创建临时函数与开发好的java-class关联"><a href="#（3）创建临时函数与开发好的java-class关联" class="headerlink" title="（3）创建临时函数与开发好的java class关联"></a>（3）创建临时函数与开发好的java class关联</h3><pre><code>hive (default)&gt; 
create temporary function my_len 
as &quot;com.atguigu.hive.udf.MyUDF&quot;;
</code></pre>
<h3 id="（4）即可在hql中使用自定义的临时函数"><a href="#（4）即可在hql中使用自定义的临时函数" class="headerlink" title="（4）即可在hql中使用自定义的临时函数"></a>（4）即可在hql中使用自定义的临时函数</h3><pre><code>hive (default)&gt; 
select 
ename,
my_len(ename) ename_len 
from emp;
</code></pre>
<h3 id="（5）删除临时函数"><a href="#（5）删除临时函数" class="headerlink" title="（5）删除临时函数"></a>（5）删除临时函数</h3><pre><code>hive (default)&gt; drop temporary function my_len;
注意：临时函数只跟会话有关系，跟库没有关系。只要创建临时函数的会话不断，在当前会话下，任意一个库都可以使用，其他会话全都不能使用。
</code></pre>
<h2 id="5）创建永久函数"><a href="#5）创建永久函数" class="headerlink" title="5）创建永久函数"></a>5）创建永久函数</h2><h3 id="（1）创建永久函数"><a href="#（1）创建永久函数" class="headerlink" title="（1）创建永久函数"></a>（1）创建永久函数</h3><p>注意：因为add jar本身也是临时生效，所以在创建永久函数的时候，需要制定路径（并且因为元数据的原因，这个路径还得是HDFS上的路径）</p>
<pre><code>hive (default)&gt; 
create function my_len2 
as &quot;com.atguigu.hive.udf.MyUDF&quot; 
using jar &quot;hdfs://hadoop102:8020/udf/myudf.jar&quot;;
</code></pre>
<h3 id="（2）即可在hql中使用自定义的永久函数"><a href="#（2）即可在hql中使用自定义的永久函数" class="headerlink" title="（2）即可在hql中使用自定义的永久函数"></a>（2）即可在hql中使用自定义的永久函数</h3><pre><code>hive (default)&gt; 
select 
ename,
my_len2(ename) ename_len 
from emp;
</code></pre>
<h3 id="（3）删除永久函数"><a href="#（3）删除永久函数" class="headerlink" title="（3）删除永久函数"></a>（3）删除永久函数</h3><pre><code>hive (default)&gt; drop function my_len2;  
</code></pre>
<p>注意：永久函数跟会话没有关系，创建函数的会话断了以后，其他会话也可以使用。  </p>
<p>永久函数创建的时候，在函数名之前需要自己加上库名，如果不指定库名的话，会默认把当前库的库名给加上。  </p>
<p>永久函数使用的时候，需要在指定的库里面操作，或者在其他库里面使用的话加上，库名.函数名。  </p>
<p>#啊席八，Hive自定义UDF函数你都学会了~！</p>
<p>#啪啪啪~ 你很厉害喔~</p>
<p><img src="/2023/07/31/hive-udf/1.png" alt="歪嘴猫"></p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/hive_learn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/hive_learn/" class="post-title-link" itemprop="url">hive学习笔记</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-30 18:56:09" itemprop="dateCreated datePublished" datetime="2023-07-30T18:56:09+08:00">2023-07-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:08:10" itemprop="dateModified" datetime="2023-08-11T13:08:10+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1g84y147sX?p=78&vd_source=326368ccf929b51406b17a280e53c102">尚硅谷大数据Hive 3.x教程全新升级版（基于hive3.1.3）</a></p>
<h1 id="手握日月摘星辰，世间无我这般人！"><a href="#手握日月摘星辰，世间无我这般人！" class="headerlink" title="手握日月摘星辰，世间无我这般人！"></a>手握日月摘星辰，世间无我这般人！</h1><p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1vdjJdb5hZtWMDK6hoH1R5g">https://pan.baidu.com/s/1vdjJdb5hZtWMDK6hoH1R5g</a><br>提取码：uce2   </p>
<p><img src="/2023/07/30/hive_learn/8.png">  </p>
<h1 id="一：Hive的基础知识"><a href="#一：Hive的基础知识" class="headerlink" title="一：Hive的基础知识"></a>一：Hive的基础知识</h1><h2 id="1-什么是Hive？"><a href="#1-什么是Hive？" class="headerlink" title="1.什么是Hive？"></a>1.什么是Hive？</h2><p>Hive是由Facebook开源，基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能</p>
<h2 id="2-Hive本质"><a href="#2-Hive本质" class="headerlink" title="2.Hive本质"></a>2.Hive本质</h2><p>Hive是一个Hadoop客户端，用于将HQL（Hive SQL）转化成MapReduce程序。<br>（1）Hive中每张表的数据存储在HDFS<br>（2）Hive分析数据底层的实现是MapReduce（也可配置为Spark或者Tez）<br>（3）执行程序运行在Yarn上</p>
<h2 id="3-用户接口：Client"><a href="#3-用户接口：Client" class="headerlink" title="3.用户接口：Client"></a>3.用户接口：Client</h2><p>CLI（command-line interface）、JDBC&#x2F;ODBC  </p>
<h3 id="JDBC和ODBC的区别"><a href="#JDBC和ODBC的区别" class="headerlink" title="JDBC和ODBC的区别:"></a>JDBC和ODBC的区别:</h3><p>（1）JDBC的移植性比ODBC好 </p>
<p>（2）两者使用的语言不同，JDBC在Java编程时使用，ODBC一般在C&#x2F;C++编程时使用  </p>
<h2 id="4-元数据：Metastore"><a href="#4-元数据：Metastore" class="headerlink" title="4:元数据：Metastore"></a>4:元数据：Metastore</h2><p>元数据包括：数据库（默认是default）、表名、表的拥有者、列&#x2F;分区字段、表的类型（是否是外部表）、表的数据所在目录等  </p>
<p>默认存储在自带的derby数据库中，由于derby数据库只支持单客户端访问，生产环境中为了多人开发，推荐使用MySQL存储Metastore  </p>
<h2 id="5-hive的存储和计算"><a href="#5-hive的存储和计算" class="headerlink" title="5.hive的存储和计算"></a>5.hive的存储和计算</h2><p>使用HDFS进行存储，可以选择MapReduce&#x2F;Tez&#x2F;Spark进行计算</p>
<h2 id="6-hiveserver2服务"><a href="#6-hiveserver2服务" class="headerlink" title="6.hiveserver2服务"></a>6.hiveserver2服务</h2><p>Hive的hiveserver2服务的作用是提供jdbc&#x2F;odbc接口，为用户提供远程访问Hive数据的功能，例如用户期望在个人电脑中访问远程服务中的Hive数据，就需要用到Hiveserver2  </p>
<h2 id="7-用户说明"><a href="#7-用户说明" class="headerlink" title="7.用户说明"></a>7.用户说明</h2><p>在远程访问Hive数据时，客户端并未直接访问Hadoop集群，而是由Hivesever2代理访问,那么访问Hadoop集群的用户身份是谁？  </p>
<p>具体是谁，由Hiveserver2的hive.server2.enable.doAs参数决定，该参数的含义是是否启用Hiveserver2用户模拟的功能  </p>
<p>若启用，则Hiveserver2会模拟成客户端的登录用户去访问Hadoop集群的数据，不启用，则Hivesever2会直接使用启动用户访问Hadoop集群数据  </p>
<p>默认为开启</p>
<p>生产环境，推荐开启用户模拟功能，因为开启后才能保证各用户之间的权限隔离</p>
<p>hivesever2的模拟用户功能，依赖于Hadoop提供的proxy user（代理用户功能），只有Hadoop中的代理用户才能模拟其他用户的身份访问Hadoop集群。因此，需要将hiveserver2的启动用户设置为Hadoop的代理用户 </p>
<h2 id="8-metastore服务"><a href="#8-metastore服务" class="headerlink" title="8.metastore服务"></a>8.metastore服务</h2><p>Hive的metastore服务的作用是为Hive CLI或者Hiveserver2提供元数据访问接口  </p>
<h2 id="9-metastore运行模式"><a href="#9-metastore运行模式" class="headerlink" title="9.metastore运行模式"></a>9.metastore运行模式</h2><p>metastore有两种运行模式，分别为嵌入式模式和独立服务模式</p>
<p><img src="/2023/07/30/hive_learn/1.png" alt="&quot;metastore运行模式&quot;"></p>
<p>生产环境中，不推荐使用嵌入式模式  </p>
<p>（1）嵌入式模式下，每个Hive CLI都需要直接连接元数据库，当Hive CLI较多时，数据库压力会比较大。  </p>
<p>（2）每个客户端都需要用户元数据库的读写权限，元数据库的安全得不到很好的保证</p>
<h2 id="10-编写Hive服务启动脚本"><a href="#10-编写Hive服务启动脚本" class="headerlink" title="10.编写Hive服务启动脚本"></a>10.编写Hive服务启动脚本</h2><p>nohup：放在命令开头，表示不挂起，也就是关闭终端进程也继续保持运行状态  </p>
<p>&#x2F;dev&#x2F;null：是Linux文件系统中的一个文件，被称为黑洞，所有写入该文件的内容都会被自动丢弃  </p>
<p>2&gt;&amp;1：表示将错误重定向到标准输出上  </p>
<p>&amp;：放在命令结尾，表示后台运行  </p>
<p>一般会组合使用：nohup  [xxx命令操作]&gt; file  2&gt;&amp;1 &amp;，表示将xxx命令运行的结果输出到file中，并保持命令启动的进程在后台运行。  </p>
<h2 id="11-hive-e-和-hive-f"><a href="#11-hive-e-和-hive-f" class="headerlink" title="11.hive -e 和 hive -f"></a>11.hive -e 和 hive -f</h2><p>“-e”不进入hive的交互窗口执行hql语句  </p>
<p><code>bin/hive -e &quot;select id from student;&quot;</code></p>
<p>“-f”执行脚本中的hql语句  </p>
<p><code>bin/hive -f /opt/module/hive/datas/hivef.sql</code>  </p>
<h2 id="12-Hive参数配置方式"><a href="#12-Hive参数配置方式" class="headerlink" title="12.Hive参数配置方式"></a>12.Hive参数配置方式</h2><p>参数的配置三种方式  </p>
<h3 id="1-配置文件方式"><a href="#1-配置文件方式" class="headerlink" title="(1).配置文件方式"></a>(1).配置文件方式</h3><p><code>hive-site.xml</code></p>
<h3 id="2-命令行参数方式"><a href="#2-命令行参数方式" class="headerlink" title="(2).命令行参数方式"></a>(2).命令行参数方式</h3><p>启动Hive时，可以在命令行添加-hiveconf param&#x3D;value来设定参数  </p>
<p>比如：bin&#x2F;hive -hiveconf   </p>
<p><code>mapreduce.job.reduces=10;</code></p>
<p>注：仅对本次Hive启动有效  </p>
<h3 id="3-参数声明方式"><a href="#3-参数声明方式" class="headerlink" title="(3).参数声明方式"></a>(3).参数声明方式</h3><p>可以在HQL中使用SET关键字设定参数  </p>
<p><code>set mapreduce.job.reduces=10;</code>  </p>
<p>上述三种设定方式的优先级依次递增,配置文件 &lt; 命令行参数 &lt; 参数声明  </p>
<h2 id="13-Hive常见属性配置"><a href="#13-Hive常见属性配置" class="headerlink" title="13.Hive常见属性配置"></a>13.Hive常见属性配置</h2><p>Hive客户端显示当前库和表头<br>Set Hive-site.xml :<br>          <code>hive.cli.print.header = true</code><br>          <code>hive.cli.print.current.db</code>  </p>
<p>Hive运行日志路径配置<br>Set hive-log4j2.properties:<br>     <code>property.hive.log.dir=/opt/module/hive/logs</code>  </p>
<p>修改Hive的堆内存<br>Set hive-env.sh:<br>     <code>export HADOOP_HEAPSIZE=2048</code>  </p>
<p>关闭Hadoop虚拟内存检查<br>Set yarn-site.xml:<br>     <code>yarn.nodemanager.vmem-check-enabled =false</code>  </p>
<h1 id="二：Hive的DDL语法"><a href="#二：Hive的DDL语法" class="headerlink" title="二：Hive的DDL语法"></a>二：Hive的DDL语法</h1><h2 id="1-创建数据库"><a href="#1-创建数据库" class="headerlink" title="1.创建数据库"></a>1.创建数据库</h2><pre><code>CREATE DATABASE [IF NOT EXISTS] database_name  
[COMMENT database_comment]  
[LOCATION hdfs_path]  
[WITH DBPROPERTIES (property_name=property_value, ...)];   
</code></pre>
<p>创建一个数据库，指定路径<br>    hive (default)&gt; create database db_hive2 location ‘&#x2F;db_hive2’;  </p>
<h2 id="2-查看数据库信息"><a href="#2-查看数据库信息" class="headerlink" title="2.查看数据库信息"></a>2.查看数据库信息</h2><pre><code>DESCRIBE DATABASE [EXTENDED] db_name;    
</code></pre>
<h3 id="1-查看基本信息"><a href="#1-查看基本信息" class="headerlink" title="(1) 查看基本信息"></a>(1) 查看基本信息</h3><pre><code>desc database db_hive3;  
</code></pre>
<h3 id="2-查看更多信息"><a href="#2-查看更多信息" class="headerlink" title="(2) 查看更多信息"></a>(2) 查看更多信息</h3><pre><code>desc database extended db_hive3;  
</code></pre>
<h2 id="3-修改数据库"><a href="#3-修改数据库" class="headerlink" title="3.修改数据库"></a>3.修改数据库</h2><p>需要注意的是：修改数据库location，不会改变当前已有表的路径信息，而只是改变后续创建的新表的默认的父目录  </p>
<h3 id="修改dbproperties"><a href="#修改dbproperties" class="headerlink" title="修改dbproperties"></a>修改dbproperties</h3><pre><code>ALTER DATABASE database_name SET DBPROPERTIES   (property_name=property_value, ...);  
</code></pre>
<h3 id="修改location"><a href="#修改location" class="headerlink" title="修改location"></a>修改location</h3><pre><code>ALTER DATABASE database_name SET LOCATION hdfs_path;  
</code></pre>
<h3 id="修改owner-user"><a href="#修改owner-user" class="headerlink" title="修改owner user"></a>修改owner user</h3><pre><code>ALTER DATABASE database_name SET OWNER USER user_name;  
</code></pre>
<h2 id="4-删除数据库"><a href="#4-删除数据库" class="headerlink" title="4.删除数据库"></a>4.删除数据库</h2><pre><code>DROP DATABASE [IF EXISTS] database_name [RESTRICT|CASCADE];    
</code></pre>
<p>RESTRICT：严格模式，若数据库不为空，则会删除失败，默认为该模式。  </p>
<p>CASCADE：级联模式，若数据库不为空，则会将库中的表一并删除。</p>
<h2 id="5-切换当前数据库"><a href="#5-切换当前数据库" class="headerlink" title="5.切换当前数据库"></a>5.切换当前数据库</h2><p>USE database_name;  </p>
<h2 id="6-创建表"><a href="#6-创建表" class="headerlink" title="6.创建表"></a>6.创建表</h2><pre><code>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name   
[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
[CLUSTERED BY (col_name, col_name, ...) 
[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
[ROW FORMAT row_format] 
[STORED AS file_format]
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
</code></pre>
<p>关键字说明:  </p>
<h3 id="1-TEMPORARY"><a href="#1-TEMPORARY" class="headerlink" title="(1)TEMPORARY"></a>(1)TEMPORARY</h3><p>临时表，该表只在当前会话可见，会话结束，表会被删除。  </p>
<h3 id="2-EXTERNAL（重点）"><a href="#2-EXTERNAL（重点）" class="headerlink" title="(2)EXTERNAL（重点）"></a>(2)EXTERNAL（重点）</h3><p>外部表，与之相对应的是内部表（管理表）。内部表意味着Hive会完全接管该表，包括元数据和HDFS中的数据。而外部表则意味着Hive只接管元数据，而不完全接管HDFS中的数据</p>
<h3 id="3-data-type（重点）"><a href="#3-data-type（重点）" class="headerlink" title="(3)data_type（重点）"></a>(3)data_type（重点）</h3><p>Hive中的字段类型可分为基本数据类型和复杂数据类型。  </p>
<p><img src="/2023/07/30/hive_learn/2.png" alt="&quot;hive数据类型&quot;">  </p>
<p>类型转换:  </p>
<p>Hive的基本数据类型可以做类型转换，转换的方式包括隐式转换以及显示转换。  </p>
<h4 id="方式一：隐式转换"><a href="#方式一：隐式转换" class="headerlink" title="方式一：隐式转换"></a>方式一：隐式转换</h4><p>隐式地转换为一个范围更广的类型   </p>
<p>Hive官方隐式转换表<br><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/hive/languagemanual+types#LanguageManualTypes-AllowedImplicitConversions">Allowed Implicit Conversions</a>  </p>
<h4 id="方式二：显示转换"><a href="#方式二：显示转换" class="headerlink" title="方式二：显示转换"></a>方式二：显示转换</h4><p>可以借助cast函数完成显示的类型转换(强制转换)  </p>
<pre><code>select &#39;1&#39; + 2, cast(&#39;1&#39; as int) + 2;
</code></pre>
<h3 id="4-PARTITIONED-BY（重点）"><a href="#4-PARTITIONED-BY（重点）" class="headerlink" title="(4) PARTITIONED BY（重点）"></a>(4) PARTITIONED BY（重点）</h3><p>创建分区表  </p>
<h3 id="5-CLUSTERED-BY-…-SORTED-BY…INTO-…-BUCKETS（重点）"><a href="#5-CLUSTERED-BY-…-SORTED-BY…INTO-…-BUCKETS（重点）" class="headerlink" title="(5) CLUSTERED BY … SORTED BY…INTO … BUCKETS（重点）"></a>(5) CLUSTERED BY … SORTED BY…INTO … BUCKETS（重点）</h3><p>创建分桶表  </p>
<h3 id="6-ROW-FORMAT（重点）"><a href="#6-ROW-FORMAT（重点）" class="headerlink" title="(6) ROW FORMAT（重点）"></a>(6) ROW FORMAT（重点）</h3><p>指定SERDE，SERDE是Serializer and Deserializer的简写。Hive使用SERDE序列化和反序列化每行数据  </p>
<p>Hive官方序列化反序列化器文档<br><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-HiveSerDe">Hive-Serde</a></p>
<h4 id="语法一："><a href="#语法一：" class="headerlink" title="语法一："></a>语法一：</h4><p>DELIMITED关键字表示对文件中的每个字段按照特定分割符进行分割，其会使用默认的SERDE对每行数据进行序列化和反序列化  </p>
<pre><code>ROW FORAMT DELIMITED 
[FIELDS TERMINATED BY char] 
[COLLECTION ITEMS TERMINATED BY char] 
[MAP KEYS TERMINATED BY char] 
[LINES TERMINATED BY char] 
[NULL DEFINED AS char]   
</code></pre>
<p>注：<br>fields terminated by ：列分隔符<br>collection items terminated by ： map、struct和array中每个元素之间的分隔符<br>map keys terminated by ：map中的key与value的分隔符<br>lines terminated by ：行分隔符  </p>
<h4 id="语法二："><a href="#语法二：" class="headerlink" title="语法二："></a>语法二：</h4><p>SERDE关键字可用于指定其他内置的SERDE或者用户自定义的SERDE。例如JSON SERDE，可用于处理JSON字符串  </p>
<pre><code>ROW FORMAT SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,property_name=property_value, ...)] 
</code></pre>
<h3 id="7-STORED-AS（重点）"><a href="#7-STORED-AS（重点）" class="headerlink" title="(7) STORED AS（重点）"></a>(7) STORED AS（重点）</h3><p>指定文件格式，常用的文件格式有，textfile（默认值），sequence file，orc file、parquet file等等  </p>
<h3 id="8-LOCATION"><a href="#8-LOCATION" class="headerlink" title="(8) LOCATION"></a>(8) LOCATION</h3><p>指定表所对应的HDFS路径，若不指定路径，其默认值为<br>${hive.metastore.warehouse.dir}&#x2F;db_name.db&#x2F;table_name  </p>
<h3 id="9-TBLPROPERTIES"><a href="#9-TBLPROPERTIES" class="headerlink" title="(9) TBLPROPERTIES"></a>(9) TBLPROPERTIES</h3><p>用于配置表的一些KV键值对参数  </p>
<h2 id="7-Create-Table-As-Select（CTAS）建表"><a href="#7-Create-Table-As-Select（CTAS）建表" class="headerlink" title="7.Create Table As Select（CTAS）建表"></a>7.Create Table As Select（CTAS）建表</h2><pre><code>CREATE [TEMPORARY] TABLE [IF NOT EXISTS] table_name 
[COMMENT table_comment] 
[ROW FORMAT row_format] 
[STORED AS file_format] 
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
[AS select_statement]
</code></pre>
<h2 id="8-Create-Table-Like语法"><a href="#8-Create-Table-Like语法" class="headerlink" title="8.Create Table Like语法"></a>8.Create Table Like语法</h2><p>该语法允许用户复刻一张已经存在的表结构，与上述的CTAS语法不同，该语法创建出来的表中不包含数据  </p>
<pre><code>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
[LIKE exist_table_name]
[ROW FORMAT row_format] 
[STORED AS file_format] 
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
</code></pre>
<h2 id="9-内部表与外部表"><a href="#9-内部表与外部表" class="headerlink" title="9.内部表与外部表"></a>9.内部表与外部表</h2><p>Hive中默认创建的表都是的内部表，有时也被称为管理表。对于内部表，Hive会完全管理表的元数据和数据文件  </p>
<p>外部表通常可用于处理其他工具上传的数据文件，对于外部表，Hive只负责管理元数据，不负责管理HDFS中的数据文件  </p>
<h2 id="10-查看表"><a href="#10-查看表" class="headerlink" title="10.查看表"></a>10.查看表</h2><pre><code>DESCRIBE [EXTENDED | FORMATTED] [db_name.]table_name  
</code></pre>
<p>EXTENDED：展示详细信息  </p>
<p>FORMATTED：对详细信息进行格式化的展示</p>
<h2 id="11-修改列信息"><a href="#11-修改列信息" class="headerlink" title="11.修改列信息"></a>11.修改列信息</h2><h3 id="增加列"><a href="#增加列" class="headerlink" title="增加列"></a>增加列</h3><pre><code>ALTER TABLE table_name ADD COLUMNS (col_name data_type [COMMENT col_comment], ...)  
</code></pre>
<p>新增列的位置位于末尾  </p>
<h3 id="更新列"><a href="#更新列" class="headerlink" title="更新列"></a>更新列</h3><pre><code>ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]
</code></pre>
<p>该语句允许用户修改指定列的列名、数据类型、注释信息以及在表中的位置  </p>
<h3 id="替换列"><a href="#替换列" class="headerlink" title="替换列"></a>替换列</h3><pre><code>ALTER TABLE table_name REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)  
</code></pre>
<p>该语句允许用户用新的列集替换表中原有的全部列  </p>
<h2 id="12-清空表"><a href="#12-清空表" class="headerlink" title="12.清空表"></a>12.清空表</h2><pre><code>TRUNCATE [TABLE] table_name  
</code></pre>
<p>truncate只能清空管理表，不能删除外部表中数据  </p>
<h1 id="三：Hive的DML语法"><a href="#三：Hive的DML语法" class="headerlink" title="三：Hive的DML语法"></a>三：Hive的DML语法</h1><h2 id="1-Load"><a href="#1-Load" class="headerlink" title="1. Load"></a>1. Load</h2><p>Load语句可将文件导入到Hive表中    </p>
<pre><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)];
</code></pre>
<p>关键字说明：  </p>
<p>（1）local：表示从本地加载数据到Hive表；否则从HDFS加载数据到Hive表。  </p>
<p>（2）overwrite：表示覆盖表中已有数据，否则表示追加。  </p>
<p>（3）partition：表示上传到指定分区，若目标是分区表，需指定分区    </p>
<h3 id="1-1加载本地文件到hive"><a href="#1-1加载本地文件到hive" class="headerlink" title="1.1加载本地文件到hive:"></a>1.1加载本地文件到hive:</h3><pre><code>load data local inpath &#39;/opt/module/datas/student.txt&#39; into table student;  
</code></pre>
<h3 id="1-2加载HDFS上数据"><a href="#1-2加载HDFS上数据" class="headerlink" title="1.2加载HDFS上数据:"></a>1.2加载HDFS上数据:</h3><pre><code>hadoop fs -put /opt/module/datas/student.txt /user/atguigu;  

load data inpath &#39;/user/atguigu/student.txt&#39; into table student;
</code></pre>
<h2 id="2-Insert"><a href="#2-Insert" class="headerlink" title="2.Insert"></a>2.Insert</h2><pre><code>INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement;
</code></pre>
<p>关键字说明：  </p>
<p>（1）INTO：将结果追加到目标表  </p>
<p>（2）OVERWRITE：用结果覆盖原有数据  </p>
<h3 id="2-1-将给定Values插入表中"><a href="#2-1-将给定Values插入表中" class="headerlink" title="2.1 将给定Values插入表中"></a>2.1 将给定Values插入表中</h3><pre><code>INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...]  
</code></pre>
<h3 id="2-2将查询结果写入目标路径"><a href="#2-2将查询结果写入目标路径" class="headerlink" title="2.2将查询结果写入目标路径"></a>2.2将查询结果写入目标路径</h3><pre><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory[ROW FORMAT row_format] [STORED AS file_format] select_statement;
</code></pre>
<h2 id="3-Export-Import"><a href="#3-Export-Import" class="headerlink" title="3.Export&amp;Import"></a>3.Export&amp;Import</h2><p>Export导出语句可将表的数据和元数据信息一并到处的HDFS路径，Import可将Export导出的内容导入Hive，表的数据和元数据信息都会恢复。Export和Import可用于两个Hive实例之间的数据迁移</p>
<h3 id="导出"><a href="#导出" class="headerlink" title="导出"></a>导出</h3><pre><code>EXPORT TABLE tablename TO &#39;export_target_path&#39;
</code></pre>
<h3 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h3><pre><code>IMPORT [EXTERNAL] TABLE new_or_original_tablename FROM &#39;source_path&#39; [LOCATION &#39;import_target_path&#39;]
</code></pre>
<h1 id="四-查询"><a href="#四-查询" class="headerlink" title="四:查询"></a>四:查询</h1><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">Hive-select语法文档</a></p>
<h2 id="1-select-查询语法"><a href="#1-select-查询语法" class="headerlink" title="1.select 查询语法"></a>1.select 查询语法</h2><pre><code>SELECT [ALL | DISTINCT] select_expr, select_expr, 
FROM table_reference       -- 从什么表查
[WHERE where_condition]   -- 过滤
[GROUP BY col_list]        -- 分组查询
[HAVING col_list]          -- 分组后过滤
[ORDER BY col_list]        -- 排序
[CLUSTER BY col_list
| [DISTRIBUTE BY col_list] [SORT BY col_list]]
[LIMIT number]                -- 限制输出的行数
</code></pre>
<p>注意：<br>（1）SQL 语言大小写不敏感。<br>（2）SQL 可以写在一行或者多行。<br>（3）关键字不能被缩写也不能分行。<br>（4）各子句一般要分行写。<br>（5）使用缩进提高语句的可读性    </p>
<h2 id="2-Limit语句"><a href="#2-Limit语句" class="headerlink" title="2.Limit语句"></a>2.Limit语句</h2><p>典型的查询会返回多行数据。limit子句用于限制返回的行数  </p>
<pre><code>select * from emp limit 2,3; -- 表示从第2行开始，向下抓取3行
</code></pre>
<h2 id="3-关系运算函数"><a href="#3-关系运算函数" class="headerlink" title="3.关系运算函数"></a>3.关系运算函数</h2><p>A&lt;&#x3D;&gt;B :  </p>
<p> 如果A和B都为null或者都不为null，则返回true，如果只有一边为null，返回false  </p>
<p>A [not] like B :  </p>
<p> B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回true；反之返回false。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母‘x’结尾，而‘%x%’表示A包含有字母‘x’,可以位于开头，结尾或者字符串中间。如果使用not关键字则可达到相反的效果。  </p>
<p>A rlike B, A regexp B:  </p>
<p>B是基于java的正则表达式，如果A与其匹配，则返回true；反之返回false。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。  </p>
<h2 id="4-聚合函数"><a href="#4-聚合函数" class="headerlink" title="4.聚合函数"></a>4.聚合函数</h2><p>count(*)，表示统计所有行数，包含null值； </p>
<p>count(某列)，表示该列一共有多少行，不包含null值； </p>
<p>max()，求最大值，不包含null，除非所有值都是null； </p>
<p>min()，求最小值，不包含null，除非所有值都是null； </p>
<p>sum()，求和，不包含null。   </p>
<p>avg()，求平均值，不包含null。    </p>
<h2 id="5-分组"><a href="#5-分组" class="headerlink" title="5.分组"></a>5.分组</h2><p>Group By语句：  </p>
<p>Group By语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</p>
<h2 id="6-Having语句"><a href="#6-Having语句" class="headerlink" title="6.Having语句"></a>6.Having语句</h2><p>having与where不同点  </p>
<p>（1）where后面不能写分组聚合函数，而having后面可以使用分组聚合函数。  </p>
<p>（2）having只用于group by分组统计语句    </p>
<h2 id="7-Join语句"><a href="#7-Join语句" class="headerlink" title="7.Join语句"></a>7.Join语句</h2><p>Hive支持通常的sql join语句，但是只支持等值连接，不支持非等值连接。  </p>
<p>inner join 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来  </p>
<p>left outer join 左外连接：join操作符左边表中符合where子句的所有记录将会被返回  </p>
<p>right outer join 右外连接：join操作符右边表中符合where子句的所有记录将会被返回  </p>
<p>full outer join : 满外连接：将会返回所有表中符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代    </p>
<p>多表连接: 连接n个表，至少需要n-1个连接条件    </p>
<p>大多数情况下，Hive会对每对join连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作</p>
<p>因为Hive总是按照从左到右的顺序执行的  </p>
<h2 id="8-笛卡尔积产生条件"><a href="#8-笛卡尔积产生条件" class="headerlink" title="8.笛卡尔积产生条件"></a>8.笛卡尔积产生条件</h2><p>（1）省略连接条件  </p>
<p>（2）连接条件无效  </p>
<p>（3）所有表中的所有行互相连接  </p>
<h2 id="9-联合（union-union-all）"><a href="#9-联合（union-union-all）" class="headerlink" title="9.联合（union &amp; union all）"></a>9.联合（union &amp; union all）</h2><p>union和union all都是上下拼接sql的结果，这点是和join有区别的，join是左右关联，union和union all是上下拼接。  </p>
<p>union去重，union all不去重  </p>
<p>union和union all在上下拼接sql结果时有两个要求：  </p>
<p>（1）两个sql的结果，列的个数必须相同  </p>
<p>（2）两个sql的结果，上下所对应列的类型必须一致    </p>
<h2 id="10-排序"><a href="#10-排序" class="headerlink" title="10.排序"></a>10.排序</h2><h4 id="全局排序-Order-By"><a href="#全局排序-Order-By" class="headerlink" title="全局排序 Order By"></a>全局排序 Order By</h4><p>全局排序，只有一个Reduce    </p>
<p>asc（ascend）：升序（默认）  </p>
<p>desc（descend）：降序  </p>
<h4 id="每个Reduce内部排序（Sort-By）"><a href="#每个Reduce内部排序（Sort-By）" class="headerlink" title="每个Reduce内部排序（Sort By）"></a>每个Reduce内部排序（Sort By）</h4><p>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用Sort by  </p>
<p>Sort by为每个reduce产生一个排序文件。每个Reduce内部进行排序，对全局结果集来说不是排序  </p>
<p>设置reduce个数  </p>
<pre><code>hive (default)&gt; set mapreduce.job.reduces=3;  
</code></pre>
<p>查看设置reduce个数  </p>
<pre><code>hive (default)&gt; set mapreduce.job.reduces;  
</code></pre>
<h2 id="11-分区（Distribute-By）"><a href="#11-分区（Distribute-By）" class="headerlink" title="11.分区（Distribute By）"></a>11.分区（Distribute By）</h2><p>Distribute By：在有些情况下，我们需要控制某个特定行应该到哪个Reducer，通常是为了进行后续的聚集操作。distribute by子句可以做这件事。distribute by类似MapReduce中partition（自定义分区），进行分区，结合sort by使用  </p>
<p>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行相除后，余数相同的分到一个区。  </p>
<p>Hive要求distribute by语句要写在sort by语句之前。  </p>
<p>演示完以后mapreduce.job.reduces的值要设置回-1，否则下面分区or分桶表load跑MapReduce的时候会报错  </p>
<h2 id="12-分区排序（Cluster-By）"><a href="#12-分区排序（Cluster-By）" class="headerlink" title="12.分区排序（Cluster By）"></a>12.分区排序（Cluster By）</h2><p>当distribute by和sort by字段相同时，可以使用cluster by方式  </p>
<p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为asc或者desc      </p>
<p>以下两种写法等价<br>hive (default)&gt;<br>    select *<br>    from emp<br>    cluster by deptno;  </p>
<p>hive (default)&gt;<br>    select<br>    *<br>    from emp<br>    distribute by deptno sort by deptno asc;  </p>
<h1 id="五-函数"><a href="#五-函数" class="headerlink" title="五:函数"></a>五:函数</h1><p>Hive会将常用的逻辑封装成函数给用户进行使用，类似于Java中的函数    </p>
<p>Hive提供了大量的内置函数，按照其特点可大致分为如下几类：单行函数、聚合函数、炸裂函数、窗口函数  </p>
<p>查看系统内置函数</p>
<pre><code>show functions;  
</code></pre>
<p>查看内置函数用法  </p>
<pre><code>desc function upper;  
</code></pre>
<p>查看内置函数详细信息  </p>
<pre><code>desc function extended upper;  
</code></pre>
<h2 id="1-单行函数"><a href="#1-单行函数" class="headerlink" title="1.单行函数"></a>1.单行函数</h2><p>单行函数的特点是一进一出，即输入一行，输出一行</p>
<p>单行函数按照功能可分为如下几类: 日期函数、字符串函数、集合函数、数学函数、流程控制函数等  </p>
<h3 id="数值函数"><a href="#数值函数" class="headerlink" title="数值函数"></a>数值函数</h3><pre><code>round：四舍五入    

ceil：向上取整   

floor：向下取整  
</code></pre>
<h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><pre><code>substring：截取字符串  

substring(string A, int start)  

substring(string A, int start, int len)   
</code></pre>
<p>replace ：替换  </p>
<pre><code>replace(string A, string B, string C)   

regexp_replace：正则替换  

regexp_replace(string A, string B, string C)   
说明：将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符  
</code></pre>
<p>regexp：正则匹配 </p>
<pre><code>字符串 regexp 正则表达式     

说明：若字符串符合正则表达式，则返回true，否则返回false

select &#39;dfsaaaa&#39; regexp &#39;dfsa+&#39;  
</code></pre>
<p>repeat：重复字符串  </p>
<pre><code>repeat(string A, int n)  

说明：将字符串A重复n遍

select repeat(&#39;123&#39;, 3);  
</code></pre>
<p>split ：字符串切割  </p>
<pre><code>split(string str, string pat)   

返回值：array

hive&gt; select split(&#39;a-b-c-d&#39;,&#39;-&#39;);

hive&gt; [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;]
</code></pre>
<p>nvl ：替换null值  </p>
<pre><code>nvl(A,B) 

若A的值不为null，则返回A，否则返回B  

hive&gt; select nvl(null,1);   
</code></pre>
<p>concat ：拼接字符串  </p>
<pre><code>concat(string A, string B, string C, ……)   

将A,B,C……等字符拼接为一个字符串  

hive&gt; select concat(&#39;beijing&#39;,&#39;-&#39;,&#39;shanghai&#39;,&#39;-&#39;,&#39;shenzhen&#39;);  

hive&gt; beijing-shanghai-shenzhen  
</code></pre>
<p>concat_ws：以指定分隔符拼接字符串或者字符串数组  </p>
<pre><code>concat_ws(string A, string…| array(string))   

使用分隔符A拼接多个字符串，或者一个数组的所有元素。  

hive&gt;select concat_ws(&#39;-&#39;,&#39;beijing&#39;,&#39;shanghai&#39;,&#39;shenzhen&#39;);  

hive&gt; beijing-shanghai-shenzhen  

hive&gt; select concat_ws(&#39;-&#39;,array(&#39;beijing&#39;,&#39;shenzhen&#39;,&#39;shanghai&#39;));  

hive&gt; beijing-shanghai-shenzhen
</code></pre>
<p>get_json_object：解析json字符串   </p>
<pre><code>get_json_object(string json_string, string path)

解析json的字符串json_string，返回path指定的内容。如果输入的json字符串无效，那么返回NULL  

hive&gt; select get_json_object(&#39;[&#123;&quot;name&quot;:&quot;大海海&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;25&quot;&#125;,&#123;&quot;name&quot;:&quot;小宋宋&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;47&quot;&#125;]&#39;,&#39;$.[0].name&#39;);  
</code></pre>
<h3 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h3><p>unix_timestamp：返回当前或指定时间的时间戳</p>
<pre><code>unix_timestamp()   

返回值：bigint    

hive&gt; select unix_timestamp(&#39;2022/08/08 08-08-08&#39;,&#39;yyyy/MM/dd HH-mm-ss&#39;);

1659946088  
</code></pre>
<p>from_unixtime：转化UNIX时间戳（从 1970-01-01 00:00:00 UTC 到指定时间的秒数）到当前时区的时间格式</p>
<pre><code>from_unixtime(bigint unixtime[, string format])  

返回值：string  

hive&gt; select from_unixtime(1659946088);  

2022-08-08 08:08:08  
</code></pre>
<p>current_date：当前日期   </p>
<p>current_timestamp：当前的日期加时间，并且精确的毫秒   </p>
<p>month：获取日期中的月  </p>
<pre><code>语法：month (string date)   

返回值：int   
</code></pre>
<p>day：获取日期中的日  </p>
<pre><code>语法：day (string date)   

返回值：int 
</code></pre>
<p>hour：获取日期中的小时  </p>
<pre><code>语法：hour (string date)   

返回值：int   
</code></pre>
<p>datediff：两个日期相差的天数（结束日期减去开始日期的天数）</p>
<pre><code>语法：datediff(string enddate, string startdate) 

返回值：int   

hive&gt; select datediff(&#39;2021-08-08&#39;,&#39;2022-10-09&#39;);    

    -427    
</code></pre>
<p>date_add：日期加天数  </p>
<pre><code>date_add(string startdate, int days)   
</code></pre>
<p>date_sub：日期减天数  </p>
<pre><code>date_sub (string startdate, int days)   
</code></pre>
<p>date_format:将标准日期解析成指定格式字符串  </p>
<pre><code>hive&gt; select date_format(&#39;2022-08-08&#39;,&#39;yyyy年-MM月-dd日&#39;)  

2022年-08月-08日 
</code></pre>
<h2 id="流程控制函数"><a href="#流程控制函数" class="headerlink" title="流程控制函数"></a>流程控制函数</h2><p>case when：条件判断函数  </p>
<pre><code>语法一： case when a then b [when c then d]* [else e] end   

语法二： case a when b then c [when d then e]* [else f] end
判断同一个字段与多个值是否相等时才能这样写  
</code></pre>
<p>if: 条件判断，类似于Java中三元运算符</p>
<pre><code>if（boolean testCondition, T valueTrue, T valueFalseOrNull）  

当条件testCondition为true时，返回valueTrue；否则返回valueFalseOrNull	 

hive&gt; select if(10 &gt; 5,&#39;正确&#39;,&#39;错误&#39;);    

输出：正确  
</code></pre>
<h2 id="集合函数"><a href="#集合函数" class="headerlink" title="集合函数"></a>集合函数</h2><p>size：集合中元素的个数  </p>
<pre><code>hive&gt; select size(friends) from test;  --2/2  每一行数据中的friends集合里的个数  
</code></pre>
<p>map：创建map集合</p>
<pre><code>语法：map (key1, value1, key2, value2, …) 

说明：根据输入的key和value对构建map类型  
</code></pre>
<p>map_keys： 返回map中的key  </p>
<pre><code>hive&gt; select map_keys(map(&#39;xiaohai&#39;,1,&#39;dahai&#39;,2));  

hive&gt;[&quot;xiaohai&quot;,&quot;dahai&quot;] 
</code></pre>
<p>map_values: 返回map中的value  </p>
<pre><code>hive&gt; select map_values(map(&#39;xiaohai&#39;,1,&#39;dahai&#39;,2));

hive&gt;[1,2]  
</code></pre>
<p>array 声明array集合  </p>
<pre><code>语法：array(val1, val2, …) 

说明：根据输入的参数构建数组array类  

hive&gt; select array(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;);  

hive&gt;[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;]  
</code></pre>
<p>array_contains: 判断array中是否包含某个元素  </p>
<pre><code>hive&gt; select array_contains(array(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;),&#39;a&#39;);   

hive&gt; true 
</code></pre>
<p>sort_array：将array中的元素排序</p>
<pre><code>hive&gt; select sort_array(array(&#39;a&#39;,&#39;d&#39;,&#39;c&#39;));  

hive&gt; [&quot;a&quot;,&quot;c&quot;,&quot;d&quot;]  
</code></pre>
<p>struct声明struct中的各属性   </p>
<pre><code>语法：struct(val1, val2, val3, …)   

说明：根据输入的参数构建结构体struct类  

hive&gt; select struct(&#39;name&#39;,&#39;age&#39;,&#39;weight&#39;);  

hive&gt; &#123;&quot;col1&quot;:&quot;name&quot;,&quot;col2&quot;:&quot;age&quot;,&quot;col3&quot;:&quot;weight&quot;&#125;  
</code></pre>
<p>named_struct声明struct的属性和值  </p>
<pre><code>hive&gt; select named_struct(&#39;name&#39;,&#39;xiaosong&#39;,&#39;age&#39;,18,&#39;weight&#39;,80);  

hive&gt; &#123;&quot;name&quot;:&quot;xiaosong&quot;,&quot;age&quot;:18,&quot;weight&quot;:80&#125;  
</code></pre>
<h2 id="2-高级聚合函数"><a href="#2-高级聚合函数" class="headerlink" title="2.高级聚合函数"></a>2.高级聚合函数</h2><p>多进一出 （多行传入，一个行输出） </p>
<p>普通聚合  </p>
<p>collect_list 收集并形成list集合，结果不去重</p>
<pre><code>hive&gt;
select sex,collect_list(job) from employee group by sex;  

女	[&quot;行政&quot;,&quot;研发&quot;,&quot;行政&quot;,&quot;前台&quot;]  
男	[&quot;销售&quot;,&quot;研发&quot;,&quot;销售&quot;,&quot;前台&quot;]  
</code></pre>
<p>collect_set 收集并形成set集合，结果去重  </p>
<pre><code>hive&gt;
select sex,collect_set(job) from employee group by sex;

女	[&quot;行政&quot;,&quot;研发&quot;,&quot;前台&quot;]	
男	[&quot;销售&quot;,&quot;研发&quot;,&quot;前台&quot;]  
</code></pre>
<h2 id="3-炸裂函数"><a href="#3-炸裂函数" class="headerlink" title="3.炸裂函数"></a>3.炸裂函数</h2><p>UDTF,接收一行数据，输出一行或多行数据。</p>
<h2 id="4-窗口函数"><a href="#4-窗口函数" class="headerlink" title="4.窗口函数"></a>4.窗口函数</h2><p>窗口函数，能为每行数据划分一个窗口，然后对窗口范围内的数据进行计算，最后将计算结果返回给该行的数据。</p>
<h3 id="窗口语法"><a href="#窗口语法" class="headerlink" title="窗口语法"></a>窗口语法</h3><h4 id="基于行"><a href="#基于行" class="headerlink" title="基于行"></a>基于行</h4><p><img src="/2023/07/30/hive_learn/3.png" alt="基于行">            </p>
<h4 id="基于值"><a href="#基于值" class="headerlink" title="基于值"></a>基于值</h4><p><img src="/2023/07/30/hive_learn/4.png" alt="基于值"></p>
<p>按照功能，常用窗口可划分为如下几类：聚合函数、跨行取值函数、排名函数  </p>
<p>聚合函数</p>
<pre><code>max：最大值

min：最小值  

sum：求和

avg：平均值

count：计数    
</code></pre>
<p>跨行取值函数</p>
<pre><code>Lead：上移  

Lag：下移

注：lag和lead函数不支持自定义窗口

first_value

last_value  
</code></pre>
<p>排名函数  </p>
<pre><code>rank 

dense_rank

row_number

三者均不支持自定义窗口  
</code></pre>
<h1 id="六：自定义函数"><a href="#六：自定义函数" class="headerlink" title="六：自定义函数"></a>六：自定义函数</h1><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">自定义函数官方文档</a> </p>
<h2 id="编程步骤"><a href="#编程步骤" class="headerlink" title="编程步骤"></a>编程步骤</h2><p>(1) 继承Hive提供的类  </p>
<p>org.apache.hadoop.hive.ql.udf.generic.GenericUDF  </p>
<p>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</p>
<p>(2) 实现类中的抽象方法  </p>
<p>(3) 在hive的命令行窗口创建函数  </p>
<p>添加jar  </p>
<pre><code>add jar linux_jar_path  
</code></pre>
<p>创建function  </p>
<pre><code>create [temporary] function [dbname.]function_name AS class_name;
</code></pre>
<p>(4) 在hive的命令行窗口删除函数  </p>
<pre><code>drop [temporary] function [if exists] [dbname.]function_name;
</code></pre>
<h1 id="七：分区表和分桶表"><a href="#七：分区表和分桶表" class="headerlink" title="七：分区表和分桶表"></a>七：分区表和分桶表</h1><h2 id="1-分区表"><a href="#1-分区表" class="headerlink" title="1.分区表"></a>1.分区表</h2><p>Hive中的分区就是把一张大表的数据按照业务需要分散的存储到多个目录，每个目录就称为该表的一个分区  </p>
<pre><code>hive (default)&gt; 
create table dept_partition
(
deptno int,    --部门编号
dname  string, --部门名称
loc    string  --部门位置
)
partitioned by (day string)
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<p>装载语句  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/dept_20220401.log&#39; 
into table dept_partition 
partition(day=&#39;20220401&#39;);  

hive (default)&gt; 
insert overwrite table dept_partition partition (day = &#39;20220402&#39;)
select deptno, dname, loc
from dept_partition
where day = &#39;2020-04-01&#39;;
</code></pre>
<h3 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a>分区表基本操作</h3><h4 id="1）查看所有分区信息"><a href="#1）查看所有分区信息" class="headerlink" title="1）查看所有分区信息"></a>1）查看所有分区信息</h4><pre><code>hive&gt; show partitions dept_partition;
</code></pre>
<h4 id="2）增加分区"><a href="#2）增加分区" class="headerlink" title="2）增加分区"></a>2）增加分区</h4><h5 id="（1）创建单个分区"><a href="#（1）创建单个分区" class="headerlink" title="（1）创建单个分区"></a>（1）创建单个分区</h5><pre><code>hive (default)&gt; 
alter table dept_partition 
add partition(day=&#39;20220403&#39;);  
</code></pre>
<h5 id="（2）同时创建多个分区（分区之间不能有逗号）"><a href="#（2）同时创建多个分区（分区之间不能有逗号）" class="headerlink" title="（2）同时创建多个分区（分区之间不能有逗号）"></a>（2）同时创建多个分区（分区之间不能有逗号）</h5><pre><code>hive (default)&gt; 
alter table dept_partition 
add partition(day=&#39;20220404&#39;) partition(day=&#39;20220405&#39;);  
</code></pre>
<h4 id="3）删除分区"><a href="#3）删除分区" class="headerlink" title="3）删除分区"></a>3）删除分区</h4><h5 id="（1）删除单个分区"><a href="#（1）删除单个分区" class="headerlink" title="（1）删除单个分区"></a>（1）删除单个分区</h5><pre><code>hive (default)&gt; 
alter table dept_partition 
drop partition (day=&#39;20220403&#39;);
</code></pre>
<h5 id="（2）同时删除多个分区（分区之间必须有逗号）"><a href="#（2）同时删除多个分区（分区之间必须有逗号）" class="headerlink" title="（2）同时删除多个分区（分区之间必须有逗号）"></a>（2）同时删除多个分区（分区之间必须有逗号）</h5><pre><code>hive (default)&gt; 
alter table dept_partition 
drop partition (day=&#39;20220404&#39;), partition(day=&#39;20220405&#39;);
</code></pre>
<h4 id="4）修复分区"><a href="#4）修复分区" class="headerlink" title="4）修复分区"></a>4）修复分区</h4><p>add partition  </p>
<pre><code>若手动创建HDFS的分区路径，Hive无法识别，可通过add partition命令增加分区元数据信息，从而使元数据和分区路径保持一致
</code></pre>
<p>drop partition  </p>
<pre><code>若手动删除HDFS的分区路径，Hive无法识别，可通过drop partition命令删除分区元数据信息，从而使元数据和分区路径保持一致
</code></pre>
<p>msck </p>
<p>若分区元数据和HDFS的分区路径不一致，还可使用msck命令进行修复，以下是该命令的用法说明 </p>
<pre><code>hive (default)&gt; 
msck repair table table_name [add/drop/sync partitions];

msck repair table table_name add partitions：该命令会增加HDFS路径存在但元数据缺失的分区信息 

msck repair table table_name drop partitions：该命令会删除HDFS路径已经删除但元数据仍然存在的分区信息 

msck repair table table_name sync partitions：该命令会同步HDFS路径和元数据分区信息，相当于同时执行上述的两个命令

msck repair table table_name：等价于msck repair table table_name add partitions命令
</code></pre>
<p><strong>所以msck修复hive元数据，首选msck repair table table_name sync partitions命令</strong>  </p>
<h4 id="二级分区表"><a href="#二级分区表" class="headerlink" title="二级分区表"></a>二级分区表</h4><p>二级分区表建表语句  </p>
<pre><code>hive (default)&gt;
create table dept_partition2(
deptno int,    -- 部门编号
dname string, -- 部门名称
loc string     -- 部门位置
)
partitioned by (day string, hour string)
row format delimited fields terminated by &#39;\t&#39;; 
</code></pre>
<p>数据装载语句  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/dept_20220401.log&#39; 
into table dept_partition2 
partition(day=&#39;20220401&#39;, hour=&#39;12&#39;);  
</code></pre>
<p>查询分区数据  </p>
<pre><code>hive (default)&gt; 
select  * 
from dept_partition2 
where day=&#39;20220401&#39; and hour=&#39;12&#39;;  
</code></pre>
<h4 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h4><p>动态分区是指向分区表insert数据时，被写往的分区不由用户指定，而是由每行数据的最后一个字段的值来动态的决定，使用动态分区，可只用一个insert语句将数据写入多个分区  </p>
<h5 id="1）动态分区相关参数"><a href="#1）动态分区相关参数" class="headerlink" title="1）动态分区相关参数"></a>1）动态分区相关参数</h5><p>(1) 动态分区功能总开关（默认true，开启）  </p>
<pre><code>set hive.exec.dynamic.partition=true  
</code></pre>
<p>(2) 严格模式和非严格模式   </p>
<p>动态分区的模式，默认strict（严格模式），要求必须指定至少一个分区为静态分区，nonstrict（非严格模式）允许所有的分区字段都使用动态分区  </p>
<pre><code>set hive.exec.dynamic.partition.mode=nonstrict
</code></pre>
<h2 id="2-分桶表"><a href="#2-分桶表" class="headerlink" title="2.分桶表"></a>2.分桶表</h2><p>并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分  </p>
<p><strong>分区针对的是数据的存储路径，分桶针对的是数据文件</strong>  </p>
<p>分桶表的基本原理是，首先为每行数据计算一个指定字段的数据的hash值，然后模以一个指定的分桶数，最后将取模运算结果相同的行，写入同一个文件中，这个文件就称为一个分桶（bucket）</p>
<p>建表语句</p>
<pre><code>hive (default)&gt; 
create table stu_buck(
id int, 
name string
)
clustered by(id) sorted by(id)
into 4 buckets
row format delimited fields terminated by &#39;\t&#39;;

load data local inpath &#39;/opt/module/hive/datas/student.txt&#39; 
into table stu_buck;
</code></pre>
<h1 id="8-Hive压缩格式和文件格式"><a href="#8-Hive压缩格式和文件格式" class="headerlink" title="8.Hive压缩格式和文件格式"></a>8.Hive压缩格式和文件格式</h1><h2 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h2><p>DEFLATE<br>gzip<br>bzip2<br>LZO<br>Snappy</p>
<h2 id="Hive文件格式"><a href="#Hive文件格式" class="headerlink" title="Hive文件格式"></a>Hive文件格式</h2><p>text file<br>orc<br>parquet<br>sequence file  </p>
<h2 id="行式存储和列式存储"><a href="#行式存储和列式存储" class="headerlink" title="行式存储和列式存储"></a>行式存储和列式存储</h2><h3 id="行式存储-textfile，sequence-file"><a href="#行式存储-textfile，sequence-file" class="headerlink" title="行式存储 - textfile，sequence file"></a>行式存储 - textfile，sequence file</h3><p>文本文件是Hive默认使用的文件格式，文本文件中的一行内容，就对应Hive表中的一行记录</p>
<p>适用于会用到很多where语句的表</p>
<h3 id="列式存储-orc，parquet"><a href="#列式存储-orc，parquet" class="headerlink" title="列式存储 - orc，parquet"></a>列式存储 - orc，parquet</h3><p>数仓中尽量选用列式存储方式</p>
<p><img src="/2023/07/30/hive_learn/5.png" alt="ORC文件基本格式">  </p>
<p><img src="/2023/07/30/hive_learn/6.png" alt="Parquet文件基本格式">   </p>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><p>在Hive表中和计算过程中，保持数据的压缩，对磁盘空间的有效利用和提高查询性能都是十分有益的  </p>
<h3 id="Hive表数据进行压缩"><a href="#Hive表数据进行压缩" class="headerlink" title="Hive表数据进行压缩"></a>Hive表数据进行压缩</h3><h4 id="1）TextFile"><a href="#1）TextFile" class="headerlink" title="1）TextFile"></a>1）TextFile</h4><p>无法直接在表结构中进行声明压缩    </p>
<p>直接将压缩后的文件导入到该表即可，Hive在查询表中数据时，可自动识别其压缩格式，进行解压  </p>
<p>TextFile压缩格式常为Gzip  </p>
<p>需要注意的是，在执行往表中导入数据的SQL语句时，用户需设置以下参数，来保证写入表中的数据是被压缩的。  </p>
<p>–SQL语句的最终输出结果是否压缩  </p>
<pre><code>set hive.exec.compress.output=true;  
</code></pre>
<p>–输出结果的压缩格式（以下示例为snappy）  </p>
<pre><code>set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec;
</code></pre>
<h4 id="2）ORC"><a href="#2）ORC" class="headerlink" title="2）ORC"></a>2）ORC</h4><p>可在建表时声明    </p>
<pre><code>create table orc_table
(column_specs)
stored as orc
tblproperties (&quot;orc.compress&quot;=&quot;snappy&quot;);  
</code></pre>
<h4 id="3）Parquet"><a href="#3）Parquet" class="headerlink" title="3）Parquet"></a>3）Parquet</h4><p>可在建表时声明   </p>
<pre><code>create table orc_table
(column_specs)
stored as parquet
tblproperties (&quot;parquet.compression&quot;=&quot;snappy&quot;);
</code></pre>
<h3 id="计算过程中使用压缩"><a href="#计算过程中使用压缩" class="headerlink" title="计算过程中使用压缩"></a>计算过程中使用压缩</h3><p>1）单个mr的中间结果进行压缩  </p>
<p>–开启MapReduce中间数据压缩功能  </p>
<pre><code>set mapreduce.map.output.compress=true;
</code></pre>
<p>–设置MapReduce中间数据数据的压缩方式（以下示例为snappy） </p>
<pre><code>set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;  
</code></pre>
<p>2）单条sql语句的中间结果进行压缩    </p>
<p>单条SQL语句的中间结果是指，两个MR（一条SQL语句可能需要通过MR进行计算）之间的临时数据，可通过以下参数进行配置：   </p>
<p>–是否对两个MR之间的临时数据进行压缩  </p>
<pre><code>set hive.exec.compress.intermediate=true;  
</code></pre>
<p>–压缩格式（以下示例为snappy）  </p>
<pre><code>set hive.intermediate.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;
</code></pre>
<h1 id="能看到这里，你是真滴牛批"><a href="#能看到这里，你是真滴牛批" class="headerlink" title="能看到这里，你是真滴牛批~"></a>能看到这里，你是真滴牛批~</h1><h1 id="猛男，帅哥儿，靓仔，点个赞再肘"><a href="#猛男，帅哥儿，靓仔，点个赞再肘" class="headerlink" title="猛男，帅哥儿，靓仔，点个赞再肘~"></a>猛男，帅哥儿，靓仔，点个赞再肘~</h1><p><img src="/2023/07/30/hive_learn/7.png" alt="歪嘴猫"></p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/hexo_erro/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/hexo_erro/" class="post-title-link" itemprop="url">hexo上传报错问题记录</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-30 12:49:06 / 修改时间：13:00:17" itemprop="dateCreated datePublished" datetime="2023-07-30T12:49:06+08:00">2023-07-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上传博文到hexo时突然报错，如下图<br><img src="/2023/07/30/hexo_erro/1.png" alt="报错"></p>
<p>解决方案1：  </p>
<p>进入站点根目录<br>cd E:&#x2F;hexo</p>
<p>删除git提交内容文件夹<br>rm -rf .deploy_git&#x2F;</p>
<p>执行<br>git config –global core.autocrlf false</p>
<p>最后<br>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</p>
<p>依旧报错</p>
<p>后仔细阅读报错发现，github与git之间的网络通信存在问题</p>
<p>解决方案2：</p>
<p>重新建立本机git与github之间的通信</p>
<p>在git-bash命令行下，输入ssh-keygen -t rsa 命令（注意空格），表示我们指定 RSA 算法生成密钥，然后敲四次回车键，之后就就会生成两个文件，分别为秘钥 id_rsa 和公钥 id_rsa.pub. （注意：git中的复制粘贴不是 Ctrl+C 和 Ctrl+V，而是 Ctrl+insert 和 Shift+insert.）文件的位置在 Git Bash 上面都有显示，默认生成在以下目录：</p>
<p>Windows 10 ：C:&#x2F;Users&#x2F;ASUS&#x2F;.ssh</p>
<p><img src="/2023/07/30/hexo_erro/2.png" alt="&quot;sshkey配置&quot;"></p>
<p>验证是否成功，我们可以通过在 Git Bash 中输入 ssh -T <a href="mailto:&#103;&#105;&#116;&#64;&#x67;&#x69;&#x74;&#104;&#x75;&#x62;&#x2e;&#99;&#111;&#x6d;">&#103;&#105;&#116;&#64;&#x67;&#x69;&#x74;&#104;&#x75;&#x62;&#x2e;&#99;&#111;&#x6d;</a> 进行检验</p>
<p>注意查看日志，验证成功后，再依次输入hexo clean , hexo g , hexo d 发布博文</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/markdown%E8%AF%AD%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/markdown%E8%AF%AD%E6%B3%95/" class="post-title-link" itemprop="url">markdown语法</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-30 12:20:49 / 修改时间：12:21:10" itemprop="dateCreated datePublished" datetime="2023-07-30T12:20:49+08:00">2023-07-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>官网地址 <a target="_blank" rel="noopener" href="https://markdown.com.cn/basic-syntax/headings.html">https://markdown.com.cn/basic-syntax/headings.html</a>  </p>
<p>1.Markdown 标题语法<br>要创建标题，请在单词或短语前面添加井号 (#) 。# 的数量代表了标题的级别。例如，添加三个 # 表示创建一个三级标题  </p>
<p>2.段落语法<br>要创建段落，请使用空白行将一行或多行文本进行分隔</p>
<p>3.换行语法<br>在一行的末尾添加两个或多个空格，然后按回车键,即可创建一个换行</p>
<p>4.粗体<br>要加粗文本，请在单词或短语的前后各添加两个星号（asterisks）或下划线（underscores）</p>
<p>5.斜体<br>要用斜体显示文本，请在单词或短语前后添加一个星号（asterisk）或下划线（underscore）</p>
<p>6.引用语法<br>要创建块引用，请在段落前添加一个 &gt; 符号</p>
<p>7.代码语法<br>要将单词或短语表示为代码，请将其包裹在反引号 (<code> </code>) 中<br>将多行代码写入一个代码框中，采用（<code>`代码`</code>）的方式</p>
<p>8.分割线语法<br>要创建分隔线，请在单独一行上使用三个或多个星号 (***)、破折号 (—) 或下划线 (___) ，并且不能包含其他内容  </p>
<p>9.超链接语法<br>要创建分隔线，请在单独一行上使用三个或多个星号 (***)、破折号 (—) 或下划线 (___) ，并且不能包含其他内容</p>
<p>10.插入图片的语法<br>插入图片Markdown语法代码：![图片alt]（图片链接 “图片title”）</p>
<p>11.转义字符<br>要显示原本用于格式化 Markdown 文档的字符，请在字符前面添加反斜杠字符 \ </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/photo-test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/photo-test/" class="post-title-link" itemprop="url">hexo博客插入本地图片的方法</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-30 10:32:03 / 修改时间：10:54:59" itemprop="dateCreated datePublished" datetime="2023-07-30T10:32:03+08:00">2023-07-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>hexo 中插入图片的操作步骤<br>1：要添加图片，先npm install 一个hexo-asset-image的依赖<br>   npm install <a target="_blank" rel="noopener" href="https://github.com/CodeFalling/hexo-asset-image">https://github.com/CodeFalling/hexo-asset-image</a> –save  </p>
<p>2.然后把_config.yml中的post_asset_folder设为true，这个配置的意思是每次new post一个博客，会增加一个和博客同名的文件夹，将需要写入博文的本地图片放到这个对应的文件夹中<br><img src="/2023/07/30/photo-test/1.png" alt="图片"></p>
<p>3.![图片描述]（.&#x2F;包名&#x2F;NO.01.001.jpg） 依照此格式代码引用本地图片</p>
<p>4.保存文件的修改之后，在命令行输入hexo clean,接着输入hexo g –d。在浏览器输入域名后查看相关文章，显示图片成功！！！</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/29/%E4%B8%AA%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/29/%E4%B8%AA%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/" class="post-title-link" itemprop="url">个人博客搭建流程</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-29 22:47:56" itemprop="dateCreated datePublished" datetime="2023-07-29T22:47:56+08:00">2023-07-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-30 10:42:52" itemprop="dateModified" datetime="2023-07-30T10:42:52+08:00">2023-07-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>个人博客搭建流程详细教程链接：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102592286">https://zhuanlan.zhihu.com/p/102592286</a></p>
<p>博客使用方式<br>在Blog目录下，将新创建的markdown文件上传到\Blog\source\_posts 目录下<br>右击文件夹空白处 进入git bash命令行界面<br>依次输入以下指令<br>hexo new post “first-page”  #创建新的博文<br>hexo clean<br>hexo g<br>hexo d<br>hexo s<br>运行完毕即可在 hadoopzyy.top或者本地localhost:4000查看博客内容</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=2042878838&auto=1&height=66"></iframe>

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张宴银"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">张宴银</p>
  <div class="site-description" itemprop="description">初级以内我无敌，中级以上我一换一</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; Sat Jul 29 2023 08:00:00 GMT+0800 (中国标准时间) – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张宴银</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>



    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共52.9k字</span>
</div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
