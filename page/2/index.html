<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="初级以内我无敌，中级以上我一换一">
<meta property="og:type" content="website">
<meta property="og:title" content="第五门徒">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="第五门徒">
<meta property="og:description" content="初级以内我无敌，中级以上我一换一">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张宴银">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>第五门徒</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="第五门徒" type="application/rss+xml">
</head>




<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">第五门徒</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="heartbeat fa-fw"></i>公益 404</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">SQLServer查询体系学习记录</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-01 11:07:31" itemprop="dateCreated datePublished" datetime="2023-08-01T11:07:31+08:00">2023-08-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 12:59:20" itemprop="dateModified" datetime="2023-08-11T12:59:20+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="SQLServer查询处理体系结构指南"><a href="#SQLServer查询处理体系结构指南" class="headerlink" title="SQLServer查询处理体系结构指南"></a>SQLServer查询处理体系结构指南</h1><h2 id="执行模式"><a href="#执行模式" class="headerlink" title="执行模式"></a>执行模式</h2><p>行执行模式<br>批执行模式 </p>
<h3 id="行执行模式"><a href="#行执行模式" class="headerlink" title="行执行模式"></a>行执行模式</h3><p>行模式执行是用于传统 RDBMS 表（其中数据以行格式存储）的查询处理方法</p>
<h3 id="批执行模式"><a href="#批执行模式" class="headerlink" title="批执行模式"></a>批执行模式</h3><p>批模式执行是一种查询处理方法，用于统一处理多个行（因此采用“批”一词）    </p>
<p>批中的每列都作为一个矢量存储在单独的内存区域中，因此批模式处理是基于矢量的</p>
<p>当在批模式下执行查询并且查询访问列存储索引中的数据时，执行树运算符和子运算符会一次读取列段中的多行。 SQL Server 仅读取结果所需的列，即 SELECT 语句、JOIN 谓词或筛选谓词引用的列</p>
<p><strong>批执行模式VS行执行模式的优势</strong>：一次读取多行，再筛选。避免了行执行模式的多次读取，提高运行效率  </p>
<h2 id="SQL-语句处理"><a href="#SQL-语句处理" class="headerlink" title="SQL 语句处理"></a>SQL 语句处理</h2><p>处理单个 Transact-SQL 语句是 SQL Server 执行 Transact-SQL 语句的最基本方法    </p>
<h3 id="逻辑运算符优先级"><a href="#逻辑运算符优先级" class="headerlink" title="逻辑运算符优先级"></a>逻辑运算符优先级</h3><p>计算顺序依次为：NOT、AND最后是 OR。算术运算符和位运算符优先于逻辑运算符处理  </p>
<h2 id="优化-SELECT-语句"><a href="#优化-SELECT-语句" class="headerlink" title="优化 SELECT 语句"></a>优化 SELECT 语句</h2><p>语句 SELECT 是非过程性的，数据库服务器必须分析语句，以决定提取所请求数据的最有效方法。  </p>
<p>执行此操作的组件称为查询优化器，可以使数据库服务器针对数据库内的更改情况进行动态调整，而无需程序员或数据库管理员输入  </p>
<p>order 排序字段上应该建索引  </p>
<p>从潜在的多个可能的计划中选择一个执行计划的过程称为“优化”  </p>
<p>SQL Server 查询优化器是基于成本的优化器（CBO）</p>
<p>当执行复杂的SQL语句时，不会去分析所有的执行计划成本，会根据算法选一个执行计划，其成本合理地接近最低可能成本的执行计划    </p>
<p><strong>除了CBO，还要考虑运行效率</strong>：SQL Server查询优化器不会仅选择资源成本最低的执行计划;它选择以合理的资源成本向用户返回结果的计划，并且以最快的速度返回结果  </p>
<p>SQL Server 查询优化器总能针对数据库的状态生成一个有效的执行计划    </p>
<p>SQL Server Management Studio 有三个选项可用于显示执行计划</p>
<pre><code>1.估计的执行计划    

2.实际执行计划    

3.实时查询统计信息，这与编译的计划及其执行上下文相同  

    这包括执行过程中的运行时信息，每秒更新一次
</code></pre>
<h2 id="密度"><a href="#密度" class="headerlink" title="密度"></a>密度</h2><p>密度定义数据中存在的唯一值的分布，或给定列的重复值平均数。 密度与值的选择性成反比，密度越小，值的选择性越大  </p>
<h2 id="处理-SELECT-语句"><a href="#处理-SELECT-语句" class="headerlink" title="处理 SELECT 语句"></a>处理 SELECT 语句</h2><p>SQL Server 处理单个 SELECT 语句的基本步骤包括如下内容：  </p>
<p>1.解析select语句</p>
<p>2.生成查询树</p>
<p>3.生成执行计划，并选择合理的执行计划</p>
<p>4.运行执行计划  </p>
<p>5.返回结果</p>
<h2 id="常量折叠和表达式计算"><a href="#常量折叠和表达式计算" class="headerlink" title="常量折叠和表达式计算"></a>常量折叠和表达式计算</h2><h3 id="可折叠表达式"><a href="#可折叠表达式" class="headerlink" title="可折叠表达式"></a>可折叠表达式</h3><p>仅包含常量的算术表达式  </p>
<p>仅包含常量的逻辑表达式  </p>
<p>被 SQL Server 认为可折叠的内置函数包括 CAST 和 CONVERT   </p>
<p>CLR 用户定义类型的确定性方法和确定性的标量值 CLR 用户定义函数  </p>
<h3 id="不可折叠的表达式"><a href="#不可折叠的表达式" class="headerlink" title="不可折叠的表达式"></a>不可折叠的表达式</h3><p>所有其他表达式类型都是不可折叠的。 特别是下列类型的表达式是不可折叠的：  </p>
<p>非常量表达式    </p>
<p>结果取决于局部变量或参数的表达式   </p>
<p>不确定性函数     </p>
<p>用户定义的 Transact-SQL 函数  </p>
<p>结果取决于语言设置的表达式   </p>
<p>结果取决于 SET 选项的表达式  </p>
<p>结果取决于服务器配置选项的表达式  </p>
<h2 id="常量折叠的优点"><a href="#常量折叠的优点" class="headerlink" title="常量折叠的优点"></a>常量折叠的优点</h2><p>表达式不必在运行时重复计算  </p>
<p>查询优化器可使用计算表达式后所得的值来估计 TotalDue &gt; 117.00 + 1000.00 查询部分的结果集的大小  </p>
<h2 id="工作表"><a href="#工作表" class="headerlink" title="工作表"></a>工作表</h2><p>工作表是用于保存中间结果的内部表。 某些 GROUP BY、 ORDER BY或 UNION 查询会生成工作表  </p>
<p>工作表在 tempdb 中生成，并在不再需要时自动删除  </p>
<p>SQL Server 查询处理器对索引视图和非索引视图将区别对待  </p>
<p>索引视图的行以表的格式存储在数据库中   </p>
<p>只有非索引视图的定义才存储，而不存储视图的行  </p>
<p> 如果索引视图中的数据包括所有或部分 Transact-SQL 语句，而且查询优化器确定视图的某个索引是低成本的访问路径，则不论查询中是否引用了该视图的名称，查询优化器都将选择此索引  </p>
<p>视图没有单独的执行计划</p>
<h2 id="存储过程和触发器执行"><a href="#存储过程和触发器执行" class="headerlink" title="存储过程和触发器执行"></a>存储过程和触发器执行</h2><p>SQL Server 仅存储存储过程和触发器的源  </p>
<p>第一次执行存储过程或触发器时，源被编译为执行计划，在内存中被释放后需要重新运行存储过程再次生成  </p>
<p>执行计划缓存和重用  </p>
<p>SQL Server 有一个用于存储执行计划和数据缓冲区的内存池，池内分配给执行计划或数据缓冲区的百分比随系统状态动态波动    </p>
<p>计划缓存有两个不用于存储计划的附加存储：  </p>
<p>“对象计划”缓存存储 (OBJCP)  </p>
<pre><code>用于与持久化对象（存储过程、函数和触发器）相关的计划   
</code></pre>
<p>“SQL 计划”缓存存储 (SQLCP)   </p>
<pre><code>用于与自动参数化、动态或已准备的查询相关的计划  
</code></pre>
<h2 id="从计划缓存中删除执行计划"><a href="#从计划缓存中删除执行计划" class="headerlink" title="从计划缓存中删除执行计划"></a>从计划缓存中删除执行计划</h2><p>只要计划缓存中有足够的存储空间，执行计划就会保留在其中   </p>
<p>当存在内存不足的情况时，SQL Server 数据库引擎将使用基于开销的方法来确定从计划缓存中删除哪些执行计划  </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/31/Hive-on-mr/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/31/Hive-on-mr/" class="post-title-link" itemprop="url">Hive on mr调优</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-31 15:57:39" itemprop="dateCreated datePublished" datetime="2023-07-31T15:57:39+08:00">2023-07-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:05:08" itemprop="dateModified" datetime="2023-08-11T13:05:08+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="我于人间全无敌，不与天战与shui战？"><a href="#我于人间全无敌，不与天战与shui战？" class="headerlink" title="我于人间全无敌，不与天战与shui战？"></a>我于人间全无敌，不与天战与shui战？</h1><p>本文测试数据和Explain可视化工具资料包<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1Q8Zt7gWBF6JkW_Kg6sIWzw">https://pan.baidu.com/s/1Q8Zt7gWBF6JkW_Kg6sIWzw</a><br>提取码：2khx     </p>
<p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1Wdc2f38csrGbC3GA1s0H_w">https://pan.baidu.com/s/1Wdc2f38csrGbC3GA1s0H_w</a><br>提取码：888i   </p>
<p>永久有效，失效来打我~<br><img src="/2023/07/31/Hive-on-mr/1.gif"></p>
<p>Hive on mr  </p>
<p>即Hive引擎选用mapreduce。（目前Hive引擎可选项为Mapreduce&#x2F;Tez&#x2F;Spark）  </p>
<p>调优主要分为下面三个方向 </p>
<p>1)：组件资源调优  </p>
<p>通过控制任务运行的组件资源，实现任务的高效运行</p>
<p>2)：Explain执行计划调优  </p>
<p>通过优化执行计划,保证相同资源配置的情况下，任务运行更流畅  </p>
<p>3): 常有调优参数设置  </p>
<p>开启Hive内置的一些有助于任务高效运行的设置,保障任务流畅运行  </p>
<h2 id="1-组件资源调优"><a href="#1-组件资源调优" class="headerlink" title="1:组件资源调优"></a>1:组件资源调优</h2><h3 id="Yarn资源配置"><a href="#Yarn资源配置" class="headerlink" title="Yarn资源配置"></a>Yarn资源配置</h3><p>需要调整的Yarn参数均与CPU、内存等资源有关，核心配置参数如下  </p>
<p>（1）yarn.nodemanager.resource.memory-mb  </p>
<p>该参数的含义是，一个NodeManager节点分配给Container使用的内存。该参数的配置，<strong>取决于NodeManager所在节点的总内存容量和该节点运行的其他服务的数量</strong>。  </p>
<p>（2）yarn.nodemanager.resource.cpu-vcores  </p>
<p>该参数的含义是，一个NodeManager节点分配给Container使用的CPU核数。该参数的配置，<strong>同样取决于NodeManager所在节点的总CPU核数和该节点运行的其他服务</strong>。</p>
<p>通常是一个核4个G  </p>
<pre><code>即（1）yarn.nodemanager.resource.memory-mb/（2）yarn.nodemanager.resource.cpu-vcores  = 4  
</code></pre>
<p>（3）yarn.scheduler.maximum-allocation-mb  </p>
<p>该参数的含义是，单个Container能够使用的最大内存</p>
<pre><code>（1）yarn.nodemanager.resource.memory-mb /（3）yarn.scheduler.maximum-allocation-mb  = 整数
</code></pre>
<p>（4）yarn.scheduler.minimum-allocation-mb  </p>
<p>该参数的含义是，单个Container能够使用的最小内存，推荐配置(512M)如下：  </p>
<pre><code>&lt;property&gt;
    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
    &lt;value&gt;512&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<h3 id="MapReduce资源配置"><a href="#MapReduce资源配置" class="headerlink" title="MapReduce资源配置"></a>MapReduce资源配置</h3><p>MapReduce资源配置主要包括Map Task的内存和CPU核数，以及Reduce Task的内存和CPU核数  </p>
<p>1）mapreduce.map.memory.mb	  </p>
<p>该参数的含义是，单个Map Task申请的container容器内存大小，其默认值为1024。该值不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围  </p>
<p>该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置： </p>
<pre><code>set  mapreduce.map.memory.mb=2048;  
</code></pre>
<p>2）mapreduce.map.cpu.vcores  </p>
<p>该参数的含义是，单个Map Task申请的container容器cpu核数，其默认值为1。该值一般无需调整</p>
<p>3）mapreduce.reduce.memory.mb	</p>
<p>该参数的含义是，单个Reduce Task申请的container容器内存大小，其默认值为1024。该值同样不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围  </p>
<p>该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置  </p>
<pre><code>set  mapreduce.reduce.memory.mb=2048;  
</code></pre>
<p>4）mapreduce.reduce.cpu.vcores	  </p>
<p>该参数的含义是，单个Reduce Task申请的container容器cpu核数，其默认值为1。该值一般无需调整  </p>
<h1 id="2-Explain执行计划调优"><a href="#2-Explain执行计划调优" class="headerlink" title="2.Explain执行计划调优"></a>2.Explain执行计划调优</h1><h2 id="测试用表"><a href="#测试用表" class="headerlink" title="测试用表"></a>测试用表</h2><h3 id="1-订单表-2000w条数据"><a href="#1-订单表-2000w条数据" class="headerlink" title="1.订单表(2000w条数据)"></a>1.订单表(2000w条数据)</h3><h4 id="建表语句"><a href="#建表语句" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt;
drop table if exists order_detail;
create table order_detail(
id           string comment &#39;订单id&#39;,
user_id      string comment &#39;用户id&#39;,
product_id   string comment &#39;商品id&#39;,
province_id  string comment &#39;省份id&#39;,
create_time  string comment &#39;下单时间&#39;,
product_num  int comment &#39;商品件数&#39;,
total_amount decimal(16, 2) comment &#39;下单金额&#39;
)
partitioned by (dt string)
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<h4 id="数据装载"><a href="#数据装载" class="headerlink" title="数据装载"></a>数据装载</h4><p>将order_detail.txt文件上传到hiveserver2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。</p>
<p>注：文件较大，请耐心等待。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/order_detail.txt&#39; overwrite into table order_detail partition(dt=&#39;2020-06-14&#39;); 
</code></pre>
<h3 id="2-支付表-600w条数据"><a href="#2-支付表-600w条数据" class="headerlink" title="2.支付表(600w条数据)"></a>2.支付表(600w条数据)</h3><h4 id="建表语句-1"><a href="#建表语句-1" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt;
drop table if exists payment_detail;
create table payment_detail(
id              string comment &#39;支付id&#39;,
order_detail_id string comment &#39;订单明细id&#39;,
user_id         string comment &#39;用户id&#39;,
payment_time    string comment &#39;支付时间&#39;,
total_amount    decimal(16, 2) comment &#39;支付金额&#39;
)
partitioned by (dt string)
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<h4 id="数据装载-1"><a href="#数据装载-1" class="headerlink" title="数据装载"></a>数据装载</h4><p>将payment_detail.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。</p>
<p>注：文件较大，请耐心等待。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/payment_detail.txt&#39; overwrite into table payment_detail partition(dt=&#39;2020-06-14&#39;);  
</code></pre>
<h3 id="3-商品信息表-100w条数据"><a href="#3-商品信息表-100w条数据" class="headerlink" title="3.商品信息表(100w条数据)"></a>3.商品信息表(100w条数据)</h3><h4 id="建表语句-2"><a href="#建表语句-2" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt; 
drop table if exists product_info;
create table product_info(
id           string comment &#39;商品id&#39;,
product_name string comment &#39;商品名称&#39;,
price        decimal(16, 2) comment &#39;价格&#39;,
category_id  string comment &#39;分类id&#39;
)
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<h4 id="数据装载-2"><a href="#数据装载-2" class="headerlink" title="数据装载"></a>数据装载</h4><p>将product_info.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/product_info.txt&#39; overwrite into table product_info;  
</code></pre>
<h3 id="4-省份信息表-34条数据"><a href="#4-省份信息表-34条数据" class="headerlink" title="4.省份信息表(34条数据)"></a>4.省份信息表(34条数据)</h3><h4 id="建表语句-3"><a href="#建表语句-3" class="headerlink" title="建表语句"></a>建表语句</h4><pre><code>hive (default)&gt; 
drop table if exists province_info;
create table province_info(
id            string comment &#39;省份id&#39;,
province_name string comment &#39;省份名称&#39;
)
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<h4 id="数据装载-3"><a href="#数据装载-3" class="headerlink" title="数据装载"></a>数据装载</h4><p>将province_info.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/province_info.txt&#39; overwrite into table province_info;  
</code></pre>
<h2 id="Explain查看执行计划（重点）"><a href="#Explain查看执行计划（重点）" class="headerlink" title="Explain查看执行计划（重点）"></a>Explain查看执行计划（重点）</h2><p>Explain呈现的执行计划，由一系列Stage组成，这一系列Stage具有依赖关系，每个Stage对应一个MapReduce Job，或者一个文件系统操作等。  </p>
<p>若某个Stage对应的一个MapReduce Job，其Map端和Reduce端的计算逻辑分别由Map Operator Tree和Reduce Operator Tree进行描述，Operator Tree由一系列的Operator组成，一个Operator代表在Map或Reduce阶段的一个单一的逻辑操作，例如TableScan Operator，Select Operator，Join Operator等</p>
<h3 id="常见的Operator及其作用如下："><a href="#常见的Operator及其作用如下：" class="headerlink" title="常见的Operator及其作用如下："></a>常见的Operator及其作用如下：</h3><p>TableScan：表扫描操作，通常map端第一个操作肯定是表扫描操作  </p>
<p>Select Operator：选取操作   </p>
<p>Group By Operator：分组聚合操作  </p>
<p>Reduce Output Operator：输出到 reduce 操作  </p>
<p>Filter Operator：过滤操作  </p>
<p>Join Operator：join 操作  </p>
<p>File Output Operator：文件输出操作  </p>
<p>Fetch Operator 客户端获取数据操作  </p>
<h3 id="Explain查看执行计划基本语法"><a href="#Explain查看执行计划基本语法" class="headerlink" title="Explain查看执行计划基本语法"></a>Explain查看执行计划基本语法</h3><p>EXPLAIN [FORMATTED | EXTENDED | DEPENDENCY] query-sql  </p>
<pre><code>FORMATTED：将执行计划以JSON字符串的形式输出  

EXTENDED：输出执行计划中的额外信息，通常是读写的文件名等信息  

DEPENDENCY：输出执行计划读取的表及分区  
</code></pre>
<h3 id="Explain执行计划可视化工具使用方法"><a href="#Explain执行计划可视化工具使用方法" class="headerlink" title="Explain执行计划可视化工具使用方法"></a>Explain执行计划可视化工具使用方法</h3><p>1.将本文开头百度网盘共享的dist文件压缩包上传至linux服务器中  </p>
<p>2.unzip dist.zip 解压  </p>
<p>3.进入解压后的dist文件夹下  </p>
<p>4.python -m SimpleHTTPServer 8901  启动可视化工具（此处的8901是我指定的可用端口号，可以按自己的想法设置）<br><img src="/2023/07/31/Hive-on-mr/1.png" alt="Explain可视化工具">  </p>
<p>5.将Explain执行计划制作成json格式粘贴到可视化工具里即可查看  </p>
<p><img src="/2023/07/31/Hive-on-mr/2.png" alt="Explain执行计划json格式">  </p>
<p>6.Explain可视化工具示例  </p>
<p><img src="/2023/07/31/Hive-on-mr/3.png" alt="示例">  </p>
<h2 id="HQL语法优化之分组聚合优化-（map-site）"><a href="#HQL语法优化之分组聚合优化-（map-site）" class="headerlink" title="HQL语法优化之分组聚合优化 （map-site）"></a>HQL语法优化之分组聚合优化 （map-site）</h2><p>Hive对分组聚合的优化主要围绕着减少Shuffle数据量进行，具体做法是map-side聚合  </p>
<p>在map端维护一个hash table进行预聚合，按照分组字段分区，发送至reduce端，完成最终的聚合。有效的减少shuffle操作的数量，达到提高运行效率的目的。  </p>
<h3 id="map-side-聚合相关的参数如下："><a href="#map-side-聚合相关的参数如下：" class="headerlink" title="map-side 聚合相关的参数如下："></a>map-side 聚合相关的参数如下：</h3><p>–启用map-side聚合 </p>
<pre><code>set hive.map.aggr=true;  
</code></pre>
<p>–用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。  </p>
<pre><code>set hive.map.aggr.hash.min.reduction=0.5;  
</code></pre>
<p>–用于检测源表是否适合map-side聚合的条数  </p>
<pre><code>set hive.groupby.mapaggr.checkinterval=100000;  
</code></pre>
<p>–map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。  </p>
<pre><code>set hive.map.aggr.hash.force.flush.memory.threshold=0.9;  
</code></pre>
<h3 id="HQL语法优化之分组聚合优化-（map-site）优化案例"><a href="#HQL语法优化之分组聚合优化-（map-site）优化案例" class="headerlink" title="HQL语法优化之分组聚合优化 （map-site）优化案例"></a>HQL语法优化之分组聚合优化 （map-site）优化案例</h3><pre><code>select
product_id,
count(*)
from order_detail
group by product_id;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/4.png" alt="启用map-site调优前后Explain执行计划对比">  </p>
<h4 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h4><p>开启map-side聚合，配置以下参数：  </p>
<pre><code>set hive.map.aggr=true;  

set hive.map.aggr.hash.min.reduction=0.5;   

set hive.groupby.mapaggr.checkinterval=100000;  

set hive.map.aggr.hash.force.flush.memory.threshold=0.9;  
</code></pre>
<h3 id="HQL语法优化之Join优化"><a href="#HQL语法优化之Join优化" class="headerlink" title="HQL语法优化之Join优化"></a>HQL语法优化之Join优化</h3><p>Join算法概述</p>
<p>Common Join : 常规join，不做优化    </p>
<p><img src="/2023/07/31/Hive-on-mr/5.png" alt="Common Join原理图">  </p>
<p>Map Join：<strong>适用于大表join小表</strong>，将小表数据缓存为hash table（内存表），然后扫描大表数据，这样在map端即可完成关联操作  </p>
<p><img src="/2023/07/31/Hive-on-mr/6.png" alt="Map Join原理图"></p>
<p>Bucket Map Join：<strong>适用于大表join大表</strong>  通过分桶对数据进行切分，让有限的内存缓存一部分分桶数据，再对另一个大表进行遍历操作     </p>
<pre><code> Bucket Map Join的使用要求：若能保证参与**join的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍**
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/7.png" alt="Bucket Map Join原理图"></p>
<p>SMB Map Join：<strong>适用于大表join大表</strong>，两个分桶之间的join实现原理为Sort Merge Join算法。前提条件是两个大表分桶数据都要排好序，这样就无需缓存在内存中，通过Sort Merge Join算法直接完成逐条遍历计算。  </p>
<pre><code>SMB Map Join的使用要求:参与join的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍    
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/8.png" alt="SMB Map Join原理图"></p>
<p>Bucket Map Join 和 SMB Map Join 的区别：  </p>
<p>1.SMB Map Join在Bucket Map join的基础上，要求分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段</p>
<p>2.两个分桶之间的join实现算法不一样  </p>
<pre><code>Bucket Map Join，两个分桶之间的join实现原理为Hash Join算法    

SMB Map Join，两个分桶之间的join实现原理为Sort Merge Join算法   
</code></pre>
<p>3.相较于Bucket Map Join，SMB Map Join对分桶大小是没有要求的,因为SMB Map Join不需要缓存数据  </p>
<h4 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h4><p>优化说明 </p>
<p><img src="/2023/07/31/Hive-on-mr/9.png" alt="Map join原理解析">   </p>
<p>寻找大表候选人：  </p>
<p>a inner join b时，a,b都可以作为大表候选人，只返回a,b都能连接上的数据  </p>
<p>A Left join  b时，只有b才可以作为大表候选人，这样才会遍历A表数据，输出A的所有数据  </p>
<p>A right join b时，只有A才可以作为大表候选人，这样才会遍历B表的数据，输出B的所有数据  </p>
<p>A full join b时，没有大表候选人，因为无论选a还是B作为大表候选人，都无法输出a和b的所有数据  </p>
<p>Conditionaltask：条件任务  </p>
<p>图中涉及到的参数如下：    </p>
<p>–启动Map Join自动转换  </p>
<pre><code>set hive.auto.convert.join=true;  
</code></pre>
<p>–一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的已知大小总和&lt;&#x3D;该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。 </p>
<pre><code>set hive.mapjoin.smalltable.filesize=250000;    
</code></pre>
<p>注意此处的内存大小参数与实际读取磁盘文件的大小是有差别的，考虑到磁盘文件解压缩，反序列化和对象信息，相同文件在内存中要比在磁盘中占用的空间放大大概10倍  </p>
<p>所以该参数设置为1G，就表明拿取磁盘中1G大小的文件，但内存需要占用10G  </p>
<p>实际生产中，将参数配置到hive-site等配置文件中。设置size参数时，通常配置为map端内存的1&#x2F;2 ~2&#x2F;3范围内作为缓存，记得size的值应该是map_memory*2&#x2F;3 的十分之一大小才行，否则磁盘文件读取到内存，会oom  </p>
<p>生产中，配置文件中的调优参数生效后，大部分sql语句性能提高了，如果极少部分任务还是慢sql，就需要单独调优，在语句中加入set参数的方式进行针对性局部调优  </p>
<p>–开启无条件转Map Join</p>
<pre><code>set hive.auto.convert.join.noconditionaltask=true;  
</code></pre>
<p>–无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和&lt;&#x3D;该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask.size=10000000;  
</code></pre>
<h5 id="Map-Join优化案例"><a href="#Map-Join优化案例" class="headerlink" title="Map Join优化案例"></a>Map Join优化案例</h5><pre><code>select  *
from order_detail od
join product_info product on od.product_id = product.id
join province_info province on od.province_id = province.id;  
</code></pre>
<p>优化前：设置 set hive.auto.convert.join&#x3D;true &#x3D; false  </p>
<p><img src="/2023/07/31/Hive-on-mr/10.png" alt="Map Join优化前">    </p>
<p>对参与关联的三张表进行分析，发现各自大小如下   </p>
<p><img src="/2023/07/31/Hive-on-mr/11.png" alt="参与关联的三张表大小"></p>
<h6 id="Map-Join方案一："><a href="#Map-Join方案一：" class="headerlink" title="Map Join方案一："></a>Map Join方案一：</h6><p>启用Map Join自动转换 </p>
<pre><code>set hive.auto.convert.join=true;  
</code></pre>
<p>不使用无条件转Map Join  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask=false;  
</code></pre>
<p>调整hive.mapjoin.smalltable.filesize参数，使其大于等于product_info  </p>
<pre><code>set hive.mapjoin.smalltable.filesize=25285707;  
</code></pre>
<p>这样可保证将两个Common Join operator均可转为Map Join operator，并保留Common Join作为后备计划，保证计算任务的稳定  </p>
<p><img src="/2023/07/31/Hive-on-mr/12.png" alt="Map Jion优化方案一">  </p>
<h6 id="Map-Join方案二："><a href="#Map-Join方案二：" class="headerlink" title="Map Join方案二："></a>Map Join方案二：</h6><p>启用Map Join自动转换  </p>
<pre><code>set hive.auto.convert.join=true;  
</code></pre>
<p>使用无条件转Map Join  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask=true;  
</code></pre>
<p>调整hive.auto.convert.join.noconditionaltask.size参数，使其大于等于product_info和province_info之和  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask.size=25286076;  
</code></pre>
<p>这样可直接将两个Common Join operator转为两个Map Join operator，并且由于两个Map Join operator的小表大小之和小于等于hive.auto.convert.join.noconditionaltask.size，故两个Map Join operator任务可合并为同一个。这个方案计算效率最高，但需要的内存也是最多的</p>
<p><img src="/2023/07/31/Hive-on-mr/13.png" alt="Map Jion优化方案二"> </p>
<h6 id="Map-Join方案三："><a href="#Map-Join方案三：" class="headerlink" title="Map Join方案三："></a>Map Join方案三：</h6><p>启用Map Join自动转换    </p>
<pre><code>set hive.auto.convert.join=true;   
</code></pre>
<p>使用无条件转Map Join  </p>
<pre><code>set hive.auto.convert.join.noconditionaltask=true;    
</code></pre>
<p>调整hive.auto.convert.join.noconditionaltask.size参数，使其等于product_info</p>
<pre><code>set hive.auto.convert.join.noconditionaltask.size=25285707;  
</code></pre>
<p>这样可直接将两个Common Join operator转为Map Join operator，但不会将两个Map Join的任务合并。该方案计算效率比方案二低，但需要的内存也更少</p>
<p><img src="/2023/07/31/Hive-on-mr/14.png" alt="Map Join优化方案三"></p>
<h4 id="Bucket-Map-Join"><a href="#Bucket-Map-Join" class="headerlink" title="Bucket Map Join"></a>Bucket Map Join</h4><p>Bucket Map Join不支持自动转换，发须通过用户在SQL语句中提供如下Hint提示，并配置如下相关参数，方可使用  </p>
<pre><code>select /*+ mapjoin(ta) */
ta.id,
tb.id
from table_a ta
join table_b tb on ta.id=tb.id; 
</code></pre>
<p>相关参数  </p>
<p>–关闭cbo优化，cbo会导致hint信息被忽略  </p>
<pre><code>set hive.cbo.enable=false;
</code></pre>
<p>–map join hint默认会被忽略(因为已经过时)，需将如下参数设置为false  </p>
<pre><code>set hive.ignore.mapjoin.hint=false;  
</code></pre>
<p>–启用bucket map join优化功能  </p>
<pre><code>set hive.optimize.bucketmapjoin = true;    
</code></pre>
<h5 id="Bucket-Map-Join优化案例"><a href="#Bucket-Map-Join优化案例" class="headerlink" title="Bucket Map Join优化案例"></a>Bucket Map Join优化案例</h5><pre><code>select
    *
from(
    select
        *
    from order_detail
    where dt=&#39;2020-06-14&#39;
)od
join(
    select
        	*
    from payment_detail
    where dt=&#39;2020-06-14&#39;
)pd
on od.id=pd.order_detail_id;  
</code></pre>
<h6 id="Bucket-Map-Join优化前"><a href="#Bucket-Map-Join优化前" class="headerlink" title="Bucket Map Join优化前"></a>Bucket Map Join优化前</h6><pre><code>set hive.auto.convert.join=false;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/15.png" alt="Bucket Map Join优化前"></p>
<h6 id="Bucket-Map-Join优化思路"><a href="#Bucket-Map-Join优化思路" class="headerlink" title="Bucket Map Join优化思路"></a>Bucket Map Join优化思路</h6><p>经分析，参与join的两张表，数据量如下  </p>
<p>order_detail	  1176009934（约1122M）<br>payment_detail	  334198480（约319M）  </p>
<p>可以认为是大表join大表，可尝试采用Bucket Map Join优化方案  </p>
<p>首先需要依据源表创建两个分桶表，order_detail建议分16个bucket  </p>
<p>payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段  </p>
<p>–订单表 </p>
<pre><code>hive (default)&gt; 
drop table if exists order_detail_bucketed;
create table order_detail_bucketed(
id           string comment &#39;订单id&#39;,
user_id      string comment &#39;用户id&#39;,
product_id   string comment &#39;商品id&#39;,
province_id  string comment &#39;省份id&#39;,
create_time  string comment &#39;下单时间&#39;,
product_num  int comment &#39;商品件数&#39;,
total_amount decimal(16, 2) comment &#39;下单金额&#39;
)
clustered by (id) into 16 buckets
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<p>–支付表  </p>
<pre><code>hive (default)&gt; 
drop table if exists payment_detail_bucketed;
create table payment_detail_bucketed(
id              string comment &#39;支付id&#39;,
order_detail_id string comment &#39;订单明细id&#39;,
user_id         string comment &#39;用户id&#39;,
payment_time    string comment &#39;支付时间&#39;,
total_amount    decimal(16, 2) comment &#39;支付金额&#39;
)
clustered by (order_detail_id) into 8 buckets
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<p>然后向两个分桶表导入数据。  </p>
<p>–订单表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table order_detail_bucketed
select
id,
user_id,
product_id,
province_id,
create_time,
product_num,
total_amount   
from order_detail
where dt=&#39;2023-07-28&#39;;
</code></pre>
<p>–分桶表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table payment_detail_bucketed
select
id,
order_detail_id,
user_id,
payment_time,
total_amount
from payment_detail
where dt=&#39;2020-07-28&#39;;
</code></pre>
<p>然后设置以下参数：  </p>
<p>–关闭cbo优化，cbo会导致hint信息被忽略，需将如下参数修改为false  </p>
<pre><code>set hive.cbo.enable=false;  
</code></pre>
<p>–map join hint默认会被忽略(因为已经过时)，需将如下参数修改为false  </p>
<pre><code>set hive.ignore.mapjoin.hint=false;  
</code></pre>
<p>–启用bucket map join优化功能,默认不启用，需将如下参数修改为true </p>
<pre><code>set hive.optimize.bucketmapjoin = true;
</code></pre>
<p>最后在重写SQL语句，如下：  </p>
<pre><code>select /*+ mapjoin(pd) */
    *
from order_detail_bucketed od
join payment_detail_bucketed pd on od.id = pd.order_detail_id; 
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/16.png">  </p>
<p>由于bucket map join和map join的执行计划非常像，如何确定该执行计划是否属于bucket map join ? </p>
<p><img src="/2023/07/31/Hive-on-mr/17.png"></p>
<h4 id="Sort-Merge-Bucket-Map-Join"><a href="#Sort-Merge-Bucket-Map-Join" class="headerlink" title="Sort Merge Bucket Map Join"></a>Sort Merge Bucket Map Join</h4><p>优化说明  </p>
<p>Sort Merge Bucket Map Join有两种触发方式，包括Hint提示和自动转换。Hint提示已过时，不推荐使用。下面是自动转换的相关参数:  </p>
<p>–启动Sort Merge Bucket Map Join优化  </p>
<pre><code>set hive.optimize.bucketmapjoin.sortedmerge=true;  
</code></pre>
<p>–使用自动转换SMB Join  </p>
<pre><code>set hive.auto.convert.sortmerge.join=true;   
</code></pre>
<h5 id="Sort-Merge-Bucket-Map-Join优化案例"><a href="#Sort-Merge-Bucket-Map-Join优化案例" class="headerlink" title="Sort Merge Bucket Map Join优化案例"></a>Sort Merge Bucket Map Join优化案例</h5><pre><code>select
        *
from(
    select
            *
    from order_detail
    where dt=&#39;2020-06-14&#39;
)od
join(
    select
        *
    from payment_detail
    where dt=&#39;2020-06-14&#39;
)pd
on od.id=pd.order_detail_id;  
</code></pre>
<h5 id="Sort-Merge-Bucket-Map-Join优化思路"><a href="#Sort-Merge-Bucket-Map-Join优化思路" class="headerlink" title="Sort Merge Bucket Map Join优化思路"></a>Sort Merge Bucket Map Join优化思路</h5><p>order_detail	1176009934（约1122M）<br>payment_detail	334198480（约319M）</p>
<p>两张表都相对较大，除了可以考虑采用Bucket Map Join算法，还可以考虑SMB Join。相较于Bucket Map Join，SMB Map Join对分桶大小是没有要求的    </p>
<p>首先需要依据源表创建两个的有序的分桶表，order_detail建议分16个bucket，payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段和排序字段</p>
<p>–订单表  </p>
<pre><code>hive (default)&gt; 
drop table if exists order_detail_sorted_bucketed;
create table order_detail_sorted_bucketed(
id           string comment &#39;订单id&#39;,
user_id      string comment &#39;用户id&#39;,
product_id   string comment &#39;商品id&#39;,
province_id  string comment &#39;省份id&#39;,
create_time  string comment &#39;下单时间&#39;,
product_num  int comment &#39;商品件数&#39;,
total_amount decimal(16, 2) comment &#39;下单金额&#39;
)	
clustered by (id) sorted by(id) into 16 buckets
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<p>–支付表  </p>
<pre><code>hive (default)&gt; 
drop table if exists payment_detail_sorted_bucketed;
create table payment_detail_sorted_bucketed(
id              string comment &#39;支付id&#39;,
order_detail_id string comment &#39;订单明细id&#39;,
user_id         string comment &#39;用户id&#39;,
payment_time    string comment &#39;支付时间&#39;,
total_amount    decimal(16, 2) comment &#39;支付金额&#39;
)
clustered by (order_detail_id) sorted by(order_detail_id) into 8 buckets
row format delimited fields terminated by &#39;\t&#39;;  
</code></pre>
<p>然后向两个分桶表导入数据。  </p>
<p>–订单表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table order_detail_sorted_bucketed
select
id,
user_id,
product_id,
province_id,
create_time,
product_num,
total_amount   
from order_detail
where dt=&#39;2023-07-28&#39;;
</code></pre>
<p>–分桶表  </p>
<pre><code>hive (default)&gt; 
insert overwrite table payment_detail_sorted_bucketed
select
id,
order_detail_id,
user_id,
payment_time,
total_amount
from payment_detail
where dt=&#39;2023-07-28&#39;;  
</code></pre>
<p>–启动Sort Merge Bucket Map Join优化  </p>
<pre><code>set hive.optimize.bucketmapjoin.sortedmerge=true;  
</code></pre>
<p>–使用自动转换SMB Join  </p>
<pre><code>set hive.auto.convert.sortmerge.join=true; 
</code></pre>
<p>最后在重写SQL语句，如下：  </p>
<pre><code>hive (default)&gt; 
select
    *
from order_detail_sorted_bucketed od
join payment_detail_sorted_bucketed pd
on od.id = pd.order_detail_id;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/18.png">  </p>
<h3 id="HQL语法优化之数据倾斜"><a href="#HQL语法优化之数据倾斜" class="headerlink" title="HQL语法优化之数据倾斜"></a>HQL语法优化之数据倾斜</h3><p>数据倾斜概述</p>
<p>数据倾斜问题，通常是指参与计算的数据分布不均，即某个key或者某些key的数据量远超其他key，导致在shuffle阶段，大量相同key的数据被发往同一个Reduce，进而导致该Reduce所需的时间远超其他Reduce，成为整个任务的瓶颈</p>
<p>Hive中的数据倾斜常出现在<strong>分组聚合</strong>和<strong>join操作</strong>的场景中  </p>
<h4 id="分组聚合导致的数据倾斜-（Map-Side聚合-Skew-GroupBy优化）"><a href="#分组聚合导致的数据倾斜-（Map-Side聚合-Skew-GroupBy优化）" class="headerlink" title="分组聚合导致的数据倾斜 （Map-Side聚合&#x2F;Skew-GroupBy优化）"></a>分组聚合导致的数据倾斜 （Map-Side聚合&#x2F;Skew-GroupBy优化）</h4><p>如果group by分组字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题   </p>
<p>由分组聚合导致的数据倾斜问题，有以下两种解决思路    </p>
<h5 id="Map-Side聚合"><a href="#Map-Side聚合" class="headerlink" title="Map-Side聚合"></a>Map-Side聚合</h5><p>开启Map-Side聚合后，数据会现在Map端完成部分聚合工作。这样一来即便原始数据是倾斜的，经过Map端的初步聚合后，发往Reduce的数据也就不再倾斜了，最佳状态下，Map-端聚合能完全屏蔽数据倾斜问题。  </p>
<p>相关参数如下：  </p>
<pre><code>set hive.map.aggr=true;

set hive.map.aggr.hash.min.reduction=0.5;  

set hive.groupby.mapaggr.checkinterval=100000;  

set hive.map.aggr.hash.force.flush.memory.threshold=0.9;   
</code></pre>
<h5 id="Skew-GroupBy优化"><a href="#Skew-GroupBy优化" class="headerlink" title="Skew-GroupBy优化"></a>Skew-GroupBy优化</h5><p>Skew-GroupBy的原理是启动两个MR任务，第一个MR按照随机数分区，将数据分散发送到Reduce，完成部分聚合，第二个MR按照分组字段分区，完成最终聚合  </p>
<p>–启用分组聚合数据倾斜优化  </p>
<pre><code>set hive.groupby.skewindata=true;  
</code></pre>
<p>–关闭map-side聚合  </p>
<pre><code>set hive.map.aggr=false;    
</code></pre>
<h4 id="Join导致的数据倾斜"><a href="#Join导致的数据倾斜" class="headerlink" title="Join导致的数据倾斜"></a>Join导致的数据倾斜</h4><p>如果关联字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题。  </p>
<p>由join导致的数据倾斜问题，有如下三种解决方案：<br>map join<br>skew join<br>调整sql，通过sql语句将倾斜数据打散成更小的块  </p>
<h5 id="map-join（适用于大表join小表时发生数据倾斜的场景）"><a href="#map-join（适用于大表join小表时发生数据倾斜的场景）" class="headerlink" title="map join（适用于大表join小表时发生数据倾斜的场景）"></a>map join（<strong>适用于大表join小表时发生数据倾斜的场景</strong>）</h5><p> 使用map join算法，join操作仅在map端就能完成，没有shuffle操作，没有reduce阶段，自然不会产生reduce端的数据倾斜     </p>
<p>相关参数参照上文中map join部分内容  </p>
<pre><code>set hive.auto.convert.join=true;  
set hive.mapjoin.smalltable.filesize=250000;  
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000000;
</code></pre>
<h5 id="skew-join（对两表中倾斜的key的数据量有要求）"><a href="#skew-join（对两表中倾斜的key的数据量有要求）" class="headerlink" title="skew join（对两表中倾斜的key的数据量有要求）"></a>skew join（<strong>对两表中倾斜的key的数据量有要求</strong>）</h5><p>skew join的原理是，为倾斜的大key单独启动一个map join任务进行计算，其余key进行正常的common join  </p>
<p><img src="/2023/07/31/Hive-on-mr/19.png" alt="Skew Join原理图">  </p>
<p>相关参数如下：  </p>
<p>–启用skew join优化  </p>
<pre><code>set hive.optimize.skewjoin=true;  
</code></pre>
<p>–触发skew join的阈值，若某个key的行数超过该参数值，则触发  </p>
<pre><code>set hive.skewjoin.key=100000;    
</code></pre>
<p>对两表中倾斜的key的数据量有要求，要求一张表中的倾斜key的数据量比较小（方便走mapjoin）  </p>
<h5 id="SQL打散"><a href="#SQL打散" class="headerlink" title="SQL打散"></a>SQL打散</h5><pre><code>select
    *
from(
    select --打散操作
    concat(id,&#39;_&#39;,cast(rand()*2 as int)) id,
    value
from A
)ta
join(
    select --扩容操作
        concat(id,&#39;_&#39;,0) id,
        value
    from B
    union all
    select
        concat(id,&#39;_&#39;,1) id,
           value
    from B
)tb
on ta.id=tb.id;  
</code></pre>
<p><img src="/2023/07/31/Hive-on-mr/20.png" alt="SQL打散"></p>
<h4 id="HQL语法优化之任务并行度"><a href="#HQL语法优化之任务并行度" class="headerlink" title="HQL语法优化之任务并行度"></a>HQL语法优化之任务并行度</h4><p>对于一个分布式的计算任务而言，设置一个合适的并行度十分重要。Hive的计算任务由MapReduce完成，故并行度的调整需要分为Map端和Reduce端</p>
<h5 id="Map端并行度"><a href="#Map端并行度" class="headerlink" title="Map端并行度"></a>Map端并行度</h5><p>Map端的并行度，也就是Map的个数。是由输入文件的切片数决定的。一般情况下，Map端的并行度无需手动调整  </p>
<p>以下特殊情况可考虑调整map端并行度：  </p>
<p>1）查询的表中存在大量小文件    </p>
<pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  
</code></pre>
<p>2）map端有复杂的查询逻辑  </p>
<p>在计算资源充足的情况下，可考虑增大map端的并行度，令map task多一些，每个map task计算的数据少一些  </p>
<p>–一个切片的最大值  </p>
<pre><code>set mapreduce.input.fileinputformat.split.maxsize=256000000;  
</code></pre>
<h5 id="Reduce端并行度"><a href="#Reduce端并行度" class="headerlink" title="Reduce端并行度"></a>Reduce端并行度</h5><p>Reduce端的并行度，也就是Reduce个数。相对来说，更需要关注。Reduce端的并行度，可由用户自己指定，也可由Hive自行根据该MR Job输入的文件大小进行估算    </p>
<p>Reduce端的并行度的相关参数如下：  </p>
<p>–指定Reduce端并行度，默认值为-1，表示用户未指定  </p>
<pre><code>set mapreduce.job.reduces;  
</code></pre>
<p>–Reduce端并行度最大值  </p>
<pre><code>set hive.exec.reducers.max;  
</code></pre>
<p>–单个Reduce Task计算的数据量，用于估算Reduce并行度  </p>
<pre><code>set hive.exec.reducers.bytes.per.reducer;
</code></pre>
<h6 id="估算逻辑"><a href="#估算逻辑" class="headerlink" title="估算逻辑"></a>估算逻辑</h6><p>假设Job输入的文件大小为totalInputBytes  </p>
<p>参数hive.exec.reducers.bytes.per.reducer的值为bytesPerReducer。  </p>
<p>参数hive.exec.reducers.max的值为maxReducers。  </p>
<p>则Reduce端的并行度为：  </p>
<pre><code>min(ceil(totalInputBytes/bytesPerReducer),maxReducers)  
</code></pre>
<h4 id="HQL语法优化之小文件合并"><a href="#HQL语法优化之小文件合并" class="headerlink" title="HQL语法优化之小文件合并"></a>HQL语法优化之小文件合并</h4><p>Map端输入文件合并   </p>
<p>合并Map端输入的小文件，是指将多个小文件划分到一个切片中，进而由一个Map Task去处理。目的是防止为单个小文件启动一个Map Task，浪费计算资源  </p>
<p>–可将多个小文件切片，合并为一个切片，进而由一个map任务处理  </p>
<pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  
</code></pre>
<p>Reduce输出文件合并  </p>
<p>合并Reduce端输出的小文件，是指将多个小文件合并成大文件。目的是减少HDFS小文件数量。其原理是根据计算任务输出文件的平均大小进行判断，若符合条件，则单独启动一个额外的任务进行合并  </p>
<p>–开启合并map only任务输出的小文件  </p>
<pre><code>set hive.merge.mapfiles=true;
</code></pre>
<p>–开启合并map reduce任务输出的小文件  </p>
<pre><code>set hive.merge.mapredfiles=true;
</code></pre>
<p>–合并后的文件大小</p>
<pre><code>set hive.merge.size.per.task=256000000;
</code></pre>
<p>–触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并 </p>
<pre><code>set hive.merge.smallfiles.avgsize=16000000;  
</code></pre>
<h3 id="其他优化"><a href="#其他优化" class="headerlink" title="其他优化"></a>其他优化</h3><h4 id="1-CBO优化"><a href="#1-CBO优化" class="headerlink" title="1.CBO优化"></a>1.CBO优化</h4><p>CBO是指Cost based Optimizer，即基于计算成本的优化</p>
<p>在Hive中，计算成本模型考虑到了：数据的行数、CPU、本地IO、HDFS IO、网络IO等方面  </p>
<p>目前CBO在hive的MR引擎下主要用于join的优化，例如多表join的join顺序  </p>
<p>–是否启用cbo优化   </p>
<pre><code>set hive.cbo.enable=true;    
</code></pre>
<h4 id="2-谓词下推"><a href="#2-谓词下推" class="headerlink" title="2.谓词下推"></a>2.谓词下推</h4><p>谓词下推（predicate pushdown）是指，尽量将过滤操作前移，以减少后续计算步骤的数据量</p>
<p>–是否启动谓词下推（predicate pushdown）优化  </p>
<pre><code>set hive.optimize.ppd = true;  
</code></pre>
<p>CBO优化也会完成一部分的谓词下推优化工作，因为在执行计划中，谓词越靠前，整个计划的计算成本就会越低</p>
<h4 id="3-矢量化查询"><a href="#3-矢量化查询" class="headerlink" title="3.矢量化查询"></a>3.矢量化查询</h4><p>Hive的矢量化查询优化，依赖于CPU的矢量化计算，CPU的矢量化计算的基本原理如下图  </p>
<p><img src="/2023/07/31/Hive-on-mr/21.png" alt="矢量化计算原理">  </p>
<pre><code>set hive.vectorized.execution.enabled=true;  
</code></pre>
<p>若执行计划中，出现“Execution mode: vectorized”字样，即表明使用了矢量化计算。  </p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution#VectorizedQueryExecution-Limitations">矢量化计算官方文档</a> </p>
<h4 id="4-Fetch抓取"><a href="#4-Fetch抓取" class="headerlink" title="4.Fetch抓取"></a>4.Fetch抓取</h4><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算  </p>
<p>Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果到控制台</p>
<p>–是否在特定场景转换为fetch 任务  </p>
<p>–设置为none表示不转换  </p>
<p>–设置为minimal表示支持select *，分区字段过滤，Limit等  </p>
<p>–设置为more表示支持select 任意字段,包括函数，过滤，和limit等  </p>
<pre><code>set hive.fetch.task.conversion=more;  
</code></pre>
<h4 id="5-本地模式（不上yarn）"><a href="#5-本地模式（不上yarn）" class="headerlink" title="5.本地模式（不上yarn）"></a>5.本地模式（不上yarn）</h4><p>Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短</p>
<p>–开启自动转换为本地模式  </p>
<pre><code>set hive.exec.mode.local.auto=true;  
</code></pre>
<p>–设置local MapReduce的最大输入数据量，当输入数据量小于这个值时采用local  MapReduce的方式，默认为134217728，即128M  </p>
<pre><code>set hive.exec.mode.local.auto.inputbytes.max=50000000;
</code></pre>
<p>–设置local MapReduce的最大输入文件个数，当输入文件个数小于这个值时采用local MapReduce的方式，默认为4  </p>
<pre><code>set hive.exec.mode.local.auto.input.files.max=10;
</code></pre>
<h4 id="6-并行执行"><a href="#6-并行执行" class="headerlink" title="6.并行执行"></a>6.并行执行</h4><p>Hive会将一个SQL语句转化成一个或者多个Stage，每个Stage对应一个MR Job。默认情况下，Hive同时只会执行一个Stage。但是某SQL语句可能会包含多个Stage，但这多个Stage可能并非完全互相依赖，也就是说有些Stage是可以并行执行的。此处提到的并行执行就是指这些Stage的并行执行   </p>
<p>–启用并行执行优化  </p>
<pre><code>set hive.exec.parallel=true;       
</code></pre>
<p>–同一个sql允许最大并行度，默认为8  </p>
<pre><code>set hive.exec.parallel.thread.number=8;   
</code></pre>
<h4 id="7-严格模式"><a href="#7-严格模式" class="headerlink" title="7.严格模式"></a>7.严格模式</h4><p>Hive可以通过设置某些参数防止危险操作：   </p>
<p>1）分区表不使用分区过滤  </p>
<p>将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行  </p>
<p>2）使用order by没有limit过滤   </p>
<p>将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句    </p>
<p>3）笛卡尔积    </p>
<p>将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/31/hive-udf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/31/hive-udf/" class="post-title-link" itemprop="url">hive-udf</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-31 11:40:34" itemprop="dateCreated datePublished" datetime="2023-07-31T11:40:34+08:00">2023-07-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:03:22" itemprop="dateModified" datetime="2023-08-11T13:03:22+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="看似寻常最奇崛，成如容易却艰辛"><a href="#看似寻常最奇崛，成如容易却艰辛" class="headerlink" title="看似寻常最奇崛，成如容易却艰辛"></a>看似寻常最奇崛，成如容易却艰辛</h1><p><img src="/2023/07/31/hive-udf/2.png"></p>
<h1 id="Hive自定义UDF函数案例"><a href="#Hive自定义UDF函数案例" class="headerlink" title="Hive自定义UDF函数案例"></a>Hive自定义UDF函数案例</h1><h2 id="0）需求"><a href="#0）需求" class="headerlink" title="0）需求"></a>0）需求</h2><p>自定义一个UDF实现计算给定基本数据类型的长度，例如：  </p>
<pre><code>hive(default)&gt; select my_len(&quot;abcd&quot;);  
4  
</code></pre>
<h2 id="1）创建一个Maven工程Hive"><a href="#1）创建一个Maven工程Hive" class="headerlink" title="1）创建一个Maven工程Hive"></a>1）创建一个Maven工程Hive</h2><h2 id="2）导入依赖"><a href="#2）导入依赖" class="headerlink" title="2）导入依赖"></a>2）导入依赖</h2><pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
        &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
        &lt;version&gt;3.1.3&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;  
</code></pre>
<h2 id="3）创建一个类"><a href="#3）创建一个类" class="headerlink" title="3）创建一个类"></a>3）创建一个类</h2><pre><code>package com.atguigu.hive.udf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

/* 我们需计算一个要给定基本数据类型的长度 */	
public class MyUDF extends GenericUDF &#123;
    /*** 判断传进来的参数的类型和长度* 约定返回的数据类型*/  
    @Override
    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123;

    if (arguments.length !=1) &#123;
        throw  new UDFArgumentLengthException(&quot;please give me  only one arg&quot;);
    &#125;

    if (!arguments[0].getCategory().equals(ObjectInspector.Category.PRIMITIVE))&#123;
        throw  new UDFArgumentTypeException(1, &quot;i need primitive type arg&quot;);
    &#125;

    return PrimitiveObjectInspectorFactory.javaIntObjectInspector;
&#125;

/**
 * 解决具体逻辑的
 */
@Override
public Object evaluate(DeferredObject[] arguments) throws HiveException &#123;

    Object o = arguments[0].get();
    if(o==null)&#123;
        return 0;
    &#125;

    return o.toString().length();
&#125;

@Override
// 用于获取解释的字符串
public String getDisplayString(String[] children) &#123;
    return &quot;&quot;;
&#125;
&#125;
</code></pre>
<h2 id="4）创建临时函数"><a href="#4）创建临时函数" class="headerlink" title="4）创建临时函数"></a>4）创建临时函数</h2><h3 id="（1）打成jar包上传到服务器-opt-module-hive-datas-myudf-jar"><a href="#（1）打成jar包上传到服务器-opt-module-hive-datas-myudf-jar" class="headerlink" title="（1）打成jar包上传到服务器&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;myudf.jar"></a>（1）打成jar包上传到服务器&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;myudf.jar</h3><h3 id="（2）将jar包添加到hive的classpath，临时生效"><a href="#（2）将jar包添加到hive的classpath，临时生效" class="headerlink" title="（2）将jar包添加到hive的classpath，临时生效"></a>（2）将jar包添加到hive的classpath，临时生效</h3><pre><code>hive (default)&gt; add jar /opt/module/hive/datas/myudf.jar;
</code></pre>
<h3 id="（3）创建临时函数与开发好的java-class关联"><a href="#（3）创建临时函数与开发好的java-class关联" class="headerlink" title="（3）创建临时函数与开发好的java class关联"></a>（3）创建临时函数与开发好的java class关联</h3><pre><code>hive (default)&gt; 
create temporary function my_len 
as &quot;com.atguigu.hive.udf.MyUDF&quot;;
</code></pre>
<h3 id="（4）即可在hql中使用自定义的临时函数"><a href="#（4）即可在hql中使用自定义的临时函数" class="headerlink" title="（4）即可在hql中使用自定义的临时函数"></a>（4）即可在hql中使用自定义的临时函数</h3><pre><code>hive (default)&gt; 
select 
ename,
my_len(ename) ename_len 
from emp;
</code></pre>
<h3 id="（5）删除临时函数"><a href="#（5）删除临时函数" class="headerlink" title="（5）删除临时函数"></a>（5）删除临时函数</h3><pre><code>hive (default)&gt; drop temporary function my_len;
注意：临时函数只跟会话有关系，跟库没有关系。只要创建临时函数的会话不断，在当前会话下，任意一个库都可以使用，其他会话全都不能使用。
</code></pre>
<h2 id="5）创建永久函数"><a href="#5）创建永久函数" class="headerlink" title="5）创建永久函数"></a>5）创建永久函数</h2><h3 id="（1）创建永久函数"><a href="#（1）创建永久函数" class="headerlink" title="（1）创建永久函数"></a>（1）创建永久函数</h3><p>注意：因为add jar本身也是临时生效，所以在创建永久函数的时候，需要制定路径（并且因为元数据的原因，这个路径还得是HDFS上的路径）</p>
<pre><code>hive (default)&gt; 
create function my_len2 
as &quot;com.atguigu.hive.udf.MyUDF&quot; 
using jar &quot;hdfs://hadoop102:8020/udf/myudf.jar&quot;;
</code></pre>
<h3 id="（2）即可在hql中使用自定义的永久函数"><a href="#（2）即可在hql中使用自定义的永久函数" class="headerlink" title="（2）即可在hql中使用自定义的永久函数"></a>（2）即可在hql中使用自定义的永久函数</h3><pre><code>hive (default)&gt; 
select 
ename,
my_len2(ename) ename_len 
from emp;
</code></pre>
<h3 id="（3）删除永久函数"><a href="#（3）删除永久函数" class="headerlink" title="（3）删除永久函数"></a>（3）删除永久函数</h3><pre><code>hive (default)&gt; drop function my_len2;  
</code></pre>
<p>注意：永久函数跟会话没有关系，创建函数的会话断了以后，其他会话也可以使用。  </p>
<p>永久函数创建的时候，在函数名之前需要自己加上库名，如果不指定库名的话，会默认把当前库的库名给加上。  </p>
<p>永久函数使用的时候，需要在指定的库里面操作，或者在其他库里面使用的话加上，库名.函数名。  </p>
<p>#啊席八，Hive自定义UDF函数你都学会了~！</p>
<p>#啪啪啪~ 你很厉害喔~</p>
<p><img src="/2023/07/31/hive-udf/1.png" alt="歪嘴猫"></p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/hive_learn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/hive_learn/" class="post-title-link" itemprop="url">hive学习笔记</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-30 18:56:09" itemprop="dateCreated datePublished" datetime="2023-07-30T18:56:09+08:00">2023-07-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 13:08:10" itemprop="dateModified" datetime="2023-08-11T13:08:10+08:00">2023-08-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1g84y147sX?p=78&vd_source=326368ccf929b51406b17a280e53c102">尚硅谷大数据Hive 3.x教程全新升级版（基于hive3.1.3）</a></p>
<h1 id="手握日月摘星辰，世间无我这般人！"><a href="#手握日月摘星辰，世间无我这般人！" class="headerlink" title="手握日月摘星辰，世间无我这般人！"></a>手握日月摘星辰，世间无我这般人！</h1><p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1vdjJdb5hZtWMDK6hoH1R5g">https://pan.baidu.com/s/1vdjJdb5hZtWMDK6hoH1R5g</a><br>提取码：uce2   </p>
<p><img src="/2023/07/30/hive_learn/8.png">  </p>
<h1 id="一：Hive的基础知识"><a href="#一：Hive的基础知识" class="headerlink" title="一：Hive的基础知识"></a>一：Hive的基础知识</h1><h2 id="1-什么是Hive？"><a href="#1-什么是Hive？" class="headerlink" title="1.什么是Hive？"></a>1.什么是Hive？</h2><p>Hive是由Facebook开源，基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能</p>
<h2 id="2-Hive本质"><a href="#2-Hive本质" class="headerlink" title="2.Hive本质"></a>2.Hive本质</h2><p>Hive是一个Hadoop客户端，用于将HQL（Hive SQL）转化成MapReduce程序。<br>（1）Hive中每张表的数据存储在HDFS<br>（2）Hive分析数据底层的实现是MapReduce（也可配置为Spark或者Tez）<br>（3）执行程序运行在Yarn上</p>
<h2 id="3-用户接口：Client"><a href="#3-用户接口：Client" class="headerlink" title="3.用户接口：Client"></a>3.用户接口：Client</h2><p>CLI（command-line interface）、JDBC&#x2F;ODBC  </p>
<h3 id="JDBC和ODBC的区别"><a href="#JDBC和ODBC的区别" class="headerlink" title="JDBC和ODBC的区别:"></a>JDBC和ODBC的区别:</h3><p>（1）JDBC的移植性比ODBC好 </p>
<p>（2）两者使用的语言不同，JDBC在Java编程时使用，ODBC一般在C&#x2F;C++编程时使用  </p>
<h2 id="4-元数据：Metastore"><a href="#4-元数据：Metastore" class="headerlink" title="4:元数据：Metastore"></a>4:元数据：Metastore</h2><p>元数据包括：数据库（默认是default）、表名、表的拥有者、列&#x2F;分区字段、表的类型（是否是外部表）、表的数据所在目录等  </p>
<p>默认存储在自带的derby数据库中，由于derby数据库只支持单客户端访问，生产环境中为了多人开发，推荐使用MySQL存储Metastore  </p>
<h2 id="5-hive的存储和计算"><a href="#5-hive的存储和计算" class="headerlink" title="5.hive的存储和计算"></a>5.hive的存储和计算</h2><p>使用HDFS进行存储，可以选择MapReduce&#x2F;Tez&#x2F;Spark进行计算</p>
<h2 id="6-hiveserver2服务"><a href="#6-hiveserver2服务" class="headerlink" title="6.hiveserver2服务"></a>6.hiveserver2服务</h2><p>Hive的hiveserver2服务的作用是提供jdbc&#x2F;odbc接口，为用户提供远程访问Hive数据的功能，例如用户期望在个人电脑中访问远程服务中的Hive数据，就需要用到Hiveserver2  </p>
<h2 id="7-用户说明"><a href="#7-用户说明" class="headerlink" title="7.用户说明"></a>7.用户说明</h2><p>在远程访问Hive数据时，客户端并未直接访问Hadoop集群，而是由Hivesever2代理访问,那么访问Hadoop集群的用户身份是谁？  </p>
<p>具体是谁，由Hiveserver2的hive.server2.enable.doAs参数决定，该参数的含义是是否启用Hiveserver2用户模拟的功能  </p>
<p>若启用，则Hiveserver2会模拟成客户端的登录用户去访问Hadoop集群的数据，不启用，则Hivesever2会直接使用启动用户访问Hadoop集群数据  </p>
<p>默认为开启</p>
<p>生产环境，推荐开启用户模拟功能，因为开启后才能保证各用户之间的权限隔离</p>
<p>hivesever2的模拟用户功能，依赖于Hadoop提供的proxy user（代理用户功能），只有Hadoop中的代理用户才能模拟其他用户的身份访问Hadoop集群。因此，需要将hiveserver2的启动用户设置为Hadoop的代理用户 </p>
<h2 id="8-metastore服务"><a href="#8-metastore服务" class="headerlink" title="8.metastore服务"></a>8.metastore服务</h2><p>Hive的metastore服务的作用是为Hive CLI或者Hiveserver2提供元数据访问接口  </p>
<h2 id="9-metastore运行模式"><a href="#9-metastore运行模式" class="headerlink" title="9.metastore运行模式"></a>9.metastore运行模式</h2><p>metastore有两种运行模式，分别为嵌入式模式和独立服务模式</p>
<p><img src="/2023/07/30/hive_learn/1.png" alt="&quot;metastore运行模式&quot;"></p>
<p>生产环境中，不推荐使用嵌入式模式  </p>
<p>（1）嵌入式模式下，每个Hive CLI都需要直接连接元数据库，当Hive CLI较多时，数据库压力会比较大。  </p>
<p>（2）每个客户端都需要用户元数据库的读写权限，元数据库的安全得不到很好的保证</p>
<h2 id="10-编写Hive服务启动脚本"><a href="#10-编写Hive服务启动脚本" class="headerlink" title="10.编写Hive服务启动脚本"></a>10.编写Hive服务启动脚本</h2><p>nohup：放在命令开头，表示不挂起，也就是关闭终端进程也继续保持运行状态  </p>
<p>&#x2F;dev&#x2F;null：是Linux文件系统中的一个文件，被称为黑洞，所有写入该文件的内容都会被自动丢弃  </p>
<p>2&gt;&amp;1：表示将错误重定向到标准输出上  </p>
<p>&amp;：放在命令结尾，表示后台运行  </p>
<p>一般会组合使用：nohup  [xxx命令操作]&gt; file  2&gt;&amp;1 &amp;，表示将xxx命令运行的结果输出到file中，并保持命令启动的进程在后台运行。  </p>
<h2 id="11-hive-e-和-hive-f"><a href="#11-hive-e-和-hive-f" class="headerlink" title="11.hive -e 和 hive -f"></a>11.hive -e 和 hive -f</h2><p>“-e”不进入hive的交互窗口执行hql语句  </p>
<p><code>bin/hive -e &quot;select id from student;&quot;</code></p>
<p>“-f”执行脚本中的hql语句  </p>
<p><code>bin/hive -f /opt/module/hive/datas/hivef.sql</code>  </p>
<h2 id="12-Hive参数配置方式"><a href="#12-Hive参数配置方式" class="headerlink" title="12.Hive参数配置方式"></a>12.Hive参数配置方式</h2><p>参数的配置三种方式  </p>
<h3 id="1-配置文件方式"><a href="#1-配置文件方式" class="headerlink" title="(1).配置文件方式"></a>(1).配置文件方式</h3><p><code>hive-site.xml</code></p>
<h3 id="2-命令行参数方式"><a href="#2-命令行参数方式" class="headerlink" title="(2).命令行参数方式"></a>(2).命令行参数方式</h3><p>启动Hive时，可以在命令行添加-hiveconf param&#x3D;value来设定参数  </p>
<p>比如：bin&#x2F;hive -hiveconf   </p>
<p><code>mapreduce.job.reduces=10;</code></p>
<p>注：仅对本次Hive启动有效  </p>
<h3 id="3-参数声明方式"><a href="#3-参数声明方式" class="headerlink" title="(3).参数声明方式"></a>(3).参数声明方式</h3><p>可以在HQL中使用SET关键字设定参数  </p>
<p><code>set mapreduce.job.reduces=10;</code>  </p>
<p>上述三种设定方式的优先级依次递增,配置文件 &lt; 命令行参数 &lt; 参数声明  </p>
<h2 id="13-Hive常见属性配置"><a href="#13-Hive常见属性配置" class="headerlink" title="13.Hive常见属性配置"></a>13.Hive常见属性配置</h2><p>Hive客户端显示当前库和表头<br>Set Hive-site.xml :<br>          <code>hive.cli.print.header = true</code><br>          <code>hive.cli.print.current.db</code>  </p>
<p>Hive运行日志路径配置<br>Set hive-log4j2.properties:<br>     <code>property.hive.log.dir=/opt/module/hive/logs</code>  </p>
<p>修改Hive的堆内存<br>Set hive-env.sh:<br>     <code>export HADOOP_HEAPSIZE=2048</code>  </p>
<p>关闭Hadoop虚拟内存检查<br>Set yarn-site.xml:<br>     <code>yarn.nodemanager.vmem-check-enabled =false</code>  </p>
<h1 id="二：Hive的DDL语法"><a href="#二：Hive的DDL语法" class="headerlink" title="二：Hive的DDL语法"></a>二：Hive的DDL语法</h1><h2 id="1-创建数据库"><a href="#1-创建数据库" class="headerlink" title="1.创建数据库"></a>1.创建数据库</h2><pre><code>CREATE DATABASE [IF NOT EXISTS] database_name  
[COMMENT database_comment]  
[LOCATION hdfs_path]  
[WITH DBPROPERTIES (property_name=property_value, ...)];   
</code></pre>
<p>创建一个数据库，指定路径<br>    hive (default)&gt; create database db_hive2 location ‘&#x2F;db_hive2’;  </p>
<h2 id="2-查看数据库信息"><a href="#2-查看数据库信息" class="headerlink" title="2.查看数据库信息"></a>2.查看数据库信息</h2><pre><code>DESCRIBE DATABASE [EXTENDED] db_name;    
</code></pre>
<h3 id="1-查看基本信息"><a href="#1-查看基本信息" class="headerlink" title="(1) 查看基本信息"></a>(1) 查看基本信息</h3><pre><code>desc database db_hive3;  
</code></pre>
<h3 id="2-查看更多信息"><a href="#2-查看更多信息" class="headerlink" title="(2) 查看更多信息"></a>(2) 查看更多信息</h3><pre><code>desc database extended db_hive3;  
</code></pre>
<h2 id="3-修改数据库"><a href="#3-修改数据库" class="headerlink" title="3.修改数据库"></a>3.修改数据库</h2><p>需要注意的是：修改数据库location，不会改变当前已有表的路径信息，而只是改变后续创建的新表的默认的父目录  </p>
<h3 id="修改dbproperties"><a href="#修改dbproperties" class="headerlink" title="修改dbproperties"></a>修改dbproperties</h3><pre><code>ALTER DATABASE database_name SET DBPROPERTIES   (property_name=property_value, ...);  
</code></pre>
<h3 id="修改location"><a href="#修改location" class="headerlink" title="修改location"></a>修改location</h3><pre><code>ALTER DATABASE database_name SET LOCATION hdfs_path;  
</code></pre>
<h3 id="修改owner-user"><a href="#修改owner-user" class="headerlink" title="修改owner user"></a>修改owner user</h3><pre><code>ALTER DATABASE database_name SET OWNER USER user_name;  
</code></pre>
<h2 id="4-删除数据库"><a href="#4-删除数据库" class="headerlink" title="4.删除数据库"></a>4.删除数据库</h2><pre><code>DROP DATABASE [IF EXISTS] database_name [RESTRICT|CASCADE];    
</code></pre>
<p>RESTRICT：严格模式，若数据库不为空，则会删除失败，默认为该模式。  </p>
<p>CASCADE：级联模式，若数据库不为空，则会将库中的表一并删除。</p>
<h2 id="5-切换当前数据库"><a href="#5-切换当前数据库" class="headerlink" title="5.切换当前数据库"></a>5.切换当前数据库</h2><p>USE database_name;  </p>
<h2 id="6-创建表"><a href="#6-创建表" class="headerlink" title="6.创建表"></a>6.创建表</h2><pre><code>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name   
[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
[CLUSTERED BY (col_name, col_name, ...) 
[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
[ROW FORMAT row_format] 
[STORED AS file_format]
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
</code></pre>
<p>关键字说明:  </p>
<h3 id="1-TEMPORARY"><a href="#1-TEMPORARY" class="headerlink" title="(1)TEMPORARY"></a>(1)TEMPORARY</h3><p>临时表，该表只在当前会话可见，会话结束，表会被删除。  </p>
<h3 id="2-EXTERNAL（重点）"><a href="#2-EXTERNAL（重点）" class="headerlink" title="(2)EXTERNAL（重点）"></a>(2)EXTERNAL（重点）</h3><p>外部表，与之相对应的是内部表（管理表）。内部表意味着Hive会完全接管该表，包括元数据和HDFS中的数据。而外部表则意味着Hive只接管元数据，而不完全接管HDFS中的数据</p>
<h3 id="3-data-type（重点）"><a href="#3-data-type（重点）" class="headerlink" title="(3)data_type（重点）"></a>(3)data_type（重点）</h3><p>Hive中的字段类型可分为基本数据类型和复杂数据类型。  </p>
<p><img src="/2023/07/30/hive_learn/2.png" alt="&quot;hive数据类型&quot;">  </p>
<p>类型转换:  </p>
<p>Hive的基本数据类型可以做类型转换，转换的方式包括隐式转换以及显示转换。  </p>
<h4 id="方式一：隐式转换"><a href="#方式一：隐式转换" class="headerlink" title="方式一：隐式转换"></a>方式一：隐式转换</h4><p>隐式地转换为一个范围更广的类型   </p>
<p>Hive官方隐式转换表<br><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/hive/languagemanual+types#LanguageManualTypes-AllowedImplicitConversions">Allowed Implicit Conversions</a>  </p>
<h4 id="方式二：显示转换"><a href="#方式二：显示转换" class="headerlink" title="方式二：显示转换"></a>方式二：显示转换</h4><p>可以借助cast函数完成显示的类型转换(强制转换)  </p>
<pre><code>select &#39;1&#39; + 2, cast(&#39;1&#39; as int) + 2;
</code></pre>
<h3 id="4-PARTITIONED-BY（重点）"><a href="#4-PARTITIONED-BY（重点）" class="headerlink" title="(4) PARTITIONED BY（重点）"></a>(4) PARTITIONED BY（重点）</h3><p>创建分区表  </p>
<h3 id="5-CLUSTERED-BY-…-SORTED-BY…INTO-…-BUCKETS（重点）"><a href="#5-CLUSTERED-BY-…-SORTED-BY…INTO-…-BUCKETS（重点）" class="headerlink" title="(5) CLUSTERED BY … SORTED BY…INTO … BUCKETS（重点）"></a>(5) CLUSTERED BY … SORTED BY…INTO … BUCKETS（重点）</h3><p>创建分桶表  </p>
<h3 id="6-ROW-FORMAT（重点）"><a href="#6-ROW-FORMAT（重点）" class="headerlink" title="(6) ROW FORMAT（重点）"></a>(6) ROW FORMAT（重点）</h3><p>指定SERDE，SERDE是Serializer and Deserializer的简写。Hive使用SERDE序列化和反序列化每行数据  </p>
<p>Hive官方序列化反序列化器文档<br><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-HiveSerDe">Hive-Serde</a></p>
<h4 id="语法一："><a href="#语法一：" class="headerlink" title="语法一："></a>语法一：</h4><p>DELIMITED关键字表示对文件中的每个字段按照特定分割符进行分割，其会使用默认的SERDE对每行数据进行序列化和反序列化  </p>
<pre><code>ROW FORAMT DELIMITED 
[FIELDS TERMINATED BY char] 
[COLLECTION ITEMS TERMINATED BY char] 
[MAP KEYS TERMINATED BY char] 
[LINES TERMINATED BY char] 
[NULL DEFINED AS char]   
</code></pre>
<p>注：<br>fields terminated by ：列分隔符<br>collection items terminated by ： map、struct和array中每个元素之间的分隔符<br>map keys terminated by ：map中的key与value的分隔符<br>lines terminated by ：行分隔符  </p>
<h4 id="语法二："><a href="#语法二：" class="headerlink" title="语法二："></a>语法二：</h4><p>SERDE关键字可用于指定其他内置的SERDE或者用户自定义的SERDE。例如JSON SERDE，可用于处理JSON字符串  </p>
<pre><code>ROW FORMAT SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,property_name=property_value, ...)] 
</code></pre>
<h3 id="7-STORED-AS（重点）"><a href="#7-STORED-AS（重点）" class="headerlink" title="(7) STORED AS（重点）"></a>(7) STORED AS（重点）</h3><p>指定文件格式，常用的文件格式有，textfile（默认值），sequence file，orc file、parquet file等等  </p>
<h3 id="8-LOCATION"><a href="#8-LOCATION" class="headerlink" title="(8) LOCATION"></a>(8) LOCATION</h3><p>指定表所对应的HDFS路径，若不指定路径，其默认值为<br>${hive.metastore.warehouse.dir}&#x2F;db_name.db&#x2F;table_name  </p>
<h3 id="9-TBLPROPERTIES"><a href="#9-TBLPROPERTIES" class="headerlink" title="(9) TBLPROPERTIES"></a>(9) TBLPROPERTIES</h3><p>用于配置表的一些KV键值对参数  </p>
<h2 id="7-Create-Table-As-Select（CTAS）建表"><a href="#7-Create-Table-As-Select（CTAS）建表" class="headerlink" title="7.Create Table As Select（CTAS）建表"></a>7.Create Table As Select（CTAS）建表</h2><pre><code>CREATE [TEMPORARY] TABLE [IF NOT EXISTS] table_name 
[COMMENT table_comment] 
[ROW FORMAT row_format] 
[STORED AS file_format] 
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
[AS select_statement]
</code></pre>
<h2 id="8-Create-Table-Like语法"><a href="#8-Create-Table-Like语法" class="headerlink" title="8.Create Table Like语法"></a>8.Create Table Like语法</h2><p>该语法允许用户复刻一张已经存在的表结构，与上述的CTAS语法不同，该语法创建出来的表中不包含数据  </p>
<pre><code>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
[LIKE exist_table_name]
[ROW FORMAT row_format] 
[STORED AS file_format] 
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
</code></pre>
<h2 id="9-内部表与外部表"><a href="#9-内部表与外部表" class="headerlink" title="9.内部表与外部表"></a>9.内部表与外部表</h2><p>Hive中默认创建的表都是的内部表，有时也被称为管理表。对于内部表，Hive会完全管理表的元数据和数据文件  </p>
<p>外部表通常可用于处理其他工具上传的数据文件，对于外部表，Hive只负责管理元数据，不负责管理HDFS中的数据文件  </p>
<h2 id="10-查看表"><a href="#10-查看表" class="headerlink" title="10.查看表"></a>10.查看表</h2><pre><code>DESCRIBE [EXTENDED | FORMATTED] [db_name.]table_name  
</code></pre>
<p>EXTENDED：展示详细信息  </p>
<p>FORMATTED：对详细信息进行格式化的展示</p>
<h2 id="11-修改列信息"><a href="#11-修改列信息" class="headerlink" title="11.修改列信息"></a>11.修改列信息</h2><h3 id="增加列"><a href="#增加列" class="headerlink" title="增加列"></a>增加列</h3><pre><code>ALTER TABLE table_name ADD COLUMNS (col_name data_type [COMMENT col_comment], ...)  
</code></pre>
<p>新增列的位置位于末尾  </p>
<h3 id="更新列"><a href="#更新列" class="headerlink" title="更新列"></a>更新列</h3><pre><code>ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]
</code></pre>
<p>该语句允许用户修改指定列的列名、数据类型、注释信息以及在表中的位置  </p>
<h3 id="替换列"><a href="#替换列" class="headerlink" title="替换列"></a>替换列</h3><pre><code>ALTER TABLE table_name REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)  
</code></pre>
<p>该语句允许用户用新的列集替换表中原有的全部列  </p>
<h2 id="12-清空表"><a href="#12-清空表" class="headerlink" title="12.清空表"></a>12.清空表</h2><pre><code>TRUNCATE [TABLE] table_name  
</code></pre>
<p>truncate只能清空管理表，不能删除外部表中数据  </p>
<h1 id="三：Hive的DML语法"><a href="#三：Hive的DML语法" class="headerlink" title="三：Hive的DML语法"></a>三：Hive的DML语法</h1><h2 id="1-Load"><a href="#1-Load" class="headerlink" title="1. Load"></a>1. Load</h2><p>Load语句可将文件导入到Hive表中    </p>
<pre><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)];
</code></pre>
<p>关键字说明：  </p>
<p>（1）local：表示从本地加载数据到Hive表；否则从HDFS加载数据到Hive表。  </p>
<p>（2）overwrite：表示覆盖表中已有数据，否则表示追加。  </p>
<p>（3）partition：表示上传到指定分区，若目标是分区表，需指定分区    </p>
<h3 id="1-1加载本地文件到hive"><a href="#1-1加载本地文件到hive" class="headerlink" title="1.1加载本地文件到hive:"></a>1.1加载本地文件到hive:</h3><pre><code>load data local inpath &#39;/opt/module/datas/student.txt&#39; into table student;  
</code></pre>
<h3 id="1-2加载HDFS上数据"><a href="#1-2加载HDFS上数据" class="headerlink" title="1.2加载HDFS上数据:"></a>1.2加载HDFS上数据:</h3><pre><code>hadoop fs -put /opt/module/datas/student.txt /user/atguigu;  

load data inpath &#39;/user/atguigu/student.txt&#39; into table student;
</code></pre>
<h2 id="2-Insert"><a href="#2-Insert" class="headerlink" title="2.Insert"></a>2.Insert</h2><pre><code>INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement;
</code></pre>
<p>关键字说明：  </p>
<p>（1）INTO：将结果追加到目标表  </p>
<p>（2）OVERWRITE：用结果覆盖原有数据  </p>
<h3 id="2-1-将给定Values插入表中"><a href="#2-1-将给定Values插入表中" class="headerlink" title="2.1 将给定Values插入表中"></a>2.1 将给定Values插入表中</h3><pre><code>INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...]  
</code></pre>
<h3 id="2-2将查询结果写入目标路径"><a href="#2-2将查询结果写入目标路径" class="headerlink" title="2.2将查询结果写入目标路径"></a>2.2将查询结果写入目标路径</h3><pre><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory[ROW FORMAT row_format] [STORED AS file_format] select_statement;
</code></pre>
<h2 id="3-Export-Import"><a href="#3-Export-Import" class="headerlink" title="3.Export&amp;Import"></a>3.Export&amp;Import</h2><p>Export导出语句可将表的数据和元数据信息一并到处的HDFS路径，Import可将Export导出的内容导入Hive，表的数据和元数据信息都会恢复。Export和Import可用于两个Hive实例之间的数据迁移</p>
<h3 id="导出"><a href="#导出" class="headerlink" title="导出"></a>导出</h3><pre><code>EXPORT TABLE tablename TO &#39;export_target_path&#39;
</code></pre>
<h3 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h3><pre><code>IMPORT [EXTERNAL] TABLE new_or_original_tablename FROM &#39;source_path&#39; [LOCATION &#39;import_target_path&#39;]
</code></pre>
<h1 id="四-查询"><a href="#四-查询" class="headerlink" title="四:查询"></a>四:查询</h1><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">Hive-select语法文档</a></p>
<h2 id="1-select-查询语法"><a href="#1-select-查询语法" class="headerlink" title="1.select 查询语法"></a>1.select 查询语法</h2><pre><code>SELECT [ALL | DISTINCT] select_expr, select_expr, 
FROM table_reference       -- 从什么表查
[WHERE where_condition]   -- 过滤
[GROUP BY col_list]        -- 分组查询
[HAVING col_list]          -- 分组后过滤
[ORDER BY col_list]        -- 排序
[CLUSTER BY col_list
| [DISTRIBUTE BY col_list] [SORT BY col_list]]
[LIMIT number]                -- 限制输出的行数
</code></pre>
<p>注意：<br>（1）SQL 语言大小写不敏感。<br>（2）SQL 可以写在一行或者多行。<br>（3）关键字不能被缩写也不能分行。<br>（4）各子句一般要分行写。<br>（5）使用缩进提高语句的可读性    </p>
<h2 id="2-Limit语句"><a href="#2-Limit语句" class="headerlink" title="2.Limit语句"></a>2.Limit语句</h2><p>典型的查询会返回多行数据。limit子句用于限制返回的行数  </p>
<pre><code>select * from emp limit 2,3; -- 表示从第2行开始，向下抓取3行
</code></pre>
<h2 id="3-关系运算函数"><a href="#3-关系运算函数" class="headerlink" title="3.关系运算函数"></a>3.关系运算函数</h2><p>A&lt;&#x3D;&gt;B :  </p>
<p> 如果A和B都为null或者都不为null，则返回true，如果只有一边为null，返回false  </p>
<p>A [not] like B :  </p>
<p> B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回true；反之返回false。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母‘x’结尾，而‘%x%’表示A包含有字母‘x’,可以位于开头，结尾或者字符串中间。如果使用not关键字则可达到相反的效果。  </p>
<p>A rlike B, A regexp B:  </p>
<p>B是基于java的正则表达式，如果A与其匹配，则返回true；反之返回false。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。  </p>
<h2 id="4-聚合函数"><a href="#4-聚合函数" class="headerlink" title="4.聚合函数"></a>4.聚合函数</h2><p>count(*)，表示统计所有行数，包含null值； </p>
<p>count(某列)，表示该列一共有多少行，不包含null值； </p>
<p>max()，求最大值，不包含null，除非所有值都是null； </p>
<p>min()，求最小值，不包含null，除非所有值都是null； </p>
<p>sum()，求和，不包含null。   </p>
<p>avg()，求平均值，不包含null。    </p>
<h2 id="5-分组"><a href="#5-分组" class="headerlink" title="5.分组"></a>5.分组</h2><p>Group By语句：  </p>
<p>Group By语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</p>
<h2 id="6-Having语句"><a href="#6-Having语句" class="headerlink" title="6.Having语句"></a>6.Having语句</h2><p>having与where不同点  </p>
<p>（1）where后面不能写分组聚合函数，而having后面可以使用分组聚合函数。  </p>
<p>（2）having只用于group by分组统计语句    </p>
<h2 id="7-Join语句"><a href="#7-Join语句" class="headerlink" title="7.Join语句"></a>7.Join语句</h2><p>Hive支持通常的sql join语句，但是只支持等值连接，不支持非等值连接。  </p>
<p>inner join 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来  </p>
<p>left outer join 左外连接：join操作符左边表中符合where子句的所有记录将会被返回  </p>
<p>right outer join 右外连接：join操作符右边表中符合where子句的所有记录将会被返回  </p>
<p>full outer join : 满外连接：将会返回所有表中符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代    </p>
<p>多表连接: 连接n个表，至少需要n-1个连接条件    </p>
<p>大多数情况下，Hive会对每对join连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作</p>
<p>因为Hive总是按照从左到右的顺序执行的  </p>
<h2 id="8-笛卡尔积产生条件"><a href="#8-笛卡尔积产生条件" class="headerlink" title="8.笛卡尔积产生条件"></a>8.笛卡尔积产生条件</h2><p>（1）省略连接条件  </p>
<p>（2）连接条件无效  </p>
<p>（3）所有表中的所有行互相连接  </p>
<h2 id="9-联合（union-union-all）"><a href="#9-联合（union-union-all）" class="headerlink" title="9.联合（union &amp; union all）"></a>9.联合（union &amp; union all）</h2><p>union和union all都是上下拼接sql的结果，这点是和join有区别的，join是左右关联，union和union all是上下拼接。  </p>
<p>union去重，union all不去重  </p>
<p>union和union all在上下拼接sql结果时有两个要求：  </p>
<p>（1）两个sql的结果，列的个数必须相同  </p>
<p>（2）两个sql的结果，上下所对应列的类型必须一致    </p>
<h2 id="10-排序"><a href="#10-排序" class="headerlink" title="10.排序"></a>10.排序</h2><h4 id="全局排序-Order-By"><a href="#全局排序-Order-By" class="headerlink" title="全局排序 Order By"></a>全局排序 Order By</h4><p>全局排序，只有一个Reduce    </p>
<p>asc（ascend）：升序（默认）  </p>
<p>desc（descend）：降序  </p>
<h4 id="每个Reduce内部排序（Sort-By）"><a href="#每个Reduce内部排序（Sort-By）" class="headerlink" title="每个Reduce内部排序（Sort By）"></a>每个Reduce内部排序（Sort By）</h4><p>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用Sort by  </p>
<p>Sort by为每个reduce产生一个排序文件。每个Reduce内部进行排序，对全局结果集来说不是排序  </p>
<p>设置reduce个数  </p>
<pre><code>hive (default)&gt; set mapreduce.job.reduces=3;  
</code></pre>
<p>查看设置reduce个数  </p>
<pre><code>hive (default)&gt; set mapreduce.job.reduces;  
</code></pre>
<h2 id="11-分区（Distribute-By）"><a href="#11-分区（Distribute-By）" class="headerlink" title="11.分区（Distribute By）"></a>11.分区（Distribute By）</h2><p>Distribute By：在有些情况下，我们需要控制某个特定行应该到哪个Reducer，通常是为了进行后续的聚集操作。distribute by子句可以做这件事。distribute by类似MapReduce中partition（自定义分区），进行分区，结合sort by使用  </p>
<p>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行相除后，余数相同的分到一个区。  </p>
<p>Hive要求distribute by语句要写在sort by语句之前。  </p>
<p>演示完以后mapreduce.job.reduces的值要设置回-1，否则下面分区or分桶表load跑MapReduce的时候会报错  </p>
<h2 id="12-分区排序（Cluster-By）"><a href="#12-分区排序（Cluster-By）" class="headerlink" title="12.分区排序（Cluster By）"></a>12.分区排序（Cluster By）</h2><p>当distribute by和sort by字段相同时，可以使用cluster by方式  </p>
<p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为asc或者desc      </p>
<p>以下两种写法等价<br>hive (default)&gt;<br>    select *<br>    from emp<br>    cluster by deptno;  </p>
<p>hive (default)&gt;<br>    select<br>    *<br>    from emp<br>    distribute by deptno sort by deptno asc;  </p>
<h1 id="五-函数"><a href="#五-函数" class="headerlink" title="五:函数"></a>五:函数</h1><p>Hive会将常用的逻辑封装成函数给用户进行使用，类似于Java中的函数    </p>
<p>Hive提供了大量的内置函数，按照其特点可大致分为如下几类：单行函数、聚合函数、炸裂函数、窗口函数  </p>
<p>查看系统内置函数</p>
<pre><code>show functions;  
</code></pre>
<p>查看内置函数用法  </p>
<pre><code>desc function upper;  
</code></pre>
<p>查看内置函数详细信息  </p>
<pre><code>desc function extended upper;  
</code></pre>
<h2 id="1-单行函数"><a href="#1-单行函数" class="headerlink" title="1.单行函数"></a>1.单行函数</h2><p>单行函数的特点是一进一出，即输入一行，输出一行</p>
<p>单行函数按照功能可分为如下几类: 日期函数、字符串函数、集合函数、数学函数、流程控制函数等  </p>
<h3 id="数值函数"><a href="#数值函数" class="headerlink" title="数值函数"></a>数值函数</h3><pre><code>round：四舍五入    

ceil：向上取整   

floor：向下取整  
</code></pre>
<h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><pre><code>substring：截取字符串  

substring(string A, int start)  

substring(string A, int start, int len)   
</code></pre>
<p>replace ：替换  </p>
<pre><code>replace(string A, string B, string C)   

regexp_replace：正则替换  

regexp_replace(string A, string B, string C)   
说明：将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符  
</code></pre>
<p>regexp：正则匹配 </p>
<pre><code>字符串 regexp 正则表达式     

说明：若字符串符合正则表达式，则返回true，否则返回false

select &#39;dfsaaaa&#39; regexp &#39;dfsa+&#39;  
</code></pre>
<p>repeat：重复字符串  </p>
<pre><code>repeat(string A, int n)  

说明：将字符串A重复n遍

select repeat(&#39;123&#39;, 3);  
</code></pre>
<p>split ：字符串切割  </p>
<pre><code>split(string str, string pat)   

返回值：array

hive&gt; select split(&#39;a-b-c-d&#39;,&#39;-&#39;);

hive&gt; [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;]
</code></pre>
<p>nvl ：替换null值  </p>
<pre><code>nvl(A,B) 

若A的值不为null，则返回A，否则返回B  

hive&gt; select nvl(null,1);   
</code></pre>
<p>concat ：拼接字符串  </p>
<pre><code>concat(string A, string B, string C, ……)   

将A,B,C……等字符拼接为一个字符串  

hive&gt; select concat(&#39;beijing&#39;,&#39;-&#39;,&#39;shanghai&#39;,&#39;-&#39;,&#39;shenzhen&#39;);  

hive&gt; beijing-shanghai-shenzhen  
</code></pre>
<p>concat_ws：以指定分隔符拼接字符串或者字符串数组  </p>
<pre><code>concat_ws(string A, string…| array(string))   

使用分隔符A拼接多个字符串，或者一个数组的所有元素。  

hive&gt;select concat_ws(&#39;-&#39;,&#39;beijing&#39;,&#39;shanghai&#39;,&#39;shenzhen&#39;);  

hive&gt; beijing-shanghai-shenzhen  

hive&gt; select concat_ws(&#39;-&#39;,array(&#39;beijing&#39;,&#39;shenzhen&#39;,&#39;shanghai&#39;));  

hive&gt; beijing-shanghai-shenzhen
</code></pre>
<p>get_json_object：解析json字符串   </p>
<pre><code>get_json_object(string json_string, string path)

解析json的字符串json_string，返回path指定的内容。如果输入的json字符串无效，那么返回NULL  

hive&gt; select get_json_object(&#39;[&#123;&quot;name&quot;:&quot;大海海&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;25&quot;&#125;,&#123;&quot;name&quot;:&quot;小宋宋&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;47&quot;&#125;]&#39;,&#39;$.[0].name&#39;);  
</code></pre>
<h3 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h3><p>unix_timestamp：返回当前或指定时间的时间戳</p>
<pre><code>unix_timestamp()   

返回值：bigint    

hive&gt; select unix_timestamp(&#39;2022/08/08 08-08-08&#39;,&#39;yyyy/MM/dd HH-mm-ss&#39;);

1659946088  
</code></pre>
<p>from_unixtime：转化UNIX时间戳（从 1970-01-01 00:00:00 UTC 到指定时间的秒数）到当前时区的时间格式</p>
<pre><code>from_unixtime(bigint unixtime[, string format])  

返回值：string  

hive&gt; select from_unixtime(1659946088);  

2022-08-08 08:08:08  
</code></pre>
<p>current_date：当前日期   </p>
<p>current_timestamp：当前的日期加时间，并且精确的毫秒   </p>
<p>month：获取日期中的月  </p>
<pre><code>语法：month (string date)   

返回值：int   
</code></pre>
<p>day：获取日期中的日  </p>
<pre><code>语法：day (string date)   

返回值：int 
</code></pre>
<p>hour：获取日期中的小时  </p>
<pre><code>语法：hour (string date)   

返回值：int   
</code></pre>
<p>datediff：两个日期相差的天数（结束日期减去开始日期的天数）</p>
<pre><code>语法：datediff(string enddate, string startdate) 

返回值：int   

hive&gt; select datediff(&#39;2021-08-08&#39;,&#39;2022-10-09&#39;);    

    -427    
</code></pre>
<p>date_add：日期加天数  </p>
<pre><code>date_add(string startdate, int days)   
</code></pre>
<p>date_sub：日期减天数  </p>
<pre><code>date_sub (string startdate, int days)   
</code></pre>
<p>date_format:将标准日期解析成指定格式字符串  </p>
<pre><code>hive&gt; select date_format(&#39;2022-08-08&#39;,&#39;yyyy年-MM月-dd日&#39;)  

2022年-08月-08日 
</code></pre>
<h2 id="流程控制函数"><a href="#流程控制函数" class="headerlink" title="流程控制函数"></a>流程控制函数</h2><p>case when：条件判断函数  </p>
<pre><code>语法一： case when a then b [when c then d]* [else e] end   

语法二： case a when b then c [when d then e]* [else f] end
判断同一个字段与多个值是否相等时才能这样写  
</code></pre>
<p>if: 条件判断，类似于Java中三元运算符</p>
<pre><code>if（boolean testCondition, T valueTrue, T valueFalseOrNull）  

当条件testCondition为true时，返回valueTrue；否则返回valueFalseOrNull	 

hive&gt; select if(10 &gt; 5,&#39;正确&#39;,&#39;错误&#39;);    

输出：正确  
</code></pre>
<h2 id="集合函数"><a href="#集合函数" class="headerlink" title="集合函数"></a>集合函数</h2><p>size：集合中元素的个数  </p>
<pre><code>hive&gt; select size(friends) from test;  --2/2  每一行数据中的friends集合里的个数  
</code></pre>
<p>map：创建map集合</p>
<pre><code>语法：map (key1, value1, key2, value2, …) 

说明：根据输入的key和value对构建map类型  
</code></pre>
<p>map_keys： 返回map中的key  </p>
<pre><code>hive&gt; select map_keys(map(&#39;xiaohai&#39;,1,&#39;dahai&#39;,2));  

hive&gt;[&quot;xiaohai&quot;,&quot;dahai&quot;] 
</code></pre>
<p>map_values: 返回map中的value  </p>
<pre><code>hive&gt; select map_values(map(&#39;xiaohai&#39;,1,&#39;dahai&#39;,2));

hive&gt;[1,2]  
</code></pre>
<p>array 声明array集合  </p>
<pre><code>语法：array(val1, val2, …) 

说明：根据输入的参数构建数组array类  

hive&gt; select array(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;);  

hive&gt;[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;]  
</code></pre>
<p>array_contains: 判断array中是否包含某个元素  </p>
<pre><code>hive&gt; select array_contains(array(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;),&#39;a&#39;);   

hive&gt; true 
</code></pre>
<p>sort_array：将array中的元素排序</p>
<pre><code>hive&gt; select sort_array(array(&#39;a&#39;,&#39;d&#39;,&#39;c&#39;));  

hive&gt; [&quot;a&quot;,&quot;c&quot;,&quot;d&quot;]  
</code></pre>
<p>struct声明struct中的各属性   </p>
<pre><code>语法：struct(val1, val2, val3, …)   

说明：根据输入的参数构建结构体struct类  

hive&gt; select struct(&#39;name&#39;,&#39;age&#39;,&#39;weight&#39;);  

hive&gt; &#123;&quot;col1&quot;:&quot;name&quot;,&quot;col2&quot;:&quot;age&quot;,&quot;col3&quot;:&quot;weight&quot;&#125;  
</code></pre>
<p>named_struct声明struct的属性和值  </p>
<pre><code>hive&gt; select named_struct(&#39;name&#39;,&#39;xiaosong&#39;,&#39;age&#39;,18,&#39;weight&#39;,80);  

hive&gt; &#123;&quot;name&quot;:&quot;xiaosong&quot;,&quot;age&quot;:18,&quot;weight&quot;:80&#125;  
</code></pre>
<h2 id="2-高级聚合函数"><a href="#2-高级聚合函数" class="headerlink" title="2.高级聚合函数"></a>2.高级聚合函数</h2><p>多进一出 （多行传入，一个行输出） </p>
<p>普通聚合  </p>
<p>collect_list 收集并形成list集合，结果不去重</p>
<pre><code>hive&gt;
select sex,collect_list(job) from employee group by sex;  

女	[&quot;行政&quot;,&quot;研发&quot;,&quot;行政&quot;,&quot;前台&quot;]  
男	[&quot;销售&quot;,&quot;研发&quot;,&quot;销售&quot;,&quot;前台&quot;]  
</code></pre>
<p>collect_set 收集并形成set集合，结果去重  </p>
<pre><code>hive&gt;
select sex,collect_set(job) from employee group by sex;

女	[&quot;行政&quot;,&quot;研发&quot;,&quot;前台&quot;]	
男	[&quot;销售&quot;,&quot;研发&quot;,&quot;前台&quot;]  
</code></pre>
<h2 id="3-炸裂函数"><a href="#3-炸裂函数" class="headerlink" title="3.炸裂函数"></a>3.炸裂函数</h2><p>UDTF,接收一行数据，输出一行或多行数据。</p>
<h2 id="4-窗口函数"><a href="#4-窗口函数" class="headerlink" title="4.窗口函数"></a>4.窗口函数</h2><p>窗口函数，能为每行数据划分一个窗口，然后对窗口范围内的数据进行计算，最后将计算结果返回给该行的数据。</p>
<h3 id="窗口语法"><a href="#窗口语法" class="headerlink" title="窗口语法"></a>窗口语法</h3><h4 id="基于行"><a href="#基于行" class="headerlink" title="基于行"></a>基于行</h4><p><img src="/2023/07/30/hive_learn/3.png" alt="基于行">            </p>
<h4 id="基于值"><a href="#基于值" class="headerlink" title="基于值"></a>基于值</h4><p><img src="/2023/07/30/hive_learn/4.png" alt="基于值"></p>
<p>按照功能，常用窗口可划分为如下几类：聚合函数、跨行取值函数、排名函数  </p>
<p>聚合函数</p>
<pre><code>max：最大值

min：最小值  

sum：求和

avg：平均值

count：计数    
</code></pre>
<p>跨行取值函数</p>
<pre><code>Lead：上移  

Lag：下移

注：lag和lead函数不支持自定义窗口

first_value

last_value  
</code></pre>
<p>排名函数  </p>
<pre><code>rank 

dense_rank

row_number

三者均不支持自定义窗口  
</code></pre>
<h1 id="六：自定义函数"><a href="#六：自定义函数" class="headerlink" title="六：自定义函数"></a>六：自定义函数</h1><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">自定义函数官方文档</a> </p>
<h2 id="编程步骤"><a href="#编程步骤" class="headerlink" title="编程步骤"></a>编程步骤</h2><p>(1) 继承Hive提供的类  </p>
<p>org.apache.hadoop.hive.ql.udf.generic.GenericUDF  </p>
<p>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</p>
<p>(2) 实现类中的抽象方法  </p>
<p>(3) 在hive的命令行窗口创建函数  </p>
<p>添加jar  </p>
<pre><code>add jar linux_jar_path  
</code></pre>
<p>创建function  </p>
<pre><code>create [temporary] function [dbname.]function_name AS class_name;
</code></pre>
<p>(4) 在hive的命令行窗口删除函数  </p>
<pre><code>drop [temporary] function [if exists] [dbname.]function_name;
</code></pre>
<h1 id="七：分区表和分桶表"><a href="#七：分区表和分桶表" class="headerlink" title="七：分区表和分桶表"></a>七：分区表和分桶表</h1><h2 id="1-分区表"><a href="#1-分区表" class="headerlink" title="1.分区表"></a>1.分区表</h2><p>Hive中的分区就是把一张大表的数据按照业务需要分散的存储到多个目录，每个目录就称为该表的一个分区  </p>
<pre><code>hive (default)&gt; 
create table dept_partition
(
deptno int,    --部门编号
dname  string, --部门名称
loc    string  --部门位置
)
partitioned by (day string)
row format delimited fields terminated by &#39;\t&#39;;
</code></pre>
<p>装载语句  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/dept_20220401.log&#39; 
into table dept_partition 
partition(day=&#39;20220401&#39;);  

hive (default)&gt; 
insert overwrite table dept_partition partition (day = &#39;20220402&#39;)
select deptno, dname, loc
from dept_partition
where day = &#39;2020-04-01&#39;;
</code></pre>
<h3 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a>分区表基本操作</h3><h4 id="1）查看所有分区信息"><a href="#1）查看所有分区信息" class="headerlink" title="1）查看所有分区信息"></a>1）查看所有分区信息</h4><pre><code>hive&gt; show partitions dept_partition;
</code></pre>
<h4 id="2）增加分区"><a href="#2）增加分区" class="headerlink" title="2）增加分区"></a>2）增加分区</h4><h5 id="（1）创建单个分区"><a href="#（1）创建单个分区" class="headerlink" title="（1）创建单个分区"></a>（1）创建单个分区</h5><pre><code>hive (default)&gt; 
alter table dept_partition 
add partition(day=&#39;20220403&#39;);  
</code></pre>
<h5 id="（2）同时创建多个分区（分区之间不能有逗号）"><a href="#（2）同时创建多个分区（分区之间不能有逗号）" class="headerlink" title="（2）同时创建多个分区（分区之间不能有逗号）"></a>（2）同时创建多个分区（分区之间不能有逗号）</h5><pre><code>hive (default)&gt; 
alter table dept_partition 
add partition(day=&#39;20220404&#39;) partition(day=&#39;20220405&#39;);  
</code></pre>
<h4 id="3）删除分区"><a href="#3）删除分区" class="headerlink" title="3）删除分区"></a>3）删除分区</h4><h5 id="（1）删除单个分区"><a href="#（1）删除单个分区" class="headerlink" title="（1）删除单个分区"></a>（1）删除单个分区</h5><pre><code>hive (default)&gt; 
alter table dept_partition 
drop partition (day=&#39;20220403&#39;);
</code></pre>
<h5 id="（2）同时删除多个分区（分区之间必须有逗号）"><a href="#（2）同时删除多个分区（分区之间必须有逗号）" class="headerlink" title="（2）同时删除多个分区（分区之间必须有逗号）"></a>（2）同时删除多个分区（分区之间必须有逗号）</h5><pre><code>hive (default)&gt; 
alter table dept_partition 
drop partition (day=&#39;20220404&#39;), partition(day=&#39;20220405&#39;);
</code></pre>
<h4 id="4）修复分区"><a href="#4）修复分区" class="headerlink" title="4）修复分区"></a>4）修复分区</h4><p>add partition  </p>
<pre><code>若手动创建HDFS的分区路径，Hive无法识别，可通过add partition命令增加分区元数据信息，从而使元数据和分区路径保持一致
</code></pre>
<p>drop partition  </p>
<pre><code>若手动删除HDFS的分区路径，Hive无法识别，可通过drop partition命令删除分区元数据信息，从而使元数据和分区路径保持一致
</code></pre>
<p>msck </p>
<p>若分区元数据和HDFS的分区路径不一致，还可使用msck命令进行修复，以下是该命令的用法说明 </p>
<pre><code>hive (default)&gt; 
msck repair table table_name [add/drop/sync partitions];

msck repair table table_name add partitions：该命令会增加HDFS路径存在但元数据缺失的分区信息 

msck repair table table_name drop partitions：该命令会删除HDFS路径已经删除但元数据仍然存在的分区信息 

msck repair table table_name sync partitions：该命令会同步HDFS路径和元数据分区信息，相当于同时执行上述的两个命令

msck repair table table_name：等价于msck repair table table_name add partitions命令
</code></pre>
<p><strong>所以msck修复hive元数据，首选msck repair table table_name sync partitions命令</strong>  </p>
<h4 id="二级分区表"><a href="#二级分区表" class="headerlink" title="二级分区表"></a>二级分区表</h4><p>二级分区表建表语句  </p>
<pre><code>hive (default)&gt;
create table dept_partition2(
deptno int,    -- 部门编号
dname string, -- 部门名称
loc string     -- 部门位置
)
partitioned by (day string, hour string)
row format delimited fields terminated by &#39;\t&#39;; 
</code></pre>
<p>数据装载语句  </p>
<pre><code>hive (default)&gt; 
load data local inpath &#39;/opt/module/hive/datas/dept_20220401.log&#39; 
into table dept_partition2 
partition(day=&#39;20220401&#39;, hour=&#39;12&#39;);  
</code></pre>
<p>查询分区数据  </p>
<pre><code>hive (default)&gt; 
select  * 
from dept_partition2 
where day=&#39;20220401&#39; and hour=&#39;12&#39;;  
</code></pre>
<h4 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h4><p>动态分区是指向分区表insert数据时，被写往的分区不由用户指定，而是由每行数据的最后一个字段的值来动态的决定，使用动态分区，可只用一个insert语句将数据写入多个分区  </p>
<h5 id="1）动态分区相关参数"><a href="#1）动态分区相关参数" class="headerlink" title="1）动态分区相关参数"></a>1）动态分区相关参数</h5><p>(1) 动态分区功能总开关（默认true，开启）  </p>
<pre><code>set hive.exec.dynamic.partition=true  
</code></pre>
<p>(2) 严格模式和非严格模式   </p>
<p>动态分区的模式，默认strict（严格模式），要求必须指定至少一个分区为静态分区，nonstrict（非严格模式）允许所有的分区字段都使用动态分区  </p>
<pre><code>set hive.exec.dynamic.partition.mode=nonstrict
</code></pre>
<h2 id="2-分桶表"><a href="#2-分桶表" class="headerlink" title="2.分桶表"></a>2.分桶表</h2><p>并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分  </p>
<p><strong>分区针对的是数据的存储路径，分桶针对的是数据文件</strong>  </p>
<p>分桶表的基本原理是，首先为每行数据计算一个指定字段的数据的hash值，然后模以一个指定的分桶数，最后将取模运算结果相同的行，写入同一个文件中，这个文件就称为一个分桶（bucket）</p>
<p>建表语句</p>
<pre><code>hive (default)&gt; 
create table stu_buck(
id int, 
name string
)
clustered by(id) sorted by(id)
into 4 buckets
row format delimited fields terminated by &#39;\t&#39;;

load data local inpath &#39;/opt/module/hive/datas/student.txt&#39; 
into table stu_buck;
</code></pre>
<h1 id="8-Hive压缩格式和文件格式"><a href="#8-Hive压缩格式和文件格式" class="headerlink" title="8.Hive压缩格式和文件格式"></a>8.Hive压缩格式和文件格式</h1><h2 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h2><p>DEFLATE<br>gzip<br>bzip2<br>LZO<br>Snappy</p>
<h2 id="Hive文件格式"><a href="#Hive文件格式" class="headerlink" title="Hive文件格式"></a>Hive文件格式</h2><p>text file<br>orc<br>parquet<br>sequence file  </p>
<h2 id="行式存储和列式存储"><a href="#行式存储和列式存储" class="headerlink" title="行式存储和列式存储"></a>行式存储和列式存储</h2><h3 id="行式存储-textfile，sequence-file"><a href="#行式存储-textfile，sequence-file" class="headerlink" title="行式存储 - textfile，sequence file"></a>行式存储 - textfile，sequence file</h3><p>文本文件是Hive默认使用的文件格式，文本文件中的一行内容，就对应Hive表中的一行记录</p>
<p>适用于会用到很多where语句的表</p>
<h3 id="列式存储-orc，parquet"><a href="#列式存储-orc，parquet" class="headerlink" title="列式存储 - orc，parquet"></a>列式存储 - orc，parquet</h3><p>数仓中尽量选用列式存储方式</p>
<p><img src="/2023/07/30/hive_learn/5.png" alt="ORC文件基本格式">  </p>
<p><img src="/2023/07/30/hive_learn/6.png" alt="Parquet文件基本格式">   </p>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><p>在Hive表中和计算过程中，保持数据的压缩，对磁盘空间的有效利用和提高查询性能都是十分有益的  </p>
<h3 id="Hive表数据进行压缩"><a href="#Hive表数据进行压缩" class="headerlink" title="Hive表数据进行压缩"></a>Hive表数据进行压缩</h3><h4 id="1）TextFile"><a href="#1）TextFile" class="headerlink" title="1）TextFile"></a>1）TextFile</h4><p>无法直接在表结构中进行声明压缩    </p>
<p>直接将压缩后的文件导入到该表即可，Hive在查询表中数据时，可自动识别其压缩格式，进行解压  </p>
<p>TextFile压缩格式常为Gzip  </p>
<p>需要注意的是，在执行往表中导入数据的SQL语句时，用户需设置以下参数，来保证写入表中的数据是被压缩的。  </p>
<p>–SQL语句的最终输出结果是否压缩  </p>
<pre><code>set hive.exec.compress.output=true;  
</code></pre>
<p>–输出结果的压缩格式（以下示例为snappy）  </p>
<pre><code>set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec;
</code></pre>
<h4 id="2）ORC"><a href="#2）ORC" class="headerlink" title="2）ORC"></a>2）ORC</h4><p>可在建表时声明    </p>
<pre><code>create table orc_table
(column_specs)
stored as orc
tblproperties (&quot;orc.compress&quot;=&quot;snappy&quot;);  
</code></pre>
<h4 id="3）Parquet"><a href="#3）Parquet" class="headerlink" title="3）Parquet"></a>3）Parquet</h4><p>可在建表时声明   </p>
<pre><code>create table orc_table
(column_specs)
stored as parquet
tblproperties (&quot;parquet.compression&quot;=&quot;snappy&quot;);
</code></pre>
<h3 id="计算过程中使用压缩"><a href="#计算过程中使用压缩" class="headerlink" title="计算过程中使用压缩"></a>计算过程中使用压缩</h3><p>1）单个mr的中间结果进行压缩  </p>
<p>–开启MapReduce中间数据压缩功能  </p>
<pre><code>set mapreduce.map.output.compress=true;
</code></pre>
<p>–设置MapReduce中间数据数据的压缩方式（以下示例为snappy） </p>
<pre><code>set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;  
</code></pre>
<p>2）单条sql语句的中间结果进行压缩    </p>
<p>单条SQL语句的中间结果是指，两个MR（一条SQL语句可能需要通过MR进行计算）之间的临时数据，可通过以下参数进行配置：   </p>
<p>–是否对两个MR之间的临时数据进行压缩  </p>
<pre><code>set hive.exec.compress.intermediate=true;  
</code></pre>
<p>–压缩格式（以下示例为snappy）  </p>
<pre><code>set hive.intermediate.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;
</code></pre>
<h1 id="能看到这里，你是真滴牛批"><a href="#能看到这里，你是真滴牛批" class="headerlink" title="能看到这里，你是真滴牛批~"></a>能看到这里，你是真滴牛批~</h1><h1 id="猛男，帅哥儿，靓仔，点个赞再肘"><a href="#猛男，帅哥儿，靓仔，点个赞再肘" class="headerlink" title="猛男，帅哥儿，靓仔，点个赞再肘~"></a>猛男，帅哥儿，靓仔，点个赞再肘~</h1><p><img src="/2023/07/30/hive_learn/7.png" alt="歪嘴猫"></p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/hexo_erro/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/hexo_erro/" class="post-title-link" itemprop="url">hexo上传报错问题记录</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-30 12:49:06 / 修改时间：13:00:17" itemprop="dateCreated datePublished" datetime="2023-07-30T12:49:06+08:00">2023-07-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上传博文到hexo时突然报错，如下图<br><img src="/2023/07/30/hexo_erro/1.png" alt="报错"></p>
<p>解决方案1：  </p>
<p>进入站点根目录<br>cd E:&#x2F;hexo</p>
<p>删除git提交内容文件夹<br>rm -rf .deploy_git&#x2F;</p>
<p>执行<br>git config –global core.autocrlf false</p>
<p>最后<br>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</p>
<p>依旧报错</p>
<p>后仔细阅读报错发现，github与git之间的网络通信存在问题</p>
<p>解决方案2：</p>
<p>重新建立本机git与github之间的通信</p>
<p>在git-bash命令行下，输入ssh-keygen -t rsa 命令（注意空格），表示我们指定 RSA 算法生成密钥，然后敲四次回车键，之后就就会生成两个文件，分别为秘钥 id_rsa 和公钥 id_rsa.pub. （注意：git中的复制粘贴不是 Ctrl+C 和 Ctrl+V，而是 Ctrl+insert 和 Shift+insert.）文件的位置在 Git Bash 上面都有显示，默认生成在以下目录：</p>
<p>Windows 10 ：C:&#x2F;Users&#x2F;ASUS&#x2F;.ssh</p>
<p><img src="/2023/07/30/hexo_erro/2.png" alt="&quot;sshkey配置&quot;"></p>
<p>验证是否成功，我们可以通过在 Git Bash 中输入 ssh -T <a href="mailto:&#x67;&#x69;&#116;&#64;&#x67;&#x69;&#x74;&#104;&#x75;&#98;&#46;&#99;&#x6f;&#x6d;">&#x67;&#x69;&#116;&#64;&#x67;&#x69;&#x74;&#104;&#x75;&#98;&#46;&#99;&#x6f;&#x6d;</a> 进行检验</p>
<p>注意查看日志，验证成功后，再依次输入hexo clean , hexo g , hexo d 发布博文</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/markdown%E8%AF%AD%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/markdown%E8%AF%AD%E6%B3%95/" class="post-title-link" itemprop="url">markdown语法</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-30 12:20:49 / 修改时间：12:21:10" itemprop="dateCreated datePublished" datetime="2023-07-30T12:20:49+08:00">2023-07-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>官网地址 <a target="_blank" rel="noopener" href="https://markdown.com.cn/basic-syntax/headings.html">https://markdown.com.cn/basic-syntax/headings.html</a>  </p>
<p>1.Markdown 标题语法<br>要创建标题，请在单词或短语前面添加井号 (#) 。# 的数量代表了标题的级别。例如，添加三个 # 表示创建一个三级标题  </p>
<p>2.段落语法<br>要创建段落，请使用空白行将一行或多行文本进行分隔</p>
<p>3.换行语法<br>在一行的末尾添加两个或多个空格，然后按回车键,即可创建一个换行</p>
<p>4.粗体<br>要加粗文本，请在单词或短语的前后各添加两个星号（asterisks）或下划线（underscores）</p>
<p>5.斜体<br>要用斜体显示文本，请在单词或短语前后添加一个星号（asterisk）或下划线（underscore）</p>
<p>6.引用语法<br>要创建块引用，请在段落前添加一个 &gt; 符号</p>
<p>7.代码语法<br>要将单词或短语表示为代码，请将其包裹在反引号 (<code> </code>) 中<br>将多行代码写入一个代码框中，采用（<code>`代码`</code>）的方式</p>
<p>8.分割线语法<br>要创建分隔线，请在单独一行上使用三个或多个星号 (***)、破折号 (—) 或下划线 (___) ，并且不能包含其他内容  </p>
<p>9.超链接语法<br>要创建分隔线，请在单独一行上使用三个或多个星号 (***)、破折号 (—) 或下划线 (___) ，并且不能包含其他内容</p>
<p>10.插入图片的语法<br>插入图片Markdown语法代码：![图片alt]（图片链接 “图片title”）</p>
<p>11.转义字符<br>要显示原本用于格式化 Markdown 文档的字符，请在字符前面添加反斜杠字符 \ </p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/photo-test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/photo-test/" class="post-title-link" itemprop="url">hexo博客插入本地图片的方法</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-30 10:32:03 / 修改时间：10:54:59" itemprop="dateCreated datePublished" datetime="2023-07-30T10:32:03+08:00">2023-07-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>hexo 中插入图片的操作步骤<br>1：要添加图片，先npm install 一个hexo-asset-image的依赖<br>   npm install <a target="_blank" rel="noopener" href="https://github.com/CodeFalling/hexo-asset-image">https://github.com/CodeFalling/hexo-asset-image</a> –save  </p>
<p>2.然后把_config.yml中的post_asset_folder设为true，这个配置的意思是每次new post一个博客，会增加一个和博客同名的文件夹，将需要写入博文的本地图片放到这个对应的文件夹中<br><img src="/2023/07/30/photo-test/1.png" alt="图片"></p>
<p>3.![图片描述]（.&#x2F;包名&#x2F;NO.01.001.jpg） 依照此格式代码引用本地图片</p>
<p>4.保存文件的修改之后，在命令行输入hexo clean,接着输入hexo g –d。在浏览器输入域名后查看相关文章，显示图片成功！！！</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/29/%E4%B8%AA%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/29/%E4%B8%AA%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/" class="post-title-link" itemprop="url">个人博客搭建流程</a>
        </h2>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-29 22:47:56" itemprop="dateCreated datePublished" datetime="2023-07-29T22:47:56+08:00">2023-07-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-30 10:42:52" itemprop="dateModified" datetime="2023-07-30T10:42:52+08:00">2023-07-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>个人博客搭建流程详细教程链接：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102592286">https://zhuanlan.zhihu.com/p/102592286</a></p>
<p>博客使用方式<br>在Blog目录下，将新创建的markdown文件上传到\Blog\source\_posts 目录下<br>右击文件夹空白处 进入git bash命令行界面<br>依次输入以下指令<br>hexo new post “first-page”  #创建新的博文<br>hexo clean<br>hexo g<br>hexo d<br>hexo s<br>运行完毕即可在 hadoopzyy.top或者本地localhost:4000查看博客内容</p>

      
    </div>

    
    
    
	
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=2042878838&auto=1&height=66"></iframe>

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张宴银"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">张宴银</p>
  <div class="site-description" itemprop="description">初级以内我无敌，中级以上我一换一</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; Sat Jul 29 2023 08:00:00 GMT+0800 (中国标准时间) – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张宴银</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>



    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共49.7k字</span>
</div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
