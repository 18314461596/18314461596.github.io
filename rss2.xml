<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>第五门徒</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>初级以内我无敌，中级以上我一换一</description>
    <pubDate>Sat, 26 Aug 2023 14:32:18 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Flink</title>
      <link>http://example.com/2023/08/24/Flink_1/</link>
      <guid>http://example.com/2023/08/24/Flink_1/</guid>
      <pubDate>Thu, 24 Aug 2023 01:58:32 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;宰相必起于州部，猛将必发于卒伍&quot;&gt;&lt;a href=&quot;#宰相必起于州部，猛将必发于卒伍&quot; class=&quot;headerlink&quot; title=&quot;宰相必起于州部，猛将必发于卒伍&quot;&gt;&lt;/a&gt;宰相必起于州部，猛将必发于卒伍&lt;/h1&gt;&lt;h1 id=&quot;枫叶云笔记&quot;&gt;&lt;a hre</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="宰相必起于州部，猛将必发于卒伍"><a href="#宰相必起于州部，猛将必发于卒伍" class="headerlink" title="宰相必起于州部，猛将必发于卒伍"></a>宰相必起于州部，猛将必发于卒伍</h1><h1 id="枫叶云笔记"><a href="#枫叶云笔记" class="headerlink" title="枫叶云笔记"></a><a href="https://cloud.fynote.com/share/d/GWHLLUWP">枫叶云笔记</a></h1><h2 id="Flink任务提交运行时架构"><a href="#Flink任务提交运行时架构" class="headerlink" title="Flink任务提交运行时架构"></a>Flink任务提交运行时架构</h2><p><img src="/2023/08/24/Flink_1/1.png" alt="Flink任务提交运行时架构"></p><ol><li>启动Flink集群首先会启动JobManager，Standalone集群模式下同时启动TaskManager，该模式资源也就固定；其他集群部署模式会根据提交任务来动态启动TaskManager；  </li><li>当在客户端提交任务后，客户端会将任务转换成JobGraph提交给JobManager；  </li><li>JobManager首先启动Dispatcher用于分发作业，运行Flink WebUI提供作业执行信息；  </li><li>Dispatcher启动后会启动JobMaster并将JobGraph提交给JobMaster，JobMaster会将JobGraph转换成可执行的ExecutionGraph。  </li><li>JobMaster向对应的资源管理器ResourceManager为当前任务申请Slot资源；  </li><li>在Standalone资源管理器中会直接找到启动的TaskManager来申请Slot资源，如果资源不足，那么任务执行失败；  </li><li>其他资源管理器会启动新的TaskManager，新启动的TaskManager会向ResourceManager进行注册资源，然后ResourceManager再向TaskManager申请Slot资源，如果资源不足会启动新的TaskManager来满足资源；  </li><li>TaskManager为对应的JobMaster offer Slot资源；  </li><li>JobMaster将要执行的task发送到对应的TaskManager上执行，TaskManager之间可以进行数据交换。</li></ol><h2 id="三种任务部署模式"><a href="#三种任务部署模式" class="headerlink" title="三种任务部署模式"></a>三种任务部署模式</h2><p>向Flink集群中提交任务有三种任务部署模式，分别如下：  </p><p>会话模式 - Session Mode<br>单作业模式 - Per-Job Mode(过时)<br>应用模式 - Application Mode    </p><p>以上三种任务提交模式的主要区别在于Flink集群的生命周期不同、资源的分配方式不同以及Flink 应用程序的main方法执行位置（Client客户端&#x2F;JobManager）不同。  </p><h3 id="会话模式-Session-Mode"><a href="#会话模式-Session-Mode" class="headerlink" title="会话模式 - Session Mode"></a>会话模式 - Session Mode</h3><p>Session模式下我们首先会启动一个集群，保持一个会话，这个会话中通过客户端提交作业，集群启动时所有的资源都已经确定，所以所有的提交的作业会竞争集群中的资源。这种模式适合单个作业规模小、执行时间短的大量作业。  </p><p>优势：只需要一个集群，所有作业提交之后都运行在这一个集群中，所有任务共享集群资源，每个任务执行完成后就释放资源。</p><p>缺点：因为集群资源是共享的，所以资源不够了，提交新的作业就会失败，如果一个作业发生故障导致TaskManager宕机，那么所有的作业都会受到影响。  </p><h3 id="单作业模式-Per-Job-Mode-过时"><a href="#单作业模式-Per-Job-Mode-过时" class="headerlink" title="单作业模式 - Per-Job Mode(过时)"></a>单作业模式 - Per-Job Mode(过时)</h3><p>Per-job模式是每提交一个作业会启动一个集群，集群只为这个作业而生，这种模式下客户端运行应用程序，然后启动集群，作业被提交给JobManager，进而分发给TaskManager执行，作业执行完成之后集群就会关闭，所有资源也会释放。  </p><h3 id="应用模式-Application-Mode"><a href="#应用模式-Application-Mode" class="headerlink" title="应用模式 - Application Mode"></a>应用模式 - Application Mode</h3><p>Session模式和Pre-Job模式都是在客户端将作业提交给JobManager。  </p><p>Application模式与Per-job类似，只是不需要客户端，每个Application提交之后就会启动一个JobManager，也就是创建一个集群，这个JobManager只为执行这一个Flink Application而存在，<br>Application中的多个job都会共用该集群，Application执行结束之后JobManager也就关闭了。这种模式下一个Application会动态创建自己的专属集群（JobManager）,所有任务共享该集群,不同<br>Application之间是完全隔离的，在生产环境中建议使用Application模式提交任务。    </p><p>Application模式是在JobManager上执行main方法，为每个Flink的Application创建一个Flink集群，如果该Application有多个任务，这些Flink任务共享一个集群。  </p><h2 id="Flink-On-Standalone任务提交"><a href="#Flink-On-Standalone任务提交" class="headerlink" title="Flink On Standalone任务提交"></a>Flink On Standalone任务提交</h2><p>Standlone集群部署时采用Session模式来构建集群。  </p><p>Flink On Standalone 任务提交支持Session会话模式和Application应用模式，不支持Per-Job单作业模式。  </p><h3 id="tandalone-Session模式"><a href="#tandalone-Session模式" class="headerlink" title="tandalone Session模式"></a>tandalone Session模式</h3><p>提交任务之前首先启动Standalone集群($FLINK_HOME&#x2F;bin&#x2F;start-cluster.sh)，然后再提交任务  </p><pre><code>[root@node4 ~]# cd /software/flink-1.16.0/bin/[root@node4 bin]# ./flink run -m node1:8081 -d -c com.mashibing.flinkjava.code.chapter3.SocketWordCount /root/FlinkJavaCode-1.0-SNAPSHOT-jar-with-dependencies.jar</code></pre><p>-m –jobmanager,指定提交任务连接的JobManager地址<br>-c –class,指定运行的class主类<br>-d –detached，任务提交后在后台独立运行，退出客户端，也可不指定<br>-p –parallelism,执行程序的并行度  </p><p>也可以通过参数”pipeline.name”来自定义指定Job 名称，提交命令如下：  </p><pre><code>[root@node4 bin]# ./flink run -m node1:8081 -d -Dpipeline.name=socket-wc -c com.mashibing.flinkjava.code.chapter3.SocketWordCount /root/FlinkJavaCode-1.0-SNAPSHOT-jar-with-dependencies.jar  </code></pre><h4 id="Standalone-Session模式任务提交流程"><a href="#Standalone-Session模式任务提交流程" class="headerlink" title="Standalone Session模式任务提交流程"></a>Standalone Session模式任务提交流程</h4><p>Standalone Session模式提交任务中首先需要创建Flink集群，集群创建启动的同时Dispatcher、JobMaster、ResourceManager对象一并创建、TaskManager也一并启动，TaskManager会向集群ResourceManager汇报Slot信息，Flink集群资源也就确定了。Standalone Session模式提交任务流程如下：  </p><p><img src="/2023/08/24/Flink_1/2.png" alt="Standalone Session模式任务提交流程">  </p><pre><code>    1.在客户端提交Flink任务，客户端会将任务转换成JobGraph提交给JobManager。      2.Dispatcher将提交任务提交给JobMaster。      3.JobMaster向ResourceManager申请Slot资源。      4.ResourceManager会在对应的TaskManager上划分Slot资源。      5.TaskManager向JobMaster offer Slot资源。      6.JobMaster将任务对应的task发送到TaskManager上执行。    </code></pre><h3 id="Standalone-Application模式"><a href="#Standalone-Application模式" class="headerlink" title="Standalone Application模式"></a>Standalone Application模式</h3><p>Standalone Application模式中不会预先创建Flink集群，在提交Flink 任务的同时会创建JobManager，启动Flink集群,然后需要手动启动TaskManager连接该Flink集群，启动的TaskManager会根据$FLINK_HOME&#x2F;conf&#x2F;flink-conf.yaml配置文件中的”jobmanager.rpc.address”配置找JobManager，所以这里选择在node1节点上提交任务并启动JobManager，方便后续其他节点启动TaskManager后连接该节点。  </p><h4 id="任务提交指令及顺序"><a href="#任务提交指令及顺序" class="headerlink" title="任务提交指令及顺序"></a>任务提交指令及顺序</h4><h5 id="1-准备Flink-jar包"><a href="#1-准备Flink-jar包" class="headerlink" title="1.准备Flink jar包"></a>1.准备Flink jar包</h5><p>在node1节点上将Flink 打好的”FlinkJavaCode-1.0-SNAPSHOT-jar-with-dependencies.jar”jar包放在 $FLINK_HOME&#x2F;lib目录下  </p><h5 id="2-提交任务，在node1-节点上启动-JobManager"><a href="#2-提交任务，在node1-节点上启动-JobManager" class="headerlink" title="2.提交任务，在node1 节点上启动 JobManager"></a>2.提交任务，在node1 节点上启动 JobManager</h5><pre><code>[root@node1 ~]# cd /software/flink-1.16.0/bin/执行如下命令，启动JobManager  [root@node1 bin]# ./standalone-job.sh start --job-classname com.mashibing.flinkjava.code.chapter3.SocketWordCount  </code></pre><h5 id="3-启动TaskManager"><a href="#3-启动TaskManager" class="headerlink" title="3. 启动TaskManager"></a>3. 启动TaskManager</h5><p>在node1、node2、node3任意一台节点上启动taskManager，根据$FLINK_HOME&#x2F;conf&#x2F;flinkconf.yaml配置文件中”jobmanager.rpc.address”配置项会找到对应node1 JobManager  </p><pre><code>在node1节点上启动TaskManager[root@node1 ~]# cd /software/flink-1.16.0/bin/[root@node1 bin]# ./taskmanager.sh start在node2节点上启动TaskManager[root@node2 ~]# cd /software/flink-1.16.0/bin/[root@node2 bin]# ./taskmanager.sh start</code></pre><p>启动两个TaskManager后可以看到Flink WebUI中对应的有2个TaskManager，可以根据自己任务使用资源的情况，手动启动多个TaskManager。    </p><h5 id="4-停止集群"><a href="#4-停止集群" class="headerlink" title="4.停止集群"></a>4.停止集群</h5><pre><code>停止启动的JobManager[root@node1 bin]# ./standalone-job.sh stop停止启动的TaskManager[root@node1 bin]# ./taskmanager.sh stop[root@node2 bin]# ./taskmanager.sh stop  </code></pre><p>可以以同样的方式在其他节点上以Standalone Application模式提交先前的Flink任务，但是每次提交都是当前提交任务独享集群资源。    </p><p>Standalone Application模式提交任务中提交任务的同时会启动JobManager创建Flink集群，但是需要手动启动TaskManager，这样提交的任务才能正常运行，如果提交的任务使用资源多，还可以启动多个TaskManager。  </p><h4 id="Standalone-Application模式任务提交流程"><a href="#Standalone-Application模式任务提交流程" class="headerlink" title="Standalone Application模式任务提交流程"></a>Standalone Application模式任务提交流程</h4><p><img src="/2023/08/24/Flink_1/3.png" alt="Standalone Application模式任务提交流程">   </p><pre><code>    1.在客户端提交Flink任务的同时启动JobManager，客户端会将任务转换成JobGraph提交给JobManager。    2.Dispatcher会启动JobMaster，Dispatcher将提交任务提交给JobMaster。    3.JobMaster向ResourceManager申请Slot资源。    4.手动启动TaskManager，TaskManager会向ResourceManager注册Slot资源    5.ResourceManager会在对应的TaskManager上划分Slot资源。    6.TaskManager向JobMaster offer Slot资源。    7.JobMaster将任务对应的task发送到TaskManager上执行。  </code></pre><h3 id="tandalone-Session模式-和-Standalone-Application模式-任务提交流程的区别"><a href="#tandalone-Session模式-和-Standalone-Application模式-任务提交流程的区别" class="headerlink" title="tandalone Session模式 和 Standalone Application模式 任务提交流程的区别"></a>tandalone Session模式 和 Standalone Application模式 任务提交流程的区别</h3><p>Standalone Session模式中启动Flink集群时JobManager、TaskManager、JobMaster会预先启动<br>Standalone Application模式中提交任务时同时启动集群JobManager、JobMaster，需要手动启动TaskManager</p><h2 id="Flink-On-Yarn任务提交"><a href="#Flink-On-Yarn任务提交" class="headerlink" title="Flink On Yarn任务提交"></a>Flink On Yarn任务提交</h2><h3 id="Flink-On-Yarn运行原理"><a href="#Flink-On-Yarn运行原理" class="headerlink" title="Flink On Yarn运行原理"></a>Flink On Yarn运行原理</h3><p><img src="/2023/08/24/Flink_1/4.png" alt="Flink On Yarn运行原理">     </p><pre><code>    1.当启动一个新的Flink YARN Client会话时，客户端首先会检查所请求的资源（容器和内存）是否可用，之后，它会上传Flink配置和JAR文件到HDFS。      2.客户端的下一步是向ResourceManager请求一个YARN容器启动ApplicationMaster。JobManager和ApplicationMaster(AM)运行在同一个容器中，一旦它们成功地启动了，AM就能够知JobManager的地址，它会为TaskManager生成一个新的Flink配置文件（这样它才能连上JobManager），该文件也同样会被上传到HDFS。另外，AM容器还提供了Flink的Web界面服务。Flink用来提供服务的端是由用户和应用程序ID作为偏移配置的，这使得用户能够并行执行多个YARN会话。        3.之后，AM开始为Flink的TaskManager分配容器（Container），从HDFS下载JAR文件和修改过的配置文件，一旦这些步骤完成了，Flink就可以基于Yarn运行任务了  </code></pre><h3 id="代码及Yarn环境准备"><a href="#代码及Yarn环境准备" class="headerlink" title="代码及Yarn环境准备"></a>代码及Yarn环境准备</h3><h4 id="1-准备代码"><a href="#1-准备代码" class="headerlink" title="1.准备代码"></a>1.准备代码</h4><p>Flink On Yarn任务提交支持Session会话模式、Per-Job单作业模式、Application应用模式。  </p><p>Flink允许在一个main方法中提交多个job任务，多Job执行的顺序不受部署模式影响，但受启动Job的调用影响，每次调用execute()或者executeAsyc()方法都会触发job执行，我们可以在一个Flink Application中执行多次execute()或者executeAsyc()方法来触发多个job执行，两者区别如下：  </p><pre><code>execute()：该方法为阻塞方法，当一个Flink Application中执行多次execute()方法触发多个job时，下一个job的执行会被推迟到该job执行完成后再执行。  executeAsyc()：该方法为非阻塞方法，一旦调用该方法触发job后，后续还有job也会立即提交执行。</code></pre><p>当一个Flink Application中有多个job时，这些job之间没有直接通信的机制，所以建议编写Flink代码时一个Application中包含一个job即可，目前只有非HA的Application模式可以支持多job运行。  </p><pre><code>//1.准备环境StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();//2.读取Socket数据 ,获取ds1和ds2DataStreamSource&lt;String&gt; ds1 = env.socketTextStream(&quot;node5&quot;, 8888);DataStreamSource&lt;String&gt; ds2 = env.socketTextStream(&quot;node5&quot;, 9999);//3.1 对ds1 直接输出原始数据SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; transDs1 = ds1.flatMap((String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) -&gt; &#123;    String[] words = line.split(&quot;,&quot;);    for (String word : words) &#123;        out.collect(Tuple2.of(word, 1));    &#125;&#125;).returns(Types.TUPLE(Types.STRING, Types.INT));transDs1.print();env.executeAsync(&quot;first job&quot;);//3.2 对ds2准备K,V格式数据 ,统计实时WordCountSingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tupleDS = ds2.flatMap((String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) -&gt; &#123;    String[] words = line.split(&quot;,&quot;);    for (String word : words) &#123;        out.collect(Tuple2.of(word, 1));    &#125;&#125;).returns(Types.TUPLE(Types.STRING, Types.INT));tupleDS.keyBy(tp -&gt; tp.f0).sum(1).print();//5.execute触发执行env.execute(&quot;second job&quot;);  </code></pre><h4 id="2-yarn-环境准备"><a href="#2-yarn-环境准备" class="headerlink" title="2.yarn 环境准备"></a>2.yarn 环境准备</h4><p>在Per-Job模式中，Flink每个job任务都会启动一个对应的Flink集群，基于Yarn提交后会在Yarn中同时运行多个实时Flink任务，在HDFS中$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;capacity-scheduler.xml中有”yarn.scheduler.capacity.maximum-am-resource-percent”配置项，该项默认值为0.1，表示Yarn集群中运行的所有ApplicationMaster的资源比例上限，默认0.1表示10%，这个参数变相控制了处于活动状态的Application个数，所以这里我们修改该值为0.5，否则后续在Yarn中运行多个Flink Application时只有一个Application处于活动运行状态，其他处于Accepted状态.  </p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%AE%B0%E7%9B%B8%E5%BF%85%E8%B5%B7%E4%BA%8E%E5%B7%9E%E9%83%A8%EF%BC%8C%E7%8C%9B%E5%B0%86%E5%BF%85%E5%8F%91%E4%BA%8E%E5%8D%92%E4%BC%8D/">宰相必起于州部，猛将必发于卒伍</category>
      
      
      <comments>http://example.com/2023/08/24/Flink_1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>java问题集</title>
      <link>http://example.com/2023/08/18/Java_questions/</link>
      <guid>http://example.com/2023/08/18/Java_questions/</guid>
      <pubDate>Fri, 18 Aug 2023 03:30:12 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;道虽迩，不行不至；事虽小，不为不成&quot;&gt;&lt;a href=&quot;#道虽迩，不行不至；事虽小，不为不成&quot; class=&quot;headerlink&quot; title=&quot;道虽迩，不行不至；事虽小，不为不成&quot;&gt;&lt;/a&gt;道虽迩，不行不至；事虽小，不为不成&lt;/h1&gt;&lt;h2 id=&quot;java中</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="道虽迩，不行不至；事虽小，不为不成"><a href="#道虽迩，不行不至；事虽小，不为不成" class="headerlink" title="道虽迩，不行不至；事虽小，不为不成"></a>道虽迩，不行不至；事虽小，不为不成</h1><h2 id="java中double类型数据如何指定保留位数？"><a href="#java中double类型数据如何指定保留位数？" class="headerlink" title="java中double类型数据如何指定保留位数？"></a>java中double类型数据如何指定保留位数？</h2><p><img src="/2023/08/18/Java_questions/1.png" alt="java中double类型数据如何指定保留位数"></p><h2 id="字符串的内容比较使用-为啥无效？"><a href="#字符串的内容比较使用-为啥无效？" class="headerlink" title="字符串的内容比较使用&#x3D;&#x3D;为啥无效？"></a>字符串的内容比较使用&#x3D;&#x3D;为啥无效？</h2><p>字符串的内容比较应该使用方法equals</p><pre><code>if(&quot;丁真&quot;.equals(name) &amp;&amp; &quot;666&quot;.equals(passwd))&#123;        //if(name == &quot;丁真&quot; &amp;&amp; passwd == &quot;666&quot;)&#123;  字符串判断相等时，不能使用 == ，会无效        System.out.println(&quot;恭喜你，登录成功~&quot;);        break;        &#125;else&#123;            chance--;            System.out.println(&quot;你还有&quot;+chance+&quot;次登录机会&quot;);        &#125;</code></pre><p>equals方法使用细节：  </p><pre><code>System.out.println(name.equals(&quot;林黛玉&quot;));System.out.println(&quot;林黛玉&quot;.equals(name));//T [推荐，可以避免空指针</code></pre><h2 id="Java中系统输入-char-字符-‘男’-应该如何做？"><a href="#Java中系统输入-char-字符-‘男’-应该如何做？" class="headerlink" title="Java中系统输入 char 字符 ‘男’ 应该如何做？"></a>Java中系统输入 char 字符 ‘男’ 应该如何做？</h2><pre><code>Scanner myScanner = new Scanner(System.in);char gender = myScanner.next().charAt(0); if (gender == &#39;男&#39;)&#123;            System.out.println(&quot;进入男子组&quot;);        &#125;else if(gender == &#39;女&#39;)&#123;            System.out.println(&quot;进入女子组&quot;);        &#125;else&#123;            System.out.println(&quot;你的性别有误，不能参加决赛 ~ &quot;);        &#125;</code></pre><h2 id="Java中数组的深拷贝和浅拷贝"><a href="#Java中数组的深拷贝和浅拷贝" class="headerlink" title="Java中数组的深拷贝和浅拷贝"></a>Java中数组的深拷贝和浅拷贝</h2><p>深拷贝： int[] arr2 &#x3D; new int[arr1.length]  ,arr2数组重新开辟了一个数组空间<br>浅拷贝： arr2 &#x3D; arr1 这是将arr1的数组地址赋值给了arr2，这样arr1和arr2其实都指向了同一个数组，此时arr1和arr2的元素都是同步变化的，并没有独立。</p><h2 id="Java-中对象在内存中的存在形式是怎样的"><a href="#Java-中对象在内存中的存在形式是怎样的" class="headerlink" title="Java 中对象在内存中的存在形式是怎样的?"></a>Java 中对象在内存中的存在形式是怎样的?</h2><p><img src="/2023/08/18/Java_questions/2.png" alt="Java 中对象在内存中的存在形式"></p><p>数值属性存储在堆内存中  </p><p>方法存储在方法区中  </p><p>方法区中的常量池负责存储数值属性之外的其他属性值  </p><h3 id="类和对象的内存分配机制"><a href="#类和对象的内存分配机制" class="headerlink" title="类和对象的内存分配机制"></a>类和对象的内存分配机制</h3><h4 id="Java-内存的结构分析"><a href="#Java-内存的结构分析" class="headerlink" title="Java 内存的结构分析"></a>Java 内存的结构分析</h4><ol><li><p>栈： 一般存放基本数据类型(局部变量)  </p></li><li><p>堆： 存放对象(Cat cat , 数组等)   </p></li><li><p>方法区：常量池(常量，比如字符串)， 类加载信息</p></li></ol><h2 id="类定义在public-class-代码块内外有何区别？-什么时候需要使用static-class-？"><a href="#类定义在public-class-代码块内外有何区别？-什么时候需要使用static-class-？" class="headerlink" title="类定义在public class 代码块内外有何区别？ 什么时候需要使用static class ？"></a>类定义在public class 代码块内外有何区别？ 什么时候需要使用static class ？</h2><p><img src="/2023/08/18/Java_questions/3.png" alt="类定义在public class 代码块内外有何区别？"></p><p><img src="/2023/08/18/Java_questions/4.png" alt="类定义在public class 代码块内外有何区别？"></p><p>在别的代码中定义了同名类，本次代码中需要重新定义时，类应当定义在public class代码块中，使用static描述，否则会报错.这个是叫静态内部类吗？  </p><p>类本质上就是一种数据类型。可以定义属性，定义方法。使用的时候和数据类型一样使用即可。  </p><h3 id="方法的调用机制原理"><a href="#方法的调用机制原理" class="headerlink" title="方法的调用机制原理"></a>方法的调用机制原理</h3><p><img src="/2023/08/18/Java_questions/5.png" alt="方法的调用机制原理"></p><h2 id="形参和实参"><a href="#形参和实参" class="headerlink" title="形参和实参"></a>形参和实参</h2><pre><code>getSumAndSub(int n1,int n2)  这就是形参  getSumAndSub(1,8) 这就是实参</code></pre><h2 id="基本数据类型的传参机制"><a href="#基本数据类型的传参机制" class="headerlink" title="基本数据类型的传参机制"></a>基本数据类型的传参机制</h2><p>基本数据类型，传递的是值(值拷贝),形参的任何改变不影响实参</p><p>main方法中的引用类型属性可以被方法修改    </p><p>main方法中的基本类型属性无法被方法修改  </p><p>引用类型传递的是地址（传递也是值，但是值是地址），可以通过形参影响实参！  </p><h2 id="Java中的可变参数"><a href="#Java中的可变参数" class="headerlink" title="Java中的可变参数"></a>Java中的可变参数</h2><p>int… 与 int[] 等价  </p><p>可变参数可以和普通类型的参数一起放在形参列表,但必须保证可变参数在最后    </p><p>一个形参列表中只能出现一个可变参数  </p><p>可变参数的实参可以是数组 </p><h2 id="全局变量和局部变量"><a href="#全局变量和局部变量" class="headerlink" title="全局变量和局部变量"></a>全局变量和局部变量</h2><p>全局变量就是类的属性<br>全局变量(属性)可以不赋值,直接使用,因为有默认值  </p><p>局部变量是类中方法代码块里定义的变量<br>局部变量必须赋值后,才能使用,因为没有默认值</p><p>方法与方法之间的局部变量无法相互调用<br>方法可以直接调用类的属性   </p><p>类和类之间的属性可以相互调用  </p><h2 id="四种访问修饰符的区别"><a href="#四种访问修饰符的区别" class="headerlink" title="四种访问修饰符的区别"></a>四种访问修饰符的区别</h2><h2 id="类，方法，构造器的定义语法汇总"><a href="#类，方法，构造器的定义语法汇总" class="headerlink" title="类，方法，构造器的定义语法汇总"></a>类，方法，构造器的定义语法汇总</h2><h3 id="类的定义语法"><a href="#类的定义语法" class="headerlink" title="类的定义语法"></a>类的定义语法</h3><pre><code>class Dog&#123;        &#125;</code></pre><h3 id="方法的定义语法"><a href="#方法的定义语法" class="headerlink" title="方法的定义语法"></a>方法的定义语法</h3><pre><code>public void getname(String name)&#123;    System.out.println(name);        &#125;</code></pre><h3 id="构造器的定义语法"><a href="#构造器的定义语法" class="headerlink" title="构造器的定义语法"></a>构造器的定义语法</h3><p>构造器是特殊的方法  </p><p>1.方法名和类名一致<br>2.方法无返回值  </p><pre><code>class Dog&#123;    public Dog()&#123;            &#125;    public Dog(String str,Int num )&#123;            //...        &#125;    &#125;</code></pre><p>构造器的使用  </p><pre><code>Dog dog1 = new Dog(&quot;Str1&quot;,10);  Dog dog2 = new Dog();  </code></pre><h2 id="java代码反编译"><a href="#java代码反编译" class="headerlink" title="java代码反编译"></a>java代码反编译</h2><p>javap指令,反编译    </p><h2 id="IDEA的常用快捷键有哪些"><a href="#IDEA的常用快捷键有哪些" class="headerlink" title="IDEA的常用快捷键有哪些 ?"></a>IDEA的常用快捷键有哪些 ?</h2><ol><li>删除当前行, 默认是 ctrl + Y 自己配置 ctrl + d  </li><li>复制当前行, 自己配置 ctrl + alt + 向下光标  </li><li>补全代码 alt + &#x2F;  </li><li>添加注释和取消注释 ctrl + &#x2F; 【第一次是添加注释，第二次是取消注释】  </li><li>导入该行需要的类 先配置 auto import , 然后使用 alt+enter 即可  </li><li>快速格式化代码 ctrl + alt + L  </li><li>快速运行程序 自己定义 alt + R  </li><li>生成构造器等 alt + insert [提高开发效率]  </li><li>查看一个类的层级关系 ctrl + H [学习继承后，非常有用]  </li><li>将光标放在一个方法上，输入 ctrl + B , 可以定位到方法 [学继承后，非常有用]  </li><li>自动的分配变量名 , 通过 在后面加 .var   </li><li>还有很多其它的快捷键</li></ol>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E9%81%93%E8%99%BD%E8%BF%A9%EF%BC%8C%E4%B8%8D%E8%A1%8C%E4%B8%8D%E8%87%B3%EF%BC%9B%E4%BA%8B%E8%99%BD%E5%B0%8F%EF%BC%8C%E4%B8%8D%E4%B8%BA%E4%B8%8D%E6%88%90/">道虽迩，不行不至；事虽小，不为不成</category>
      
      
      <comments>http://example.com/2023/08/18/Java_questions/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark报错经验记录</title>
      <link>http://example.com/2023/08/12/Spark-practice/</link>
      <guid>http://example.com/2023/08/12/Spark-practice/</guid>
      <pubDate>Sat, 12 Aug 2023 11:02:51 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;大鹏飞兮振八裔，中天摧兮力不济&quot;&gt;&lt;a href=&quot;#大鹏飞兮振八裔，中天摧兮力不济&quot; class=&quot;headerlink&quot; title=&quot;大鹏飞兮振八裔，中天摧兮力不济&quot;&gt;&lt;/a&gt;大鹏飞兮振八裔，中天摧兮力不济&lt;/h1&gt;&lt;h1 id=&quot;使用Spark的报错经验&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="大鹏飞兮振八裔，中天摧兮力不济"><a href="#大鹏飞兮振八裔，中天摧兮力不济" class="headerlink" title="大鹏飞兮振八裔，中天摧兮力不济"></a>大鹏飞兮振八裔，中天摧兮力不济</h1><h1 id="使用Spark的报错经验"><a href="#使用Spark的报错经验" class="headerlink" title="使用Spark的报错经验"></a>使用Spark的报错经验</h1><h2 id="IDEA工具中运行Spark程序报缺失scala-compiler-2-12-16-jar包"><a href="#IDEA工具中运行Spark程序报缺失scala-compiler-2-12-16-jar包" class="headerlink" title="IDEA工具中运行Spark程序报缺失scala-compiler-2.12.16.jar包"></a>IDEA工具中运行Spark程序报缺失scala-compiler-2.12.16.jar包</h2><p><img src="/2023/08/12/Spark-practice/1.png" alt="IDEA的Scala SDK设置位置">  </p><p>重新设置IDEA中Scala SDK即可  </p><h2 id="使用Spark-SQL往Mysql中写入数据，出现中文乱码"><a href="#使用Spark-SQL往Mysql中写入数据，出现中文乱码" class="headerlink" title="使用Spark SQL往Mysql中写入数据，出现中文乱码"></a>使用Spark SQL往Mysql中写入数据，出现中文乱码</h2><h3 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h3><p><a href="https://blog.csdn.net/MASILEJFOAISEGJIAE/article/details/89314591?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-89314591-blog-109695269.235%5Ev38%5Epc_relevant_sort_base2&spm=1001.2101.3001.4242.1&utm_relevant_index=4">解决方案</a></p><p>补充一下，如果在调整编码之前就已经创建了数据库和表的话，需要再单独修改数据库和表的编码格式  </p><p><img src="/2023/08/12/Spark-practice/2.png" alt="Spark SQL写入Mysql数据的案例代码">    </p><pre><code>package com.atguigu.bigdata.spark.core.sqlimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;SaveMode, SparkSession&#125;object Spark04_SparkSQL_JDBC &#123;  def main(args: Array[String]): Unit = &#123;    // TODO 创建SparkSQL的运行环境    val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)    val spark = SparkSession.builder().config(sparkConf).getOrCreate()    import spark.implicits._    // 读取MySQL数据    val df = spark.read      .format(&quot;jdbc&quot;)      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306/test&quot;)      .option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)      .option(&quot;user&quot;, &quot;root&quot;)      .option(&quot;password&quot;, &quot;Wo@6930886&quot;)      .option(&quot;dbtable&quot;, &quot;course&quot;)      .load()    df.show    // 保存数据    df.write      .format(&quot;jdbc&quot;)      .option(&quot;url&quot;,&quot;jdbc:mysql://hadoop101:3306/test?useUnicode=true&amp;characterEncoding=utf8&quot;)      .option(&quot;driver&quot;,&quot;com.mysql.jdbc.Driver&quot;)      .option(&quot;user&quot;,&quot;root&quot;)      .option(&quot;password&quot;,&quot;Wo@6930886&quot;)      .option(&quot;dbtable&quot;,&quot;course1&quot;)      .mode(SaveMode.Overwrite)      .save()    // TODO 关闭环境    spark.close()  &#125;&#125;</code></pre><h2 id="使用IDEA新建项目时，无法创建Scala代码"><a href="#使用IDEA新建项目时，无法创建Scala代码" class="headerlink" title="使用IDEA新建项目时，无法创建Scala代码"></a>使用IDEA新建项目时，无法创建Scala代码</h2><p>先随便创建一个文件，以scala结尾，之后代码编辑器右上角会弹出“设置Scala SDK”,点击设置之后，就可以正常创建Scala程序了  </p><p><img src="/2023/08/12/Spark-practice/3.png" alt="新增一个文件后缀写成scala">  </p><p><img src="/2023/08/12/Spark-practice/4.png" alt="可以创建Scala程序了">  </p><h2 id="IDEA中Alt-Enter自动导入import失效，且手动输入import语句，apache报错"><a href="#IDEA中Alt-Enter自动导入import失效，且手动输入import语句，apache报错" class="headerlink" title="IDEA中Alt + Enter自动导入import失效，且手动输入import语句，apache报错"></a>IDEA中Alt + Enter自动导入import失效，且手动输入import语句，apache报错</h2><p><img src="/2023/08/12/Spark-practice/5.png" alt="导包报错"> </p><p>原因是没有在pom文件中导入Spark相关包    </p><p>先在pom文件中配置参数导入Spark相关的包，再修改Maven仓库的版本为3.8.1以前的版本，重新使用maven加载包即可  </p><p><img src="/2023/08/12/Spark-practice/7.png" alt="导包报错">   </p><p>Pom文件配置示例  </p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;     xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;     xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;groupId&lt;/groupId&gt;&lt;artifactId&gt;SGG_scala_Spark3.0.0_practice&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;dependencies&gt;&lt;!--        spark--&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-yarn_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-streaming-kafka-0-10_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;&lt;!--mysql--&gt;    &lt;dependency&gt;        &lt;groupId&gt;mysql&lt;/groupId&gt;        &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;        &lt;version&gt;5.1.27&lt;/version&gt;    &lt;/dependency&gt;&lt;!--        jackson--&gt;    &lt;dependency&gt;        &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;        &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;        &lt;version&gt;2.9.9&lt;/version&gt;    &lt;/dependency&gt;&lt;!--        &lt;dependency&gt;--&gt;&lt;!--            &lt;groupId&gt; com.fasterxml.jackson.core &lt;/groupId&gt;--&gt;&lt;!--            &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;--&gt;&lt;!--            &lt;version&gt;2.9.9&lt;/version&gt;--&gt;&lt;!--        &lt;/dependency&gt;--&gt;        &lt;dependency&gt;        &lt;groupId&gt;com.thoughtworks.paranamer&lt;/groupId&gt;        &lt;artifactId&gt;paranamer&lt;/artifactId&gt;        &lt;version&gt;2.8&lt;/version&gt;    &lt;/dependency&gt;    &lt;!--common--&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;        &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;        &lt;version&gt;3.3&lt;/version&gt;    &lt;/dependency&gt;    &lt;!-- https://mvnrepository.com/artifact/com.alibaba/druid --&gt;    &lt;dependency&gt;        &lt;groupId&gt;com.alibaba&lt;/groupId&gt;        &lt;artifactId&gt;druid&lt;/artifactId&gt;        &lt;version&gt;1.1.10&lt;/version&gt;    &lt;/dependency&gt;&lt;!--hive--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;            &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;            &lt;version&gt;2.1.1&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;            &lt;artifactId&gt;hive-metastore&lt;/artifactId&gt;            &lt;version&gt;2.1.1&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><p>清除缓存重启IDEA    </p><p><img src="/2023/08/12/Spark-practice/8.png" alt="IDEA中修复import Spark包报错问题">   </p><p>OK鸟 ~  </p><h2 id="IDEA中下载源码和查看源码的方法"><a href="#IDEA中下载源码和查看源码的方法" class="headerlink" title="IDEA中下载源码和查看源码的方法"></a>IDEA中下载源码和查看源码的方法</h2><p><img src="/2023/08/12/Spark-practice/6.png" alt="IDEA中下载源码"> </p><p>ctrl + H 跳到源码内      </p><h2 id="IDEA中编写好Scala程序后无法运行程序"><a href="#IDEA中编写好Scala程序后无法运行程序" class="headerlink" title="IDEA中编写好Scala程序后无法运行程序"></a>IDEA中编写好Scala程序后无法运行程序</h2><p><img src="/2023/08/12/Spark-practice/9.png" alt="IDEA中无法运行Scala程序">  </p><p>手动构建应用程序  </p><p><img src="/2023/08/12/Spark-practice/10.png" alt="手动构建应用程序"></p><p>Scala编写Spark运行程序时，应当选用Object而非class类型文件  </p><p><img src="/2023/08/12/Spark-practice/11.png" alt="Scala编写Spark程序选用Object">  </p><h2 id="IDEA中本地编写Spark程序，提交到Yarn集群上运行出错"><a href="#IDEA中本地编写Spark程序，提交到Yarn集群上运行出错" class="headerlink" title="IDEA中本地编写Spark程序，提交到Yarn集群上运行出错"></a>IDEA中本地编写Spark程序，提交到Yarn集群上运行出错</h2><p>1.修改IDEA中本地代码  </p><p>本地开发测试时，val SparkConf &#x3D; new SparkConf().setMaster(“local[<em>]”).setAppName(“SparkSQL_mysql_join_mysql”)<br>打包上传到yarn集群运行前，需要删掉setMaster(“local[</em>]”)，即改成val SparkConf &#x3D; new SparkConf().setAppName(“SparkSQL_mysql_join_mysql”)  </p><p><img src="/2023/08/12/Spark-practice/12.png" alt="Spark程序打包前修改Sparkconf参数">  </p><p>2.先Maven clean，再构建项目，生成target文件夹，尤其注意，一定要获取到target&#x2F;classes文件夹   </p><p><img src="/2023/08/12/Spark-practice/13.png" alt="Spark程序打包"> </p><p>3.将打好的Spark程序jar包提交到服务器上  </p><p><img src="/2023/08/12/Spark-practice/14.png" alt="Spark程序jar包上传到服务器上">  </p><p>4.如果程序运行时需要调用到其他Jar包的类，集群运行时会报错，需要添加jars参数指定jar包位置  </p><pre><code>/opt/cloudera/parcels/CDH/lib/spark/bin/spark-submit --jars /opt/module/spark/mysql-connector-java.jar --class com.zyy.sparksql.SparkSQL_mysql_join_mysql --master yarn --deploy-mode cluster /opt/module/spark/SGG_scala_Spark3.0.0_practice-1.0-SNAPSHOT.jar 10  </code></pre><p>5.查看运行结果，运行成功   </p><p><img src="/2023/08/12/Spark-practice/15.png" alt="Spark程序运行成功">   </p><p><img src="/2023/08/12/Spark-practice/16.png" alt="Spark程序运行成功">    </p><p><img src="/2023/08/12/Spark-practice/17.png" alt="Spark History中运行记录"></p><p><img src="/2023/08/12/Spark-practice/18.png" alt="Spark History中查看工作流图">   </p><h2 id="IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表"><a href="#IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表" class="headerlink" title="IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表"></a>IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表</h2><p><img src="/2023/08/12/Spark-practice/19.png" alt="Hive_2_mysql">  </p><p>代码示例 </p><pre><code>package com.zyy.sparksqlimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;SaveMode, SparkSession&#125;object SparkSQL_Hive_2_Mysql &#123;  def main(args: Array[String]): Unit = &#123;    System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;)    // 1. TODO 创建SparkSQL运行环境    val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL_Hive&quot;)    val spark = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()    // 2. 编写Spark 操作Hive 的计算逻辑    //spark.sql(&quot;show databases&quot;).show()    //spark.sql(&quot;use hive_tiaoyou&quot;)    //spark.sql(&quot;show tables&quot;).show()    //spark.sql(&quot;select * from order_detail&quot;).show()    //spark.sql(&quot;select * from province_info as info1 left join province_info info2 on info1.id = info2.id&quot;).show()    spark.sql(&quot;use hive_tiaoyou&quot;)    val databases = spark.sql(&quot;show tables&quot;)    databases.show()    databases.write.format(&quot;jdbc&quot;)      .option(&quot;url&quot;,&quot;jdbc:mysql://hadoop101:3306/test?useUnicode=true&amp;characterEncoding=utf8&quot;)      .option(&quot;driver&quot;,&quot;com.mysql.jdbc.Driver&quot;)      .option(&quot;user&quot;,&quot;root&quot;)      .option(&quot;password&quot;,&quot;Wo@6930886&quot;)      .option(&quot;dbtable&quot;,&quot;test_databases&quot;)      .mode(SaveMode.Overwrite)      .save()    // 3. TODO 关闭环境    spark.close()      &#125;    &#125;  </code></pre><p>Spark要操作Hive之前，需要注意，要先把Hive-site.xml,hdfs-site.xml,core-site.xml,yarn-site.xml这几个文件从集群中下载下来，放到IDEA中resourses文件夹下  </p><p>之后通过创建环境时，设置enableHiveSupport，即可通过spark.sql(“Hive语句”)进行访问Hive的数据  </p><pre><code>val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL_Hive&quot;)  val spark = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()</code></pre>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%A4%A7%E9%B9%8F%E9%A3%9E%E5%85%AE%E6%8C%AF%E5%85%AB%E8%A3%94%EF%BC%8C%E4%B8%AD%E5%A4%A9%E6%91%A7%E5%85%AE%E5%8A%9B%E4%B8%8D%E6%B5%8E/">大鹏飞兮振八裔，中天摧兮力不济</category>
      
      
      <comments>http://example.com/2023/08/12/Spark-practice/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark-Streaming</title>
      <link>http://example.com/2023/08/11/Spark-Streaming/</link>
      <guid>http://example.com/2023/08/11/Spark-Streaming/</guid>
      <pubDate>Fri, 11 Aug 2023 14:03:30 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也&quot;&gt;&lt;a href=&quot;#英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也&quot; class=&quot;headerlink&quot; title=&quot;英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也&quot;&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也"><a href="#英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也" class="headerlink" title="英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也"></a>英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也</h1><h1 id="SparkStreaming-概述"><a href="#SparkStreaming-概述" class="headerlink" title="SparkStreaming 概述"></a>SparkStreaming 概述</h1><h2 id="Spark-Streaming-是什么"><a href="#Spark-Streaming-是什么" class="headerlink" title="Spark Streaming 是什么"></a>Spark Streaming 是什么</h2><p>Spark Streaming 用于流式数据的处理。Spark Streaming 支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如：map、reduce、join、window 等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。  </p><h2 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h2><p>和 Spark 基于 RDD 的概念很相似，Spark Streaming 使用离散化流(discretized stream)作为抽象表示，叫作 DStream。  </p><p>DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此得名“离散化”)。所以简单来将，DStream 就是对 RDD 在实时数据处理场景的一种封装。  </p><h2 id="Spark-Streaming-的特点"><a href="#Spark-Streaming-的特点" class="headerlink" title="Spark Streaming 的特点"></a>Spark Streaming 的特点</h2><p>易用    </p><p>容错  </p><p>易整合到 Spark 体系    </p><h2 id="Spark-Streaming-架构"><a href="#Spark-Streaming-架构" class="headerlink" title="Spark Streaming 架构"></a>Spark Streaming 架构</h2><p><img src="/2023/08/11/Spark-Streaming/1.png" alt="整体架构图">  </p><p><img src="/2023/08/11/Spark-Streaming/2.png" alt="架构图"> </p><h2 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h2><p>背压机制（即 Spark Streaming Backpressure）: 根据JobScheduler 反馈作业的执行信息来动态调整 Receiver 数据接收率。  </p><p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用 backpressure 机制，默认值false，即不启用。    </p><h2 id="Dstream-入门"><a href="#Dstream-入门" class="headerlink" title="Dstream 入门"></a>Dstream 入门</h2><h3 id="WordCount-案例实操"><a href="#WordCount-案例实操" class="headerlink" title="WordCount 案例实操"></a>WordCount 案例实操</h3><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><pre><code>&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;    </code></pre><h4 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h4><pre><code>object StreamWordCount &#123; def main(args: Array[String]): Unit = &#123;     //1.初始化 Spark 配置信息     val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;StreamWordCount&quot;)     //2.初始化 SparkStreamingContext     val ssc = new StreamingContext(sparkConf, Seconds(3))     //3.通过监控端口创建 DStream，读进来的数据为一行行     val lineStreams = ssc.socketTextStream(&quot;linux1&quot;, 9999)     //将每一行数据做切分，形成一个个单词     val wordStreams = lineStreams.flatMap(_.split(&quot; &quot;))     //将单词映射成元组（word,1）     val wordAndOneStreams = wordStreams.map((_, 1))     //将相同的单词次数做统计     val wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_)     //打印     wordAndCountStreams.print()     //启动 SparkStreamingContext     ssc.start()     ssc.awaitTermination()     &#125;    &#125;  </code></pre><h4 id="WordCount-解析"><a href="#WordCount-解析" class="headerlink" title="WordCount 解析"></a>WordCount 解析</h4><p>在内部实现上，DStream 是一系列连续的 RDD 来表示。每个 RDD 含有一段时间间隔内的数据。</p><p><img src="/2023/08/11/Spark-Streaming/3.png" alt="DStream">   </p><p>对数据的操作也是按照 RDD 为单位来进行的  </p><p><img src="/2023/08/11/Spark-Streaming/4.png" alt="DStream对数据的操作">     </p><p>计算过程由 Spark Engine 来完成  </p><p><img src="/2023/08/11/Spark-Streaming/5.png" alt="DStream的计算过程"> </p><h2 id="DStream创建"><a href="#DStream创建" class="headerlink" title="DStream创建"></a>DStream创建</h2><h3 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h3><h4 id="Kafka-0-10-Direct-模式"><a href="#Kafka-0-10-Direct-模式" class="headerlink" title="Kafka 0-10 Direct 模式"></a>Kafka 0-10 Direct 模式</h4><h5 id="需求：通过-SparkStreaming-从-Kafka-读取数据，并将读取过来的数据做简单计算，最终打印到控制台。"><a href="#需求：通过-SparkStreaming-从-Kafka-读取数据，并将读取过来的数据做简单计算，最终打印到控制台。" class="headerlink" title="需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。"></a>需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</h5><h5 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h5><pre><code>&lt;dependency&gt;     &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;     &lt;artifactId&gt;spark-streaming-kafka-0-10_2.12&lt;/artifactId&gt;     &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;     &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;     &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;     &lt;version&gt;2.10.1&lt;/version&gt;&lt;/dependency&gt;  </code></pre><h5 id="编写代码-1"><a href="#编写代码-1" class="headerlink" title="编写代码"></a>编写代码</h5><pre><code>import org.apache.kafka.clients.consumer.&#123;ConsumerConfig, ConsumerRecord&#125;  import org.apache.spark.SparkConf  import org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;  import org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;  import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;  object DirectAPI &#123;     def main(args: Array[String]): Unit = &#123;     //1.创建 SparkConf     val sparkConf: SparkConf = new      SparkConf().setAppName(&quot;ReceiverWordCount&quot;).setMaster(&quot;local[*]&quot;)     //2.创建 StreamingContext     val ssc = new StreamingContext(sparkConf, Seconds(3))     //3.定义 Kafka 参数     val kafkaPara: Map[String, Object] = Map[String, Object](     ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; &quot;linux1:9092,linux2:9092,linux3:9092&quot;,     ConsumerConfig.GROUP_ID_CONFIG -&gt; &quot;atguigu&quot;,     &quot;key.deserializer&quot; -&gt;     &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;,     &quot;value.deserializer&quot; -&gt;     &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;     )     //4.读取 Kafka 数据创建 DStream     val kafkaDStream: InputDStream[ConsumerRecord[String, String]] =     KafkaUtils.createDirectStream[String, String](ssc,     LocationStrategies.PreferConsistent,     ConsumerStrategies.Subscribe[String, String](Set(&quot;atguigu&quot;), kafkaPara))     //5.将每条消息的 KV 取出     val valueDStream: DStream[String] = kafkaDStream.map(record =&gt; record.value())     //6.计算 WordCount     valueDStream.flatMap(_.split(&quot; &quot;))     .map((_, 1))     .reduceByKey(_ + _)     .print()     //7.开启任务     ssc.start()     ssc.awaitTermination()     &#125;    &#125;  </code></pre><h5 id="查看-Kafka-消费进度"><a href="#查看-Kafka-消费进度" class="headerlink" title="查看 Kafka 消费进度"></a>查看 Kafka 消费进度</h5><pre><code>bin/kafka-consumer-groups.sh --describe --bootstrap-server linux1:9092 --group atguigu</code></pre><h2 id="DStream转换"><a href="#DStream转换" class="headerlink" title="DStream转换"></a>DStream转换</h2><p>DStream 上的操作与 RDD 的类似，分为 Transformations（转换）和 Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种 Window 相关的原语。  </p><h3 id="无状态转化操作"><a href="#无状态转化操作" class="headerlink" title="无状态转化操作"></a>无状态转化操作</h3><p>无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每<br>一个 RDD。<br>注意，针对键值对的 DStream 转化操作(比如reduceByKey())要添加 import StreamingContext._才能在 Scala 中使用。     </p><p><img src="/2023/08/11/Spark-Streaming/6.png" alt="无状态转化操作">   </p><p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部<br>是由许多 RDD（批次）组成，且无状态转化操作是分别应用到每个 RDD 上的。  </p><p>例如：reduceByKey()会归约每个时间区间中的数据，但不会归约不同区间之间的数据。  </p><h4 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h4><p>Transform 允许 DStream 上执行任意的 RDD-to-RDD 函数。即使这些函数并没有在 DStream<br>的 API 中暴露出来，通过该函数可以方便的扩展 Spark API。该函数每一批次调度一次。其实也<br>就是对 DStream 中的 RDD 应用转换。  </p><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>两个流之间的 join 需要两个流的批次大小一致，这样才能做到同时触发计算。计算过程就是对当前批次的两个流中各自的 RDD 进行 join，与两个 RDD 的 join 效果相同。  </p><pre><code>import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;object JoinTest &#123;     def main(args: Array[String]): Unit = &#123;     //1.创建 SparkConf     val sparkConf: SparkConf = new      SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;JoinTest&quot;)     //2.创建 StreamingContext     val ssc = new StreamingContext(sparkConf, Seconds(5))     //3.从端口获取数据创建流     val lineDStream1: ReceiverInputDStream[String] =      ssc.socketTextStream(&quot;linux1&quot;, 9999)     val lineDStream2: ReceiverInputDStream[String] =      ssc.socketTextStream(&quot;linux2&quot;, 8888)     //4.将两个流转换为 KV 类型     val wordToOneDStream: DStream[(String, Int)] = lineDStream1.flatMap(_.split(&quot; &quot;)).map((_, 1))     val wordToADStream: DStream[(String, String)] = lineDStream2.flatMap(_.split(&quot; &quot;)).map((_, &quot;a&quot;))     //5.流的 JOIN     val joinDStream: DStream[(String, (Int, String))] = wordToOneDStream.join(wordToADStream)     //6.打印     joinDStream.print()     //7.启动任务     ssc.start()     ssc.awaitTermination()     &#125;    &#125;</code></pre><h3 id="有状态转化操作"><a href="#有状态转化操作" class="headerlink" title="有状态转化操作"></a>有状态转化操作</h3><h4 id="UpdateStateByKey"><a href="#UpdateStateByKey" class="headerlink" title="UpdateStateByKey"></a>UpdateStateByKey</h4><p>UpdateStateByKey 原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加 wordcount)。  </p><p>updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。      </p><p>updateStateByKey 操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功<br>能，需要做下面两步：<br>    1.定义状态，状态可以是一个任意的数据类型。<br>    2.定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更<br>新。  </p><h4 id="WindowOperations"><a href="#WindowOperations" class="headerlink" title="WindowOperations"></a>WindowOperations</h4><p>Window Operations 可以设置窗口的大小和滑动窗口的间隔来动态的获取当前 Steaming 的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。  </p><p>窗口时长：计算内容的时间范围；  </p><p>滑动步长：隔多久触发一次计算。  </p><p>注意：这两者都必须为采集周期大小的整数倍。  </p><pre><code>object WorldCount &#123;     def main(args: Array[String]) &#123;         val conf = new          SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;NetworkWordCount&quot;)         val ssc = new StreamingContext(conf, Seconds(3))         ssc.checkpoint(&quot;./ck&quot;)         // Create a DStream that will connect to hostname:port, like localhost:9999         val lines = ssc.socketTextStream(&quot;linux1&quot;, 9999)         // Split each line into words         val words = lines.flatMap(_.split(&quot; &quot;))         // Count each word in each batch         val pairs = words.map(word =&gt; (word, 1))         val wordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b),Seconds(12), Seconds(6))         // Print the first ten elements of each RDD generated in this DStream to the console         wordCounts.print()         ssc.start() // Start the computation         ssc.awaitTermination() // Wait for the computation to terminate         &#125;        &#125;      </code></pre><p>关于 Window 的操作还有如下方法：  </p><pre><code>window(windowLength, slideInterval): 基于对源 DStream 窗化的批次进行计算返回一个新的 Dstream；  countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；  reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；  reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V)对的 DStream 上调用此函数，会返回一个新(K,V)对的 DStream，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值。  reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的 reduce 值都是通过用前一个窗的 reduce 值来递增计算。  </code></pre><p><img src="/2023/08/11/Spark-Streaming/7.png" alt="可逆的reduce函数">  </p><p>countByWindow()和 countByValueAndWindow()作为对数据进行计数操作的简写。countByWindow()返回一个表示每个窗口中元素个数的 DStream，而countByValueAndWindow()返回的 DStream 则包含窗口中每个值的个数。   </p><h2 id="DStream-输出"><a href="#DStream-输出" class="headerlink" title="DStream 输出"></a>DStream 输出</h2><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。<br>与 RDD 中的惰性求值类似，如果一个 DStream 及其派生出的 DStream 都没有被执行输出操作，那么这些 DStream 就都不会被求值。如果 StreamingContext 中没有设定输出操作，整个 context 就都不会启动。    </p><p>输出操作清单：</p><pre><code>print()：在运行流程序的驱动结点上打印 DStream 中每一批次数据的最开始 10 个元素 saveAsTextFiles(prefix, [suffix])：以 text 文件形式存储这个 DStream 的内容每一批次的存储文件名基于参数中的 prefix 和 suffix。”prefix-Time_IN_MS[.suffix]”  saveAsObjectFiles(prefix, [suffix])：以 Java 对象序列化的方式将 Stream 中的数据保存为SequenceFiles . 每一批次的存储文件名基于参数中的为&quot;prefix-TIME_IN_MS[.suffix]&quot;. Python中目前不可用  saveAsHadoopFiles(prefix, [suffix])：将 Stream 中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为&quot;prefix-TIME_IN_MS[.suffix]&quot;。Python API 中目前不可用  foreachRDD(func)：这是最通用的输出操作，即将函数 func 用于产生于 stream 的每一个RDD。其中参数传入的函数 func 应该实现将每一个 RDD 中数据推送到外部系统，如将RDD 存入文件或者通过网络将其写入数据库。  </code></pre><p>通用的输出操作 foreachRDD()，它用来对 DStream 中的 RDD 运行任意计算  </p><p>在 foreachRDD()中，可以重用我们在 Spark 中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如 MySQL 的外部数据库中。  </p><p>注意：<br>    1) 连接不能写在 driver 层面（序列化）<br>    2) 如果写在 foreach 则每个 RDD 中的每一条数据都创建，得不偿失；<br>    3) 增加 foreachPartition，在分区创建（获取）   </p><h2 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h2><p>流式任务需要 7*24 小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。使用外部文件系统来控制内部程序关闭。  </p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E8%8B%B1%E9%9B%84%E8%80%85%EF%BC%8C%E8%83%B8%E6%80%80%E5%A4%A7%E5%BF%97%EF%BC%8C%E8%85%B9%E6%9C%89%E8%89%AF%E8%B0%8B%EF%BC%8C%E6%9C%89%E5%8C%85%E8%97%8F%E5%AE%87%E5%AE%99%E4%B9%8B%E6%9C%BA%EF%BC%8C%E5%90%9E%E5%90%90%E5%A4%A9%E5%9C%B0%E4%B9%8B%E5%BF%97%E8%80%85%E4%B9%9F/">英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也</category>
      
      
      <comments>http://example.com/2023/08/11/Spark-Streaming/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark-SQL</title>
      <link>http://example.com/2023/08/11/Spark-SQL/</link>
      <guid>http://example.com/2023/08/11/Spark-SQL/</guid>
      <pubDate>Fri, 11 Aug 2023 12:19:25 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功&quot;&gt;&lt;a href=&quot;#无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功&quot; class=&quot;headerlink&quot; title=&quot;无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功&quot;&gt;&lt;/a&gt;无冥冥之志者，无昭昭之</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功"><a href="#无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功" class="headerlink" title="无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功"></a>无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功</h1><h1 id="SparkSQL概述"><a href="#SparkSQL概述" class="headerlink" title="SparkSQL概述"></a>SparkSQL概述</h1><h2 id="SparkSQL是什么？"><a href="#SparkSQL是什么？" class="headerlink" title="SparkSQL是什么？"></a>SparkSQL是什么？</h2><p>Spark SQL是Spark用于结构化数据(structured data)处理的Spark模块。  </p><pre><code>➢ 数据兼容方面 SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据；➢ 性能优化方面 除了采取 In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等；➢ 组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展。  </code></pre><p>应用Spark的两个支线：SparkSQL 和 Hive on Spark  </p><p>SparkSQL 可以简化 RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。  </p><p>Spark SQL 为了简化 RDD 的开发，提高开发效率，提供了2个编程抽象，类似 Spark Core 中的RDD。</p><pre><code>➢ DataFrame➢ DataSet</code></pre><h2 id="SparkSQL-特点"><a href="#SparkSQL-特点" class="headerlink" title="SparkSQL 特点"></a>SparkSQL 特点</h2><h3 id="易整合"><a href="#易整合" class="headerlink" title="易整合"></a>易整合</h3><p>无缝的整合了 SQL 查询和 Spark 编程  </p><h3 id="统一的数据访问"><a href="#统一的数据访问" class="headerlink" title="统一的数据访问"></a>统一的数据访问</h3><p>使用相同的方式连接不同的数据源   </p><h3 id="兼容-Hive"><a href="#兼容-Hive" class="headerlink" title="兼容 Hive"></a>兼容 Hive</h3><p>在已有的仓库上直接运行 SQL 或者 HiveQL  </p><h3 id="标准数据连接"><a href="#标准数据连接" class="headerlink" title="标准数据连接"></a>标准数据连接</h3><p>通过 JDBC 或者 ODBC 来连接  </p><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。  </p><p>DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL得以洞察更多的结构信息，达到大幅提升运行时效率的目标。  </p><p>反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在 stage 层面进行简单、通用的流水线优化。  </p><p><img src="/2023/08/11/Spark-SQL/1.png" alt="DataFrame和RDD的区别">    </p><p>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待。     </p><p>DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计划通过 Spark catalyst optimiser 进行优化。  </p><p><img src="/2023/08/11/Spark-SQL/2.png" alt="逻辑查询计划优化">     </p><p>逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操<br>作的过程。     </p><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>DataSet 是分布式数据集合。  </p><p>它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）。  </p><pre><code>➢ DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象  ➢ 用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；  ➢ DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。  ➢ DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。     </code></pre><h2 id="SparkSQL-核心编程"><a href="#SparkSQL-核心编程" class="headerlink" title="SparkSQL 核心编程"></a>SparkSQL 核心编程</h2><p>SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和HiveContext的组合。  </p><h3 id="DataFrame-1"><a href="#DataFrame-1" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>Spark SQL 的 DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL表达式。    </p><h4 id="创建-DataFrame"><a href="#创建-DataFrame" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h4><p>创建 DataFrame有三种方式：<br>    1.通过 Spark 的数据源进行创建；<br>    2.从一个存在的 RDD 进行转换；<br>    3.从 Hive Table 进行查询返回。   </p><h5 id="从-Spark-数据源进行创建"><a href="#从-Spark-数据源进行创建" class="headerlink" title="从 Spark 数据源进行创建"></a>从 Spark 数据源进行创建</h5><pre><code>scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]</code></pre><h5 id="从一个存在的-RDD-进行转换"><a href="#从一个存在的-RDD-进行转换" class="headerlink" title="从一个存在的 RDD 进行转换"></a>从一个存在的 RDD 进行转换</h5><h5 id="从-Hive-Table-进行查询返回"><a href="#从-Hive-Table-进行查询返回" class="headerlink" title="从 Hive Table 进行查询返回"></a>从 Hive Table 进行查询返回</h5><h3 id="SQL-语法"><a href="#SQL-语法" class="headerlink" title="SQL 语法"></a>SQL 语法</h3><p>SQL 语法风格是指我们查询数据的时候使用 SQL 语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助。    </p><ol><li><p>读取 JSON 文件创建 DataFrame  </p><p> scala&gt; val df &#x3D; spark.read.json(“data&#x2F;user.json”)<br> df: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， username: string]</p></li><li><p>对 DataFrame 创建一个临时表 </p><p> scala&gt; df.createOrReplaceTempView(“people”)</p></li><li><p>通过 SQL 语句实现查询全表  </p><p> scala&gt; val sqlDF &#x3D; spark.sql(“SELECT * FROM people”)<br> sqlDF: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， name: string]</p></li><li><p>结果展示</p><p> scala&gt; sqlDF.show<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|</p></li></ol><p>注意：普通临时表是 Session 范围内的，如果想应用范围内有效，可以使用全局临时表。使<br>用全局临时表时需要全路径访问，如：global_temp.people  </p><ol start="5"><li><p>对于 DataFrame 创建一个全局表  </p><p> scala&gt; df.createGlobalTempView(“people”)</p></li><li><p>通过 SQL 语句实现查询全表  </p><p> scala&gt; spark.sql(“SELECT * FROM global_temp.people”).show()<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|<br> +—+——–+<br> scala&gt; spark.newSession().sql(“SELECT * FROM global_temp.people”).show()<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|<br> +—+——–+</p></li></ol><h3 id="DSL-语法"><a href="#DSL-语法" class="headerlink" title="DSL 语法"></a>DSL 语法</h3><p>DataFrame 提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。  </p><p>可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了。    </p><ol><li><p>创建一个 DataFrame  </p><p> scala&gt; val df &#x3D; spark.read.json(“data&#x2F;user.json”)<br> df: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， name: string]</p></li><li><p>查看 DataFrame 的 Schema 信息  </p><p> scala&gt; df.printSchema<br> root<br> |– age: Long (nullable &#x3D; true)<br> |– username: string (nullable &#x3D; true)  </p></li><li><p>只查看”username”列数据  </p><p> scala&gt; df.select(“username”).show()<br> +——–+<br> |username|<br> +——–+<br> |zhangsan|<br> | lisi|<br> | wangwu|<br> +——–+  </p></li><li><p>查看”username”列数据以及”age+1”数据  </p><p> 注意:涉及到运算的时候, 每列都必须使用$, 或者采用引号表达式：单引号+字段名<br> scala&gt; df.select($”username”,$”age” + 1).show<br> scala&gt; df.select(‘username, ‘age + 1).show()  </p><p> scala&gt; df.select(‘username, ‘age + 1 as “newage”).show()<br> +——–+———+<br> |username|(age + 1)|<br> +——–+———+<br> |zhangsan| 21|<br> | lisi| 31|<br> | wangwu| 41|<br> +——–+———+  </p></li><li><p>查看”age”大于”30”的数据  </p><p> scala&gt; df.filter($”age”&gt;30).show<br> +—+———+<br> |age| username|<br> +—+———+<br> | 40| wangwu|<br> +—+———+  </p></li><li><p>按照”age”分组，查看数据条数  </p><p> scala&gt; df.groupBy(“age”).count.show<br> +—+—–+<br> |age|count|<br> +—+—–+<br> | 20| 1|<br> | 30| 1|<br> | 40| 1|<br> +—+—–+</p></li></ol><h3 id="RDD-转换为-DataFrame"><a href="#RDD-转换为-DataFrame" class="headerlink" title="RDD 转换为 DataFrame"></a>RDD 转换为 DataFrame</h3><p>在 IDEA 中开发程序时，如果需要 RDD 与 DF 或者 DS 之间互相操作，那么需要引入import spark.implicits._   </p><p>这里的 spark 不是 Scala 中的包名，而是创建的 sparkSession 对象的变量名称，所以必<br>须先创建 SparkSession 对象再导入。这里的 spark 对象不能使用 var 声明，因为 <strong>Scala 只支持val 修饰的对象的引入</strong>。    </p><h4 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h4><pre><code>scala&gt; val idRDD = sc.textFile(&quot;data/id.txt&quot;)scala&gt; idRDD.toDF(&quot;id&quot;).show    +---+    | id|    +---+    | 1|    | 2|    | 3|    | 4|     +---+  </code></pre><h4 id="通过样例类-RDD-DataFrame"><a href="#通过样例类-RDD-DataFrame" class="headerlink" title="通过样例类 RDD -&gt; DataFrame"></a>通过样例类 RDD -&gt; DataFrame</h4><p>实际开发中，一般通过样例类将 RDD 转换为 DataFrame  </p><pre><code>scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,40))).map(t=&gt;User(t._1, t._2)).toDF.show    +--------+---+    | name|age|    +--------+---+    |zhangsan| 30|    | lisi| 40|    +--------+---+    </code></pre><h3 id="DataFrame-转换为-RDD"><a href="#DataFrame-转换为-RDD" class="headerlink" title="DataFrame 转换为 RDD"></a>DataFrame 转换为 RDD</h3><p>DataFrame 其实就是对 RDD 的封装，所以可以直接获取内部的 RDD  </p><h4 id="df-rdd"><a href="#df-rdd" class="headerlink" title="df.rdd"></a>df.rdd</h4><pre><code>scala&gt; val rdd = df.rddrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] at rdd at &lt;console&gt;:25  </code></pre><p>注意：此时得到的 RDD 存储类型为 Row   </p><h3 id="DataSet-1"><a href="#DataSet-1" class="headerlink" title="DataSet"></a>DataSet</h3><p>DataSet 是具有强类型的数据集合，需要提供对应的类型信息。  </p><h4 id="创建-DataSet"><a href="#创建-DataSet" class="headerlink" title="创建 DataSet"></a>创建 DataSet</h4><h5 id="使用样例类序列创建-DataSet"><a href="#使用样例类序列创建-DataSet" class="headerlink" title="使用样例类序列创建 DataSet"></a>使用样例类序列创建 DataSet</h5><pre><code>scala&gt; case class Person(name: String, age: Long)defined class Personscala&gt; val caseClassDS = Seq(Person(&quot;zhangsan&quot;,2)).toDS()caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]scala&gt; caseClassDS.show    +---------+---+    | name|age|    +---------+---+    | zhangsan| 2|    +---------+---+    </code></pre><h5 id="使用基本类型的序列创建-DataSet"><a href="#使用基本类型的序列创建-DataSet" class="headerlink" title="使用基本类型的序列创建 DataSet"></a>使用基本类型的序列创建 DataSet</h5><pre><code>scala&gt; val ds = Seq(1,2,3,4,5).toDSds: org.apache.spark.sql.Dataset[Int] = [value: int]      scala&gt; ds.show    +-----+    |value|    +-----+    | 1|    | 2|    | 3|    | 4|    | 5|    +-----+    </code></pre><p>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet  </p><h3 id="RDD-转换为-DataSet"><a href="#RDD-转换为-DataSet" class="headerlink" title="RDD 转换为 DataSet"></a>RDD 转换为 DataSet</h3><p>SparkSQL 能够自动将包含有 case 类的 RDD 转换成 DataSet，case 类定义了 table 的结构，case 类属性通过反射变成了表的列名。Case 类可以包含诸如 Seq 或者 Array 等复杂的结构。  </p><h4 id="toDS"><a href="#toDS" class="headerlink" title="toDS"></a>toDS</h4><pre><code>scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).map(t=&gt;User(t._1, t._2)).toDSres11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</code></pre><h3 id="DataSet-转换为-RDD"><a href="#DataSet-转换为-RDD" class="headerlink" title="DataSet 转换为 RDD"></a>DataSet 转换为 RDD</h3><p>DataSet 其实也是对 RDD 的封装，所以可以直接获取内部的 RDD    </p><h4 id="ds-rdd"><a href="#ds-rdd" class="headerlink" title="ds.rdd"></a>ds.rdd</h4><pre><code>scala&gt; val rdd = res11.rddrdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at &lt;console&gt;:25</code></pre><h3 id="DataFrame-和-DataSet-转换"><a href="#DataFrame-和-DataSet-转换" class="headerlink" title="DataFrame 和 DataSet 转换"></a>DataFrame 和 DataSet 转换</h3><p>DataFrame 其实是 DataSet 的特例，所以它们之间是可以互相转换的。  </p><h4 id="DataFrame-转换为-DataSet"><a href="#DataFrame-转换为-DataSet" class="headerlink" title="DataFrame 转换为 DataSet"></a>DataFrame 转换为 DataSet</h4><h5 id="df-as-样例类"><a href="#df-as-样例类" class="headerlink" title="df.as[样例类]"></a>df.as[样例类]</h5><pre><code>scala&gt; case class User(name:String, age:Int)defined class User  scala&gt; val df = sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).toDF(&quot;name&quot;,&quot;age&quot;)df: org.apache.spark.sql.DataFrame = [name: string, age: int]scala&gt; val ds = df.as[User]ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]  </code></pre><h4 id="DataSet-转换为-DataFrame"><a href="#DataSet-转换为-DataFrame" class="headerlink" title="DataSet 转换为 DataFrame"></a>DataSet 转换为 DataFrame</h4><h5 id="ds-toDF"><a href="#ds-toDF" class="headerlink" title="ds.toDF"></a>ds.toDF</h5><pre><code>scala&gt; val ds = df.as[User]ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]scala&gt; val df = ds.toDFdf: org.apache.spark.sql.DataFrame = [name: string, age: int]  </code></pre><h3 id="RDD、DataFrame、DataSet-三者的关系"><a href="#RDD、DataFrame、DataSet-三者的关系" class="headerlink" title="RDD、DataFrame、DataSet 三者的关系"></a>RDD、DataFrame、DataSet 三者的关系</h3><p>同样的数据都给到这三个数据结构,计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。    </p><h4 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h4><p>都是 spark 平台下的分布式弹性数据集。  </p><p>都有惰性机制。  </p><p>三者有许多共同的函数，如 filter，排序等。  </p><p>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）  </p><p>三者都会根据 Spark 的内存情况自动缓存运算。  </p><p>三者都有 partition 的概念  </p><p>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型   </p><h4 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h4><p>RDD 不支持 sparksql 操作。  </p><p>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直<br>接访问，只有通过解析才能获取各个字段的值。  </p><p>DataFrame 与 DataSet 一般不与 spark mllib 同时使用。  </p><p>DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能<br>注册临时表&#x2F;视窗，进行 sql 语句操作。  </p><p>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然。    </p><p>DataFrame 其实就是 DataSet 的一个特例 type DataFrame &#x3D; Dataset[Row]  </p><p>DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了case class 之后可以很自由的获得每一行的信息。      </p><p><img src="/2023/08/11/Spark-SQL/3.png" alt="三者的相互转换">  </p><h2 id="IDEA开发SparkSQL"><a href="#IDEA开发SparkSQL" class="headerlink" title="IDEA开发SparkSQL"></a>IDEA开发SparkSQL</h2><p>实际开发中，都是使用 IDEA 进行开发的。   </p><h3 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h3><pre><code>&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>object SparkSQL01_Demo &#123; def main(args: Array[String]): Unit = &#123;     //创建上下文环境配置对象     val conf: SparkConf = new         SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL01_Demo&quot;)     //创建 SparkSession 对象         val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()         //RDD=&gt;DataFrame=&gt;DataSet 转换需要引入隐式转换规则，否则无法转换         //spark 不是包名，是上下文环境对象名         import spark.implicits._         //读取 json 文件 创建 DataFrame &#123;&quot;username&quot;: &quot;lisi&quot;,&quot;age&quot;: 18&#125;         val df: DataFrame = spark.read.json(&quot;input/test.json&quot;)         //df.show()         //SQL 风格语法         df.createOrReplaceTempView(&quot;user&quot;)         //spark.sql(&quot;select avg(age) from user&quot;).show         //DSL 风格语法         //df.select(&quot;username&quot;,&quot;age&quot;).show()         //*****RDD=&gt;DataFrame=&gt;DataSet*****         //RDD         val rdd1: RDD[(Int, String, Int)] =          spark.sparkContext.makeRDD(List((1,&quot;zhangsan&quot;,30),(2,&quot;lisi&quot;,28),(3,&quot;wangwu&quot;,20)))         //DataFrame         val df1: DataFrame = rdd1.toDF(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;)         //df1.show()         //DateSet         val ds1: Dataset[User] = df1.as[User]         //ds1.show()         //*****DataSet=&gt;DataFrame=&gt;RDD*****         //DataFrame         val df2: DataFrame = ds1.toDF()         //RDD 返回的 RDD 类型为 Row，里面提供的 getXXX 方法可以获取字段值，类似 jdbc 处理结果集，        但是索引从 0 开始         val rdd2: RDD[Row] = df2.rdd         //rdd2.foreach(a=&gt;println(a.getString(1)))         //*****RDD=&gt;DataSet*****         rdd1.map&#123;          case (id,name,age)=&gt;User(id,name,age)         &#125;.toDS()         //*****DataSet=&gt;=&gt;RDD*****         ds1.rdd         //释放资源         spark.stop()     &#125;    &#125;case class User(id:Int,name:String,age:Int)  </code></pre><h3 id="toDF和toDS的用法区别："><a href="#toDF和toDS的用法区别：" class="headerlink" title="toDF和toDS的用法区别："></a>toDF和toDS的用法区别：</h3><p>使用toDF时：  </p><pre><code>rdd1.toDF(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;)  指定字段名即可，字段类型会自动解析rdd中的数据进行获取。 </code></pre><p>使用toDS时：  </p><pre><code>case class User(id:Int,name:String,age:Int)   rdd1.toDS  </code></pre><h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><h4 id="创建-DataFrame-1"><a href="#创建-DataFrame-1" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h4><pre><code>scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]</code></pre><h4 id="注册-UDF"><a href="#注册-UDF" class="headerlink" title="注册 UDF"></a>注册 UDF</h4><pre><code>scala&gt; spark.udf.register(&quot;addName&quot;,(x:String)=&gt; &quot;Name:&quot;+x)res9: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))</code></pre><h4 id="创建临时表"><a href="#创建临时表" class="headerlink" title="创建临时表"></a>创建临时表</h4><pre><code>scala&gt; df.createOrReplaceTempView(&quot;people&quot;)</code></pre><h4 id="应用-UDF"><a href="#应用-UDF" class="headerlink" title="应用 UDF"></a>应用 UDF</h4><pre><code>scala&gt; spark.sql(&quot;Select addName(name),age from people&quot;).show()   </code></pre><h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><p>用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从 Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以统一采用强类型聚合函数Aggregator。  </p><h2 id="数据的加载和保存"><a href="#数据的加载和保存" class="headerlink" title="数据的加载和保存"></a>数据的加载和保存</h2><p>SparkSQL 默认读取和保存的文件格式为 parquet。  </p><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>如果读取不同格式的数据，可以对不同的数据格式进行设定  </p><pre><code>scala&gt; spark.read.format(&quot;…&quot;)[.option(&quot;…&quot;)].load(&quot;…&quot;)  ➢ format(&quot;…&quot;)：指定加载的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。➢ load(&quot;…&quot;)：在&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入加载数据的路径。➢ option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable     </code></pre><p>也可以直接在文件上进行查询: 文件格式.<code>文件路径</code></p><pre><code>scala&gt;spark.sql(&quot;select * from json.`/opt/module/data/user.json`&quot;).show  </code></pre><h3 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h3><p>df.write.save 是保存数据的通用方法  </p><pre><code>scala&gt;df.write.format(&quot;…&quot;)[.option(&quot;…&quot;)].save(&quot;…&quot;)  ➢ format(&quot;…&quot;)：指定保存的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。➢ save (&quot;…&quot;)：在&quot;csv&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入保存数据的路径。➢ option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable  </code></pre><p>保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。  </p><pre><code>df.write.mode(&quot;append&quot;).json(&quot;/opt/module/data/output&quot;)   </code></pre><p><img src="/2023/08/11/Spark-SQL/4.png" alt="SaveMode枚举值">    </p><h3 id="修改默认数据源格式"><a href="#修改默认数据源格式" class="headerlink" title="修改默认数据源格式"></a>修改默认数据源格式</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式<br>存储格式。</p><p>修改配置项 spark.sql.sources.default，可修改默认数据源格式。   </p><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以<br>通过 SparkSession.read.json()去加载 JSON 文件。  </p><p>注意：Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串 </p><pre><code>&#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;， &quot;age&quot;:30&#125;[&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;,&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;]  </code></pre><h3 id="Spark读取本地Json文件的案例"><a href="#Spark读取本地Json文件的案例" class="headerlink" title="Spark读取本地Json文件的案例"></a>Spark读取本地Json文件的案例</h3><h4 id="导入隐式转换"><a href="#导入隐式转换" class="headerlink" title="导入隐式转换"></a>导入隐式转换</h4><pre><code>import spark.implicits._</code></pre><h4 id="加载-JSON-文件"><a href="#加载-JSON-文件" class="headerlink" title="加载 JSON 文件"></a>加载 JSON 文件</h4><pre><code>val path = &quot;/opt/module/spark-local/people.json&quot;val peopleDF = spark.read.json(path)  </code></pre><h4 id="创建临时表-1"><a href="#创建临时表-1" class="headerlink" title="创建临时表"></a>创建临时表</h4><pre><code>peopleDF.createOrReplaceTempView(&quot;people&quot;)</code></pre><h4 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h4><pre><code>val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)  teenagerNamesDF.show()    +------+    | name|    +------+    |Justin|    +------+  </code></pre><h3 id="Spark读取本地CSV文件的案例"><a href="#Spark读取本地CSV文件的案例" class="headerlink" title="Spark读取本地CSV文件的案例"></a>Spark读取本地CSV文件的案例</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为<br>数据列。  </p><pre><code>spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;;&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).load(&quot;data/user.csv&quot;)  </code></pre><h3 id="Spark通过JDBC连接Mysql的案例"><a href="#Spark通过JDBC连接Mysql的案例" class="headerlink" title="Spark通过JDBC连接Mysql的案例"></a>Spark通过JDBC连接Mysql的案例</h3><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对<br>DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用 spark-shell 操作，可在启动 shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下。  </p><pre><code>bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar  </code></pre><p>在 Idea 中通过 JDBC 对 Mysql 进行操作的案例代码如下  </p><h4 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h4><pre><code>&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.27&lt;/version&gt;&lt;/dependency&gt;  </code></pre><h4 id="读取数据-（选用方式一）"><a href="#读取数据-（选用方式一）" class="headerlink" title="读取数据 （选用方式一）"></a>读取数据 （选用方式一）</h4><pre><code>val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)//创建 SparkSession 对象val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()import spark.implicits._      //方式 1：通用的 load 方法读取spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;).option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123123&quot;).option(&quot;dbtable&quot;, &quot;user&quot;).load().show  //方式 2:通用的 load 方法读取 参数另一种形式spark.read.format(&quot;jdbc&quot;)    .options(Map(&quot;url&quot;-&gt;&quot;jdbc:mysql://linux1:3306/spark-sql?user=root&amp;password=123123&quot;,    &quot;dbtable&quot;-&gt;&quot;user&quot;,&quot;driver&quot;-&gt;&quot;com.mysql.jdbc.Driver&quot;)).load().show//方式 3:使用 jdbc 方法读取val props: Properties = new Properties()props.setProperty(&quot;user&quot;, &quot;root&quot;)props.setProperty(&quot;password&quot;, &quot;123123&quot;)val df: DataFrame = spark.read.jdbc(&quot;jdbc:mysql://linux1:3306/spark-sql&quot;, &quot;user&quot;, props)df.show  //释放资源spark.stop()    </code></pre><h4 id="写入数据-选用方式一"><a href="#写入数据-选用方式一" class="headerlink" title="写入数据  (选用方式一)"></a>写入数据  (选用方式一)</h4><pre><code>case class User2(name: String, age: Long)。。。val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)//创建 SparkSession 对象val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()  import spark.implicits._    val rdd: RDD[User2] = spark.sparkContext.makeRDD(List(User2(&quot;lisi&quot;, 20), User2(&quot;zs&quot;, 30)))val ds: Dataset[User2] = rdd.toDS    //方式 1：通用的方式 format 指定写出类型ds.write.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123123&quot;).option(&quot;dbtable&quot;, &quot;user&quot;).mode(SaveMode.Append).save()//方式 2：通过 jdbc 方法val props: Properties = new Properties()props.setProperty(&quot;user&quot;, &quot;root&quot;)props.setProperty(&quot;password&quot;, &quot;123123&quot;)ds.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql://linux1:3306/spark-sql&quot;, &quot;user&quot;, props)    //释放资源  spark.stop() </code></pre><h4 id="使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作"><a href="#使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作" class="headerlink" title="使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作"></a>使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作</h4><pre><code>val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)//创建 SparkSession 对象val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()import spark.implicits._      //方式 1：通用的 load 方法读取res1 = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;).option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123123&quot;).option(&quot;dbtable&quot;, &quot;user&quot;).load()     res2 = spark.sql(&quot;select * from user1 where age &gt; 10&quot;)res2.write.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123123&quot;).option(&quot;dbtable&quot;, &quot;user&quot;).mode(SaveMode.Append).save() //释放资源  spark.stop()   </code></pre><h3 id="Spark操作Hive"><a href="#Spark操作Hive" class="headerlink" title="Spark操作Hive"></a>Spark操作Hive</h3><p>若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到<br>Spark 的配置文件目录中($SPARK_HOME&#x2F;conf)。   </p><p>需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 &#x2F;user&#x2F;hive&#x2F;warehouse 目录中(如果你的 classpath 中有配好的hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。   </p><p>spark-shell 默认是 Hive 支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。    </p><p>在实际使用中, 几乎没有任何人会使用内置的 Hive    </p><h4 id="Spark访问外部Hive的前置条件"><a href="#Spark访问外部Hive的前置条件" class="headerlink" title="Spark访问外部Hive的前置条件"></a>Spark访问外部Hive的前置条件</h4><p>如果想连接外部已经部署好的 Hive，需要通过以下几个步骤：  </p><pre><code>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下➢ 把 Mysql 的驱动 copy 到 jars/目录下➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下➢ 重启 spark-shell   </code></pre><h4 id="Spark-shell中访问Hive"><a href="#Spark-shell中访问Hive" class="headerlink" title="Spark-shell中访问Hive"></a>Spark-shell中访问Hive</h4><pre><code>scala&gt; spark.sql(&quot;show tables&quot;).show   </code></pre><h4 id="运行-Spark-SQL-CLI"><a href="#运行-Spark-SQL-CLI" class="headerlink" title="运行 Spark SQL CLI"></a>运行 Spark SQL CLI</h4><p>Spark SQL CLI 可以很方便的在本地运行 Hive 元数据服务以及从命令行执行查询任务。在Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一 Hive 窗口    </p><pre><code>bin/spark-sql    </code></pre><h4 id="运行-Spark-beeline"><a href="#运行-Spark-beeline" class="headerlink" title="运行 Spark beeline"></a>运行 Spark beeline</h4><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。Spark Thrift Server 的目的也只是取代HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。<br>如果想连接 Thrift Server，需要通过以下几个步骤：  </p><pre><code>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下➢ 把 Mysql 的驱动 copy 到 jars/目录下➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下➢ 启动 Thrift Server    sbin/start-thriftserver.sh      </code></pre><h5 id="使用-beeline-连接-Thrift-Server"><a href="#使用-beeline-连接-Thrift-Server" class="headerlink" title="使用 beeline 连接 Thrift Server"></a>使用 beeline 连接 Thrift Server</h5><pre><code>bin/beeline -u jdbc:hive2://linux1:10000 -n root  </code></pre><h4 id="Spark操作Hive的代码示例"><a href="#Spark操作Hive的代码示例" class="headerlink" title="Spark操作Hive的代码示例"></a>Spark操作Hive的代码示例</h4><h5 id="导入依赖-1"><a href="#导入依赖-1" class="headerlink" title="导入依赖"></a>导入依赖</h5><pre><code>&lt;dependency&gt;     &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;     &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;     &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;     &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;     &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;     &lt;version&gt;1.2.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;     &lt;groupId&gt;mysql&lt;/groupId&gt;     &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;     &lt;version&gt;5.1.27&lt;/version&gt;&lt;/dependency&gt;  </code></pre><h5 id="拷贝Hive-Site-xml"><a href="#拷贝Hive-Site-xml" class="headerlink" title="拷贝Hive-Site.xml"></a>拷贝Hive-Site.xml</h5><p>将 hive-site.xml 文件拷贝到项目的 resources 目录中</p><h5 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h5><pre><code>//创建 SparkSessionval spark: SparkSession = SparkSession.builder().enableHiveSupport().master(&quot;local[*]&quot;).appName(&quot;sql&quot;).getOrCreate()  </code></pre><p>在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址:<br>config(“spark.sql.warehouse.dir”, “hdfs:&#x2F;&#x2F;linux1:8020&#x2F;user&#x2F;hive&#x2F;warehouse”)</p><p>代码最前面增加如下代码解决权限不足的问题：  </p><p>System.setProperty(“HADOOP_USER_NAME”, “root”)</p><p>此处的 root 改为你们自己的 hadoop 用户名称      </p><h5 id="整理后代码实现"><a href="#整理后代码实现" class="headerlink" title="整理后代码实现"></a>整理后代码实现</h5><pre><code>//创建 SparkSessionSystem.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)val spark: SparkSession = SparkSession.builder().config(&quot;spark.sql.warehouse.dir&quot;, &quot;hdfs://linux1:8020/user/hive/warehouse&quot;).enableHiveSupport().master(&quot;local[*]&quot;).appName(&quot;show databases&quot;).getOrCreate() </code></pre>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E6%97%A0%E5%86%A5%E5%86%A5%E4%B9%8B%E5%BF%97%E8%80%85%EF%BC%8C%E6%97%A0%E6%98%AD%E6%98%AD%E4%B9%8B%E6%98%8E%EF%BC%9B%E6%97%A0%E6%83%9B%E6%83%9B%E4%B9%8B%E4%BA%8B%E8%80%85%EF%BC%8C%E6%97%A0%E8%B5%AB%E8%B5%AB%E4%B9%8B%E5%8A%9F/">无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功</category>
      
      
      <comments>http://example.com/2023/08/11/Spark-SQL/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Java数据结构和算法</title>
      <link>http://example.com/2023/08/07/Java_datastrcut/</link>
      <guid>http://example.com/2023/08/07/Java_datastrcut/</guid>
      <pubDate>Mon, 07 Aug 2023 03:10:06 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;君子藏器于身，待时而动&quot;&gt;&lt;a href=&quot;#君子藏器于身，待时而动&quot; class=&quot;headerlink&quot; title=&quot;君子藏器于身，待时而动&quot;&gt;&lt;/a&gt;君子藏器于身，待时而动&lt;/h1&gt;&lt;h2 id=&quot;线性结构和非线性结构&quot;&gt;&lt;a href=&quot;#线性结构和非线</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="君子藏器于身，待时而动"><a href="#君子藏器于身，待时而动" class="headerlink" title="君子藏器于身，待时而动"></a>君子藏器于身，待时而动</h1><h2 id="线性结构和非线性结构"><a href="#线性结构和非线性结构" class="headerlink" title="线性结构和非线性结构"></a>线性结构和非线性结构</h2><h3 id="线性结构"><a href="#线性结构" class="headerlink" title="线性结构"></a>线性结构</h3><p>线性结构作为最常用的数据结构，其特点是数据元素之间存在一对一的线性关系  </p><p>线性结构有两种不同的存储结构，即顺序存储结构和链式存储结构。顺序存储的线性表称为顺序表，顺序表中的存储元素是连续的  </p><p>链式存储的线性表称为链表，链表中的存储元素不一定是连续的，元素节点中存放数据元素以及相邻元素的地址信息  </p><p>线性结构常见的有：数组、队列、链表和栈，后面我们会详细讲解   </p><p>非线性结构包括：二维数组，多维数组，广义表，树结构，图结构  </p><h1 id="稀疏数组"><a href="#稀疏数组" class="headerlink" title="稀疏数组"></a>稀疏数组</h1><p>当一个数组中大部分元素为０，或者为同一个值的数组时，可以使用稀疏数组来保存该数组。  </p><p>稀疏数组的处理方法是:  </p><p>记录数组一共有几行几列，有多少个不同的值  </p><p>把具有不同值的元素的行列及值记录在一个小规模的数组中，从而缩小程序的规模  </p><p><img src="/2023/08/07/Java_datastrcut/1.png" alt="稀疏数组">   </p><p><img src="/2023/08/07/Java_datastrcut/2.png" alt="稀疏数组转换思路">    </p><p>二维表转稀疏数组代码实现：    </p><pre><code>package com.zyy;public class SparseArray &#123;    public static void main(String[] args)&#123;        //创建一个原始的二维数组 11*11        // 0: 表示没有棋子，1表示黑子 2表示蓝子        int chessArr1[][] = new int[11][11];        chessArr1[1][2] = 1; //第二行第三列 有一颗黑子        chessArr1[2][3] = 2; //第三行第四列 有一颗蓝子        chessArr1[4][5] = 2; //第五行第六列 有一颗蓝子        //输出原始的二维数组        System.out.println(&quot;原始的二维数组~~&quot;);        //从二维数组中拿出每一行数据，返回为一维数组int[] row        for(int[] row:chessArr1) &#123;            //从拿到的每一行数据中拿到每一个值            for (int data:row)&#123;                System.out.printf(&quot;%d\t&quot;,data);            &#125;            System.out.println();        &#125;        // 将二维数组转稀疏数组的思想        //1.先遍历二维数组，得到非0数据的个数        int sum = 0;        for (int i = 0; i &lt; 11; i++)&#123;            for (int j = 0;j &lt; 11; j++)&#123;                if (chessArr1[i][j] != 0 )&#123;                    sum++;                &#125;            &#125;        &#125;        //2.创建对应的稀疏数组        //由统计出来的非0数个数+1，构成稀疏数组的行数        //稀疏数组的列数固定为3，记录行坐标，列坐标，值        int sparseArr[][] = new int[sum+1][3];        //给稀疏数组赋值        sparseArr[0][0] = 11;        sparseArr[0][1] = 11;        sparseArr[0][2] = sum;        //遍历二维数组，将非0的值存放到sparseArr中        int count = 0;  //count用于记录是第几个非0数据        for(int i = 0;i &lt; 11; i++)&#123;            for (int j = 0 ;j &lt; 11; j++)&#123;                if(chessArr1[i][j] != 0 )&#123;                    count++;                    //记录第count个非0数据的行i                    sparseArr[count][0] = i;                    //记录第count个非0数据的列j                    sparseArr[count][1] = j;                    //记录第count个非0数据的值                    sparseArr[count][2] = chessArr1[i][j];                &#125;            &#125;        &#125;        //输出稀疏数组的形式        System.out.println();        System.out.println(&quot;得到稀疏数组为~~~&quot;);        //二维数组结构为[[数组1],[数组2],[数组3]]        //所以sparseArr.length实际上是在统计外层一维数组的长度        for(int i = 0; i &lt; sparseArr.length;i++)&#123;            System.out.printf(&quot;%d\t%d\t%d\t\n&quot;,sparseArr[i][0],sparseArr[i][1],sparseArr[i][2]);        &#125;        System.out.println();        //将稀疏数组 -》 恢复成 原始的二维数组        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组，比如上面的chessArr2 = int[11][11]        //2.在读取稀疏数组后几行的数据，并赋值给原始的二维数组，即可        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组        int chessArr2[][] = new int[sparseArr[0][0]][sparseArr[0][1]];        //2.在读取稀疏数组后几行的数据（从第二行开始），并赋值给原始的二维数组即可        for (int i = 1; i &lt; sparseArr.length; i++)&#123;            chessArr2[sparseArr[i][0]][sparseArr[i][1]] = sparseArr[i][2];        &#125;        //输出恢复后的二维数组        System.out.println();        System.out.println(&quot;恢复后的二维数组&quot;);        for (int[] row:chessArr2)&#123;            for(int data:row)&#123;                System.out.printf(&quot;%d\t&quot;,data);            &#125;            //每行数据打印完之后，执行换行            System.out.println();        &#125;    &#125;&#125;原始的二维数组~~0000000000000100000000000200000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000得到稀疏数组为~~~11113121232452恢复后的二维数组0000000000000100000000000200000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000</code></pre><h1 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h1><p>队列是一个有序列表，可以用数组或者链表来实现。遵循先进先出原则（FIFO）</p><p><img src="/2023/08/07/Java_datastrcut/3.png" alt="数组模拟队列">  </p><h2 id="数组模拟队列"><a href="#数组模拟队列" class="headerlink" title="数组模拟队列"></a>数组模拟队列</h2><pre><code>package com.zyy;import java.util.Scanner;public class ArrayQueueDemo &#123;public static void main(String[] args) &#123;    // 测试一把    // 创建一个队列    ArrayQueue queue = new ArrayQueue(5);    char key ;//接收用户输入    Scanner scanner = new Scanner(System.in);    boolean loop = true;    //输出一个菜单    while(loop)&#123;        System.out.println(&quot;s(show):显示队列&quot;);        System.out.println(&quot;e(exit):退出程序&quot;);        System.out.println(&quot;a(add):添加数据到队列&quot;);        System.out.println(&quot;g(get):从队列取出数据&quot;);        System.out.println(&quot;h(head):查看队列头的数据&quot;);        key = scanner.next().charAt(0);//接收一个字符        switch(key)&#123;            case &#39;s&#39;:                queue.showQueue();                break;            case &#39;a&#39;:                System.out.println(&quot;输入一个数&quot;);                int value = scanner.nextInt();                queue.addQueue(value);                break;            case &#39;g&#39;://取出数据                try&#123;                    int res = queue.getQueue();                    System.out.printf(&quot;取出的数据是%d\n&quot;,res);                &#125;catch (Exception e)&#123;                    //TODO:handle exception                    System.out.println(e.getMessage());                &#125;                break;            case &#39;h&#39;://查看队列头的数据                try&#123;                    int res = queue.headQueue();                    System.out.printf(&quot;队列头的数据是%d\n&quot;,res);                &#125;catch(Exception e)&#123;                    //TODO:handle exception                    System.out.println(e.getMessage());                &#125;                break;            case &#39;e&#39;://退出                scanner.close();                loop = false;                break;            default:                break;        &#125;    &#125;    System.out.println(&quot;程序退出~~&quot;);&#125;    //使用数组模拟队列-编写一个ArrayQueue类static class ArrayQueue&#123;        private int maxSize;// 表示数组的最大容量        private int front;//队列头        private int rear;//队列尾        private int[] arr;//该数据用于存放数据，模拟队列        //创建队列的构造器        public ArrayQueue(int arrMaxSize)&#123;            maxSize = arrMaxSize;            arr = new int[maxSize];            front = -1;//指向队列头部，分析出front是指向队列头的前一个位置            rear = -1;//指向队列尾，指向队列尾的数据（即就是队列最后一个数据）        &#125;        //判断队列是否满        public boolean isFull()&#123;            return rear == maxSize - 1;        &#125;        //判断队列是否为空        public boolean isEmpty()&#123;            return rear == front;        &#125;        //添加数据到队列        public void addQueue(int n)&#123;            //判断队列是否满            if(isFull())&#123;                System.out.println(&quot;队列满，不能加入数据~&quot;);                return;            &#125;            rear++;//让rear后移            arr[rear] = n;        &#125;        //获取队列的数据，出队列        public int getQueue()&#123;            //判断队列是否为空            if(isEmpty())&#123;                //通过抛出异常                throw new RuntimeException(&quot;队列空，不能取数据&quot;);            &#125;            front++;//front后移            return arr[front];        &#125;        //显示队列的所有数据        public void showQueue()&#123;            //遍历            if(isEmpty())&#123;                System.out.println(&quot;队列空的，没有数据~~&quot;);                return;            &#125;            for(int i=0;i&lt;arr.length;i++)&#123;                System.out.printf(&quot;arr[%d]=%d\n&quot;,i,arr[i]);            &#125;        &#125;        //显示队列的头数据，注意不是取出数据        public int headQueue()&#123;            //判断            if(isEmpty())&#123;                throw new RuntimeException(&quot;队列空的，没有数据~~&quot;);            &#125;            return arr[front + 1];        &#125;    &#125;&#125;</code></pre><h3 id="使用数组模拟队列存在的问题："><a href="#使用数组模拟队列存在的问题：" class="headerlink" title="使用数组模拟队列存在的问题："></a>使用数组模拟队列存在的问题：</h3><p>数组只能使用一次，因为front和rear指针无法再回头指向已经走过的数组位置  </p><h3 id="优化方案："><a href="#优化方案：" class="headerlink" title="优化方案："></a>优化方案：</h3><p>通过取模运算，让front和rear指针能循环指向已经走过的数组位置，让数组复用  </p><p>分析说明:  </p><p>1):尾索引的下一个为头索引时表示队列满，即将队列容量空出一个作为约定，这个在做判断队列满的时候需要注意（rear+1）%maxSize &#x3D;&#x3D; front 满</p><p>2):rear &#x3D;&#x3D; front(空)</p><p>3):与数组模拟队列不同，数组模拟环形队列时，front指向队列的第一个元素，front的初始值为0 ，rear指向队列的最后一个元素的最后一个位置，因为希望空出一个空间作为约定，rear的初始值为0</p><p>4):队列中的有效数据个数计算方法:  </p><p>(rear+maxSize-front)%maxSize</p><p><img src="/2023/08/07/Java_datastrcut/4.png" alt="循环队列相关判断条件"></p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%90%9B%E5%AD%90%E8%97%8F%E5%99%A8%E4%BA%8E%E8%BA%AB%EF%BC%8C%E5%BE%85%E6%97%B6%E8%80%8C%E5%8A%A8/">君子藏器于身，待时而动</category>
      
      
      <comments>http://example.com/2023/08/07/Java_datastrcut/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hadoop学习笔记</title>
      <link>http://example.com/2023/08/06/hadoop/</link>
      <guid>http://example.com/2023/08/06/hadoop/</guid>
      <pubDate>Sun, 06 Aug 2023 02:45:20 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;博观而约取，厚积而薄发&quot;&gt;&lt;a href=&quot;#博观而约取，厚积而薄发&quot; class=&quot;headerlink&quot; title=&quot;博观而约取，厚积而薄发&quot;&gt;&lt;/a&gt;博观而约取，厚积而薄发&lt;/h1&gt;&lt;p&gt;相关学习文档&lt;br&gt;链接：&lt;a href=&quot;https://pan.</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="博观而约取，厚积而薄发"><a href="#博观而约取，厚积而薄发" class="headerlink" title="博观而约取，厚积而薄发"></a>博观而约取，厚积而薄发</h1><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA">https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA</a><br>提取码：mvcs   </p><h1 id="免密登录原理"><a href="#免密登录原理" class="headerlink" title="免密登录原理"></a>免密登录原理</h1><p><img src="/2023/08/06/hadoop/4.png" alt="免密登录原理">    </p><h1 id="HDFS架构概述"><a href="#HDFS架构概述" class="headerlink" title="HDFS架构概述"></a>HDFS架构概述</h1><p><img src="/2023/08/06/hadoop/1.png" alt="HDFS架构概述">  </p><p>HDFS适合一次写入，多次读出的场景，且不支持文件的修改  </p><p>HDFS的缺点：仅支持数据append，不支持文件的随机修改  </p><p><img src="/2023/08/06/hadoop/5.png" alt="HDFS组成架构">  </p><p><img src="/2023/08/06/hadoop/6.png" alt="HDFS组成架构">   </p><p>HDFS文件块大小：在Hadoop2.x版本中是128M  </p><p>寻址时间为传输时间的1%时，是最佳状态  </p><p>HDFS块的大小设置主要取决于磁盘传输速率  </p><h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><p><img src="/2023/08/06/hadoop/7.png" alt="HDFS写数据流程">    </p><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据    </p><h3 id="HDFS副本节点选择"><a href="#HDFS副本节点选择" class="headerlink" title="HDFS副本节点选择"></a>HDFS副本节点选择</h3><p><img src="/2023/08/06/hadoop/8.png" alt="HDFS副本节点选择">       </p><h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><p><img src="/2023/08/06/hadoop/9.png" alt="HDFS读数据流程">    </p><h1 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h1><p>NameNode中的元数据是存储在内存中，在内存上维护一个Edits文件，磁盘上维护一个FsImage文件，每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据  </p><p>需要定期进行FsImage和Edits的合并，由SecondaryNamenode完成，专门负责FsImage和Edits的合并  </p><h2 id="NameNode工作机制"><a href="#NameNode工作机制" class="headerlink" title="NameNode工作机制"></a>NameNode工作机制</h2><p><img src="/2023/08/06/hadoop/10.png" alt="NameNode工作机制">    </p><h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录    </p><p>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中  </p><p><img src="/2023/08/06/hadoop/11.png" alt="集群安全模式">    </p><p>（1）bin&#x2F;hdfs dfsadmin -safemode get（功能描述：查看安全模式状态）  </p><p>（2）bin&#x2F;hdfs dfsadmin -safemode enter  （功能描述：进入安全模式状态）  </p><p>（3）bin&#x2F;hdfs dfsadmin -safemode leave（功能描述：离开安全模式状态）  </p><p>（4）bin&#x2F;hdfs dfsadmin -safemode wait（功能描述：等待安全模式状态）  </p><h2 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h2><p><img src="/2023/08/06/hadoop/12.png" alt="DataNode工作机制">   </p><h3 id="DataNode如何保证数据完整性？"><a href="#DataNode如何保证数据完整性？" class="headerlink" title="DataNode如何保证数据完整性？"></a>DataNode如何保证数据完整性？</h3><p>1）当DataNode读取Block的时候，它会计算CheckSum。  </p><p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。  </p><p>3）Client读取其他DataNode上的Block。  </p><p>4）DataNode在其文件创建后周期验证CheckSum    </p><p><img src="/2023/08/06/hadoop/13.png" alt="DataNode数据完整性">    </p><p>HDFS中默认DataNode掉线的超时时长为10分钟+30秒    </p><h3 id="DataNode配置多目录"><a href="#DataNode配置多目录" class="headerlink" title="DataNode配置多目录"></a>DataNode配置多目录</h3><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本   </p><p>hdfs-site.xml  </p><pre><code>&lt;property&gt;    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;    &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt;</code></pre><h2 id="HDFS-HA故障转移机制"><a href="#HDFS-HA故障转移机制" class="headerlink" title="HDFS-HA故障转移机制"></a>HDFS-HA故障转移机制</h2><p><img src="/2023/08/06/hadoop/14.png" alt="HDFS-HA故障转移机制">   </p><h1 id="YARN架构概述"><a href="#YARN架构概述" class="headerlink" title="YARN架构概述"></a>YARN架构概述</h1><p><img src="/2023/08/06/hadoop/2.png" alt="YARN架构概述">    </p><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序  </p><p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成</p><h2 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h2><p><img src="/2023/08/06/hadoop/25.png" alt="Yarn工作机制">    </p><p>工作机制详解  </p><pre><code>（1）MR程序提交到客户端所在的节点。  （2）YarnRunner向ResourceManager申请一个Application。  （3）RM将该应用程序的资源路径返回给YarnRunner。（4）该程序将运行所需资源提交到HDFS上。（5）程序资源提交完毕后，申请运行mrAppMaster。（6）RM将用户的请求初始化成一个Task。（7）其中一个NodeManager领取到Task任务。（8）该NodeManager创建容器Container，并产生MRAppmaster。（9）Container从HDFS上拷贝资源到本地。（10）MRAppmaster向RM 申请运行MapTask资源。（11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。（12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。  （14）ReduceTask向MapTask获取相应分区的数据。（15）程序运行完毕后，MR会向RM申请注销自己</code></pre><p>总结：程序提交之后，找RM申请Application,告知运行程序需要的资源。RM生成一个资源分配Task任务放进yarn队列。这个Task会随机分配给Nodemanager，NodeManager领取任务后会根据资源要求创建Container容器，开始运行程序，运行完毕后向RM报告，注销资源占用和Task任务。  </p><h3 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h3><p>1.先进先出调度器（FIFO）  </p><p>2.容量调度器（Capacity Scheduler）  </p><p>3．公平调度器（Fair Scheduler）  </p><h1 id="MapReduce架构概述"><a href="#MapReduce架构概述" class="headerlink" title="MapReduce架构概述"></a>MapReduce架构概述</h1><p><img src="/2023/08/06/hadoop/3.png" alt="MapReduce架构概述">     </p><h2 id="MapReduce核心思想"><a href="#MapReduce核心思想" class="headerlink" title="MapReduce核心思想"></a>MapReduce核心思想</h2><p><img src="/2023/08/06/hadoop/15.png" alt="MapReduce核心思想">   </p><p>MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行  </p><h3 id="MapTask并行度决定机制"><a href="#MapTask并行度决定机制" class="headerlink" title="MapTask并行度决定机制"></a>MapTask并行度决定机制</h3><p>数据块：Block是HDFS物理上把数据分成一块一块。  </p><p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储  </p><p><img src="/2023/08/06/hadoop/16.png" alt="MapTask并行度决定机制">  </p><h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><p><img src="/2023/08/06/hadoop/17.png" alt="MapReduce工作流程">  </p><p><img src="/2023/08/06/hadoop/18.png" alt="MapReduce工作流程">  </p><p>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快,默认100M   </p><h2 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h2><p><img src="/2023/08/06/hadoop/19.png" alt="Shuffle机制">    </p><h2 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h2><p><img src="/2023/08/06/hadoop/20.png" alt="MapTask工作机制">    </p><h2 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a>ReduceTask工作机制</h2><p><img src="/2023/08/06/hadoop/21.png" alt="ReduceTask工作机制">  </p><p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置  </p><h1 id="Hadoop文件压缩"><a href="#Hadoop文件压缩" class="headerlink" title="Hadoop文件压缩"></a>Hadoop文件压缩</h1><p><img src="/2023/08/06/hadoop/22.png" alt="Hadoop文件压缩">  </p><h3 id="Hadoop文件压缩性能比较"><a href="#Hadoop文件压缩性能比较" class="headerlink" title="Hadoop文件压缩性能比较"></a>Hadoop文件压缩性能比较</h3><p><img src="/2023/08/06/hadoop/23.png" alt="Hadoop文件压缩性能比较">  </p><p>Gzip压缩：每个文件压缩后都在130M以内的（一个块大小内），都可以考虑Gzip压缩格式  ，不支持Split  </p><p>Bzip压缩：支持Split，压缩率很高，但压缩&#x2F;解压缩 速度慢  </p><p>LZO压缩：支持Split，合理的压缩率，是Hadoop中最流行的压缩格式    </p><p>Snappy压缩：不支持Split，压缩率比Gzip低，但Hadoop本身不支持，需要安装  </p><h3 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h3><p><img src="/2023/08/06/hadoop/24.png" alt="压缩位置选择">   </p><p>压缩可以在MapReduce作用的任意阶段启用    </p><p>速度是最优先考虑的因素，而不是压缩率  </p><h1 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h1><p>是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。   </p><p>ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功 能稳定的系统提供给用户    </p><h2 id="Zookeeper工作机制"><a href="#Zookeeper工作机制" class="headerlink" title="Zookeeper工作机制"></a>Zookeeper工作机制</h2><p><img src="/2023/08/06/hadoop/26.png" alt="Zookeeper工作机制">  </p><h3 id="Zookeeper特点"><a href="#Zookeeper特点" class="headerlink" title="Zookeeper特点"></a>Zookeeper特点</h3><p><img src="/2023/08/06/hadoop/27.png" alt="Zookeeper特点">    </p><h3 id="Zookeeper的数据结构"><a href="#Zookeeper的数据结构" class="headerlink" title="Zookeeper的数据结构"></a>Zookeeper的数据结构</h3><p><img src="/2023/08/06/hadoop/28.png" alt="Zookeeper的数据结构">    </p><h3 id="软负载均衡"><a href="#软负载均衡" class="headerlink" title="软负载均衡"></a>软负载均衡</h3><p>在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求  </p><h2 id="选举机制（面试重点）"><a href="#选举机制（面试重点）" class="headerlink" title="选举机制（面试重点）"></a>选举机制（面试重点）</h2><p><img src="/2023/08/06/hadoop/29.png" alt="Zookeeper选举机制-第一次启动">    </p><p><img src="/2023/08/06/hadoop/30.png" alt="Zookeeper选举机制-非第一次启动">   </p><h3 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h3><p>1）启动客户端    </p><pre><code>[atguigu@hadoop102 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop102:2181</code></pre><p>2）显示所有操作命令  </p><pre><code>[zk: hadoop102:2181(CONNECTED) 1] help  </code></pre><h4 id="znode-节点数据信息"><a href="#znode-节点数据信息" class="headerlink" title="znode 节点数据信息"></a>znode 节点数据信息</h4><p>1）查看当前znode中所包含的内容   </p><pre><code>[zk: hadoop102:2181(CONNECTED) 0] ls /  </code></pre><p>2）查看当前节点详细数据  </p><pre><code>[zk: hadoop102:2181(CONNECTED) 5] ls -s /[zookeeper]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x0cversion = -1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1    </code></pre><p>（1）czxid：创建节点的事务 zxid  </p><p>每次修改 ZooKeeper 状态都会产生一个 ZooKeeper 事务 ID。事务ID是 ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1在zxid2之前发生。  </p><p>（2）ctime：znode 被创建的毫秒数（从 1970 年开始）  </p><p>（3）mzxid：znode 最后更新的事务 zxid  </p><p>（4）mtime：znode 最后修改的毫秒数（从 1970 年开始）  </p><p>（5）pZxid：znode 最后更新的子节点 zxid    </p><p>（6）cversion：znode 子节点变化号，znode子节点修改次数  </p><p>（7）dataversion：znode 数据变化号  </p><p>（8）aclVersion：znode 访问控制列表的变化号  </p><p>（9）ephemeralOwner：如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。  </p><p>（10）dataLength：znode 的数据长度  </p><p>（11）numChildren：znode 子节点数量  </p><h4 id="节点类型（持久-短暂-有序号-无序号）"><a href="#节点类型（持久-短暂-有序号-无序号）" class="headerlink" title="节点类型（持久&#x2F;短暂&#x2F;有序号&#x2F;无序号）"></a>节点类型（持久&#x2F;短暂&#x2F;有序号&#x2F;无序号）</h4><p><img src="/2023/08/06/hadoop/31.png" alt="Zookeeper节点类型">   </p><p>1）分别创建2个普通节点（永久节点 + 不带序号）   </p><pre><code>[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;diaochan&quot; 注意：创建节点时，要赋值  </code></pre><p>2）获得节点的值  </p><pre><code>[zk: localhost:2181(CONNECTED) 5] get -s /sanguo  </code></pre><p>3）创建带序号的节点（永久节点 + 带序号）  </p><p>（1）先创建一个普通的根节点&#x2F;sanguo&#x2F;weiguo    </p><pre><code>[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot;</code></pre><p>（2）创建带序号的节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/zhangliao &quot;zhangliao&quot;  </code></pre><p>如果原来没有序号节点，序号从 0 开始依次递增。如果原节点下已有 2 个节点，则再排序时从 2 开始，以此类推。    </p><p>4）创建短暂节点（短暂节点 + 不带序号 or 带序号）   </p><p>（1）创建短暂的不带序号的节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot;</code></pre><p>（2）创建短暂的带序号的节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 2] create -e -s /sanguo/wuguo &quot;zhouyu&quot;    </code></pre><p>（3）在当前客户端是能查看到的  </p><pre><code>[zk: localhost:2181(CONNECTED) 3] ls /sanguo   </code></pre><p>（4）修改节点数据值   </p><pre><code>[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo &quot;simayi&quot;  </code></pre><h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><p>监听机制保证 ZooKeeper 保存的任何的数据的任何改变都能快速的响应到监听了该节点的应用程序。  </p><p><img src="/2023/08/06/hadoop/32.png" alt="监听器原理">      </p><h4 id="节点删除与查看"><a href="#节点删除与查看" class="headerlink" title="节点删除与查看"></a>节点删除与查看</h4><p>1）删除节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin</code></pre><p>2）递归删除节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 15] deleteall /sanguo/shuguo</code></pre><p>3）查看节点状态  </p><pre><code>[zk: localhost:2181(CONNECTED) 17] stat /sanguo</code></pre><h4 id="客户端向服务端写数据流程"><a href="#客户端向服务端写数据流程" class="headerlink" title="客户端向服务端写数据流程"></a>客户端向服务端写数据流程</h4><p><img src="/2023/08/06/hadoop/33.png" alt="客户端向服务端写数据流程">  </p><h2 id="ZooKeeper-分布式锁"><a href="#ZooKeeper-分布式锁" class="headerlink" title="ZooKeeper 分布式锁"></a>ZooKeeper 分布式锁</h2><p>“进程1” 在使用该资源的时候，会先去获得锁，保持独占，这样其他进程就无法访问该资源,用完该资源以后就将锁释放掉,保证了分布式系统中多个进程能够有序的访问该临界资源。  </p><p><img src="/2023/08/06/hadoop/34.png" alt="Zookeeper分布式锁">      </p><h2 id="Zookeeper企业面试真题（面试重点）总结"><a href="#Zookeeper企业面试真题（面试重点）总结" class="headerlink" title="Zookeeper企业面试真题（面试重点）总结"></a>Zookeeper企业面试真题（面试重点）总结</h2><h3 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h3><p>半数机制，超过半数的投票通过，即通过。  </p><p>（1）第一次启动选举规则：  </p><pre><code>投票过半数时，服务器 id 大的胜出  </code></pre><p>（2）第二次启动选举规则： </p><pre><code>①EPOCH 大的直接胜出  ②EPOCH 相同，事务 id 大的胜出  ③事务 id 相同，服务器 id 大的胜出  </code></pre><p>生产集群安装多少 zk 合适？  </p><pre><code>安装奇数台  </code></pre><p>生产经验：  </p><pre><code>10 台服务器：3 台 zk  20 台服务器：5 台 zk  100 台服务器：11 台 zk  200 台服务器：11 台 zk    </code></pre><p>服务器台数多：好处，提高可靠性；坏处：提高通信延时  </p><p>常用命令  </p><pre><code>ls、get、create、delete</code></pre>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%8D%9A%E8%A7%82%E8%80%8C%E7%BA%A6%E5%8F%96%EF%BC%8C%E5%8E%9A%E7%A7%AF%E8%80%8C%E8%96%84%E5%8F%91/">博观而约取，厚积而薄发</category>
      
      
      <comments>http://example.com/2023/08/06/hadoop/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>JavaSE</title>
      <link>http://example.com/2023/08/04/JavaSE/</link>
      <guid>http://example.com/2023/08/04/JavaSE/</guid>
      <pubDate>Fri, 04 Aug 2023 09:50:53 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;天行健，君子以自强不息~  &lt;/p&gt;
&lt;h2 id=&quot;Java-核心机制-Java-虚拟机-JVM-java-virtual-machine&quot;&gt;&lt;a href=&quot;#Java-核心机制-Java-虚拟机-JVM-java-virtual-machine&quot; class=&quot;hea</description>
        
      
      
      
      <content:encoded><![CDATA[<p>天行健，君子以自强不息~  </p><h2 id="Java-核心机制-Java-虚拟机-JVM-java-virtual-machine"><a href="#Java-核心机制-Java-虚拟机-JVM-java-virtual-machine" class="headerlink" title="Java 核心机制-Java 虚拟机 [JVM java virtual machine]"></a>Java 核心机制-Java 虚拟机 [JVM java virtual machine]</h2><p>JVM 是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器，包含在JDK中  </p><p>对于不同的平台，有不同的虚拟机    </p><h2 id="什么是-JDK，JRE"><a href="#什么是-JDK，JRE" class="headerlink" title="什么是 JDK，JRE"></a>什么是 JDK，JRE</h2><h3 id="JDK-JRE-java-的开发工具-java-javac-javadoc-javap-等"><a href="#JDK-JRE-java-的开发工具-java-javac-javadoc-javap-等" class="headerlink" title="JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等]"></a>JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等]</h3><p>JDK 是提供给 Java 开发人员使用的，其中包含了 java 的开发工具，也包括了 JRE。所以安装了 JDK，就不用在单独安装JRE了  </p><h3 id="JRE-JVM-Java-的核心类库-类"><a href="#JRE-JVM-Java-的核心类库-类" class="headerlink" title="JRE &#x3D; JVM + Java 的核心类库[类]"></a>JRE &#x3D; JVM + Java 的核心类库[类]</h3><p>包括 Java 虚拟机(JVM Java Virtual Machine)和 Java 程序所需的核心类库等，如果想要运行一个开发好的 Java 程序，计算机中只需要安装 JRE 即可    </p><h4 id="Java-转义字符"><a href="#Java-转义字符" class="headerlink" title="Java 转义字符"></a>Java 转义字符</h4><p>\t ：一个制表位，实现对齐的功能  </p><p>\n ：换行符  </p><p>\ ：一个\  </p><p>&quot; :一个”  </p><p>&#39; ：一个’ \r :一个回车 System.out.println(“韩顺平教育\r 北京”);  </p><h4 id="Java-中的注释类型"><a href="#Java-中的注释类型" class="headerlink" title="Java 中的注释类型"></a>Java 中的注释类型</h4><ol><li><p>单行注释 &#x2F;&#x2F;  </p></li><li><p>多行注释 &#x2F;* *&#x2F;    </p><p> 多行注释里面不允许有多行注释嵌套</p></li><li><p>文档注释 &#x2F;** *&#x2F;</p></li></ol><p><strong>文档注释：</strong>  </p><p>&#x2F;**  </p><ul><li>@author 韩顺平  </li><li>@version 1.0<br>*&#x2F;</li></ul><h4 id="Java-代码规范"><a href="#Java-代码规范" class="headerlink" title="Java 代码规范"></a>Java 代码规范</h4><p><img src="/2023/08/04/JavaSE/1.png" alt="Java代码规范">    </p><h3 id="DOS-介绍"><a href="#DOS-介绍" class="headerlink" title="DOS 介绍"></a>DOS 介绍</h3><p>Dos： Disk Operating System 磁盘操作系统  </p><h4 id="常用的-dos-命令"><a href="#常用的-dos-命令" class="headerlink" title="常用的 dos 命令"></a>常用的 dos 命令</h4><ol><li><p>查看当前目录是有什么内容 dir  </p><p> dir dir d:\abc2\test200</p></li><li><p>切换到其他盘下：盘符号 cd : change directory</p></li></ol><p>案例演示：切换到 c 盘 cd &#x2F;D c:  </p><ol start="3"><li>切换到当前盘的其他目录下 (使用相对路径和绝对路径演示), ..\表示上一级目录</li></ol><p>案例演示： cd d:\abc2\test200 cd ....\abc2\test200  </p><ol start="4"><li>切换到上一级：</li></ol><p>案例演示： cd .. 5) 切换到根目录：cd \  </p><p>案例演示：cd \  </p><ol start="6"><li><p>查看指定的目录下所有的子级目录 tree  </p></li><li><p>清屏 cls [苍老师]  </p></li><li><p>退出 DOS exit</p></li></ol><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p><strong>变量三要素：变量名+值+数据类型</strong>  </p><p>变量相当于内存中一个数据存储空间的表示，你可以把变量看做是一个房间的门牌号，通过门牌号我们可以找到房间，而通过变量名可以访问到变量(值)    </p><ol><li><p>声明变量  </p><p> int a;  </p></li><li><p>赋值  </p><p> a &#x3D; 60; &#x2F;&#x2F;应该这么说: 把 60 赋给 a</p></li></ol><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>每一种数据都定义了明确的数据类型，在内存中分配了不同大小的内存空间(字节)。  </p><p><img src="/2023/08/04/JavaSE/2.png" alt="数据类型">    </p><h4 id="整型的类型"><a href="#整型的类型" class="headerlink" title="整型的类型"></a>整型的类型</h4><p><img src="/2023/08/04/JavaSE/3.png" alt="数据类型">     </p><p>&#x2F;&#x2F;Java 的整型常量（具体值）默认为 int 型，声明 long 型常量须后加‘l’或‘L’  </p><pre><code>int n1 = 1;//4 个字节long n3 = 1L;//长整型  </code></pre><h4 id="浮点类型"><a href="#浮点类型" class="headerlink" title="浮点类型"></a>浮点类型</h4><p><img src="/2023/08/04/JavaSE/4.png" alt="数据类型">    </p><p>&#x2F;&#x2F;Java 的浮点型常量(具体值)默认为 double 型，声明 float 型常量，须后加‘f’或‘F’  </p><pre><code>float num2 = 1.1F;    double num3 = 1.1; double num4 = 1.1f; </code></pre><p>十进制数形式：如：5.12 512.0f .512 (必须有小数点）   </p><p>Java类的组织形式  </p><p><img src="/2023/08/04/JavaSE/5.png" alt="Java类的组织形式">   </p><h4 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h4><p><img src="/2023/08/04/JavaSE/6.png" alt="字符编码">  </p><h4 id="基本数据类型转换"><a href="#基本数据类型转换" class="headerlink" title="基本数据类型转换"></a>基本数据类型转换</h4><h5 id="自动类型转换"><a href="#自动类型转换" class="headerlink" title="自动类型转换"></a>自动类型转换</h5><p><img src="/2023/08/04/JavaSE/7.png" alt="自动类型转换">    </p><h5 id="强制类型转换"><a href="#强制类型转换" class="headerlink" title="强制类型转换"></a>强制类型转换</h5><p>自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符 ( )，但可能造成精度降低或溢出,格外要注意  </p><p><img src="/2023/08/04/JavaSE/8.png" alt="强制类型转换">  </p><h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><h3 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h3><p>说明逻辑运算规则：  </p><ol><li><p>a&amp;b : &amp; 叫逻辑与：规则：当 a 和 b 同时为 true ,则结果为 true, 否则为 false  </p></li><li><p>a&amp;&amp;b : &amp;&amp; 叫短路与：规则：当 a 和 b 同时为 true ,则结果为 true,否则为 false  </p></li><li><p>a|b : | 叫逻辑或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p></li><li><p>a||b : || 叫短路或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p></li><li><p>!a : 叫取反，或者非运算。当 a 为 true, 则结果为 false, 当 a 为 false 是，结果为 true  </p></li><li><p>a^b: 叫逻辑异或，当 a 和 b 不同时，则结果为 true, 否则为 false</p></li></ol><p><strong>&amp;&amp; 和 &amp; 使用区别</strong>  </p><ol><li><p>&amp;&amp;短路与：如果第一个条件为 false，则第二个条件不会判断，最终结果为 false，效率高  </p></li><li><p>&amp; 逻辑与：不管第一个条件是否为 false，第二个条件都要判断，效率低</p></li></ol><p><strong>|| 和 | 使用区别</strong>  </p><ol><li><p>||短路或：如果第一个条件为 true，则第二个条件不会判断，最终结果为 true，效率高  </p></li><li><p>| 逻辑或：不管第一个条件是否为 true，第二个条件都要判断，效率低</p></li></ol><h3 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h3><p>条件表达式 ? 表达式 1: 表达式 2;  </p><p>运算规则：  </p><p>1.如果条件表达式为 true，运算后的结果是表达式 1；  </p><p>2.如果条件表达式为 false，运算后的结果是表达式 2；  </p><p>口诀: [一灯大师：一真大师]  </p><h5 id="接收控制台输入Scanner"><a href="#接收控制台输入Scanner" class="headerlink" title="接收控制台输入Scanner"></a>接收控制台输入Scanner</h5><pre><code>import java.util.Scanner;    Scanner myScanner = new Scanner(System.in);  </code></pre><h5 id="原码、反码、补码-重点-难点"><a href="#原码、反码、补码-重点-难点" class="headerlink" title="原码、反码、补码(重点 难点)"></a>原码、反码、补码(重点 难点)</h5><p><img src="/2023/08/04/JavaSE/9.png">    </p><h2 id="程序控制结构"><a href="#程序控制结构" class="headerlink" title="程序控制结构"></a>程序控制结构</h2><p>主要有三大流程控制语句。  </p><ol><li><p>顺序控制  </p></li><li><p>分支控制    </p><ol><li>单分支 if  </li><li>双分支 if-else  </li><li>多分支 if-else if -….-else  </li><li>switch分支<br> switch(表达式){<br>         case xxx<br>                 }</li></ol></li><li><p>循环控制</p></li></ol><p><strong>for 循环控制</strong><br>    for(循环变量初始化;循环条件;循环变量迭代){<br>        循环操作(可以多条语句)<br>            }  </p><pre><code>eg:  for(int i = 1;i&lt;=10;i++)&#123;    System.out.println(&quot;Hello World ~ ！&quot;)&#125;</code></pre><p><strong>while 循环控制</strong>  </p><pre><code>循环变量初始化;  while（循环条件）&#123;    循环体（语句）；    循环变量迭代；&#125;  eg:int i = 1;while (i &lt;= 10)&#123;    System.out.println(&quot;Hello World ~ !&quot;)      i ++ &#125;  </code></pre><p><strong>do..while 循环控制</strong>  </p><pre><code>循环变量初始化;do&#123;    循环体(语句);    循环变量迭代;&#125;while(循环条件);  </code></pre><p>先执行，再判断，也就是说，一定会至少执行一次    </p><h2 id="跳转控制语句-break"><a href="#跳转控制语句-break" class="headerlink" title="跳转控制语句-break"></a>跳转控制语句-break</h2><p>break 语句用于终止某个语句块的执行，一般使用在 switch 或者循环[for , while , do-while]中  </p><h2 id="跳转控制语句-continue"><a href="#跳转控制语句-continue" class="headerlink" title="跳转控制语句-continue"></a>跳转控制语句-continue</h2><p>continue 语句用于结束本次循环，继续执行下一次循环  </p><h2 id="跳转控制语句-return"><a href="#跳转控制语句-return" class="headerlink" title="跳转控制语句-return"></a>跳转控制语句-return</h2><p>return 使用在方法，表示跳出所在的方法  </p><h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>数组可以存放多个同一类型的数据。数组也是一种数据类型，是<strong>引用类型</strong>  </p><p><img src="/2023/08/04/JavaSE/10.png" alt="数组的使用">  </p><pre><code>方式一：int a[] = new Int[5]   方式二：       int[] a;       a = new Int[10];   方式三：      int a[] = &#123;2,3,4,5,6&#125;  </code></pre><h3 id="数组赋值机制"><a href="#数组赋值机制" class="headerlink" title="数组赋值机制"></a>数组赋值机制</h3><p><img src="/2023/08/04/JavaSE/11.png" alt="数组的使用">      </p><h1 id="面向对象编程-基础部分"><a href="#面向对象编程-基础部分" class="headerlink" title="面向对象编程(基础部分)"></a>面向对象编程(基础部分)</h1><p>类和对象的区别和联系</p><ol><li><p>类是抽象的，概念的，代表一类事物,比如人类,猫类.., 即它是数据类型  </p></li><li><p>对象是具体的，实际的，代表一个具体事物, 即 是实例   </p></li><li><p>类是对象的模板，对象是类的一个个体，对应一个实例</p></li></ol><h2 id="对象在内存中存在形式-重要的-必须搞清楚"><a href="#对象在内存中存在形式-重要的-必须搞清楚" class="headerlink" title="对象在内存中存在形式(重要的)必须搞清楚"></a>对象在内存中存在形式(重要的)必须搞清楚</h2><p><img src="/2023/08/04/JavaSE/12.png" alt="对象在内存中存在形式">     </p><h3 id="类和对象的内存分配机制"><a href="#类和对象的内存分配机制" class="headerlink" title="类和对象的内存分配机制"></a>类和对象的内存分配机制</h3><p>Java 内存的结构分析  </p><ol><li><p>栈： 一般存放基本数据类型(局部变量)  </p></li><li><p>堆： 存放对象(Cat cat , 数组等)  </p></li><li><p>方法区：常量池(常量，比如字符串)， 类加载信息</p></li></ol><h2 id="属性-成员变量-字段"><a href="#属性-成员变量-字段" class="headerlink" title="属性&#x2F;成员变量&#x2F;字段"></a>属性&#x2F;成员变量&#x2F;字段</h2><p>成员变量 &#x3D; 属性 &#x3D; field(字段)  </p><p>属性是类的一个组成部分，一般是基本数据类型,也可是引用类型(对象，数组)    </p><h2 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h2><p>Cat cat1 &#x3D; new Cat();  </p><h3 id="成员方法"><a href="#成员方法" class="headerlink" title="成员方法"></a>成员方法</h3><pre><code>class Person &#123;    String name;    int age;      //方法(成员方法)      public void speak() &#123;        System.out.println(&quot;我是一个好人&quot;);        &#125;    &#125;  </code></pre><h3 id="方法的调用机制原理"><a href="#方法的调用机制原理" class="headerlink" title="方法的调用机制原理"></a>方法的调用机制原理</h3><p><img src="/2023/08/04/JavaSE/13.png">  </p><h3 id="成员方法的好处"><a href="#成员方法的好处" class="headerlink" title="成员方法的好处"></a>成员方法的好处</h3><ol><li><p>提高代码的复用性  </p></li><li><p>可以将实现的细节封装起来，然后供其他用户来调用即可</p></li></ol><h3 id="成员方法的定义"><a href="#成员方法的定义" class="headerlink" title="成员方法的定义"></a>成员方法的定义</h3><pre><code>访问修饰符 返回数据类型 方法名（形参列表..） &#123;    //方法体语句；    return 返回值;&#125;</code></pre><h3 id="传参"><a href="#传参" class="headerlink" title="传参"></a>传参</h3><p>引用类型传递的是地址（传递也是值，但是值是地址），可以通过形参影响实参！  </p><h3 id="方法递归调用-非常非常重要，比较难"><a href="#方法递归调用-非常非常重要，比较难" class="headerlink" title="方法递归调用(非常非常重要，比较难)"></a>方法递归调用(非常非常重要，比较难)</h3><p>递归就是方法自己调用自己  </p><p>递归重要规则  </p><p><img src="/2023/08/04/JavaSE/14.png"></p><h2 id="方法重载-OverLoad"><a href="#方法重载-OverLoad" class="headerlink" title="方法重载(OverLoad)"></a>方法重载(OverLoad)</h2><p>java 中允许同一个类中，多个同名方法的存在，但要求 形参列表不一致！  </p><p>案例：类：MyCalculator 方法：calculate  </p><ol><li>calculate(int n1, int n2) &#x2F;&#x2F;两个整数的和  </li><li>calculate(int n1, double n2) &#x2F;&#x2F;一个整数，一个 double 的和  </li><li>calculate(double n2, int n1)&#x2F;&#x2F;一个 double ,一个 Int 和  </li><li>calculate(int n1, int n2,int n3)&#x2F;&#x2F;三个 int 的和</li></ol><p><img src="/2023/08/04/JavaSE/15.png" alt="方法重载">  </p><h3 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h3><p>java 允许将同一个类中多个同名同功能但参数个数不同的方法，封装成一个方法。<br>就可以通过可变参数实现</p><p>eg:方法 sum 【可以计算 2 个数的和，3 个数的和 ， 4. 5， 。。】  </p><pre><code>//1. int... 表示接受的是可变参数，类型是 int ,即可以接收多个 int(0-多)//2. 使用可变参数时，可以当做数组来使用 即 nums 可以当做数组//3. 遍历 nums 求和即可public int sum(int... nums) &#123;//System.out.println(&quot;接收的参数个数=&quot; + nums.length);int res = 0;for(int i = 0; i &lt; nums.length; i++) &#123;res += nums[i];&#125;return res;&#125;&#125;  </code></pre><h3 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h3><p>变量：  </p><p>1.全局变量（属性）</p><p>2.局部变量（局部变量一般是指在成员方法中定义的变量）</p><p>全局变量和局部变量可以重名</p><p><img src="/2023/08/04/JavaSE/16.png" alt="变量作用域">  </p><p>全局变量和局部变量的区别  </p><p><img src="/2023/08/04/JavaSE/17.png" alt="全局变量和局部变量的区别">    </p><h3 id="构造方法-构造器"><a href="#构造方法-构造器" class="headerlink" title="构造方法&#x2F;构造器"></a>构造方法&#x2F;构造器</h3><p>在创建人类的对象时，就直接指定这个对象的年龄和姓名，该怎么做? 这时就可以使用构造器  </p><p>[修饰符] 方法名(形参列表){<br>方法体;<br>}   </p><ol><li><p>构造器的修饰符可以默认， 也可以是 public protected private  </p></li><li><p>构造器没有返回值   </p></li><li><p>方法名 和类名字必须一样  </p></li><li><p>参数列表 和 成员方法一样的规则  </p></li><li><p>构造器的调用, 由系统完成</p></li></ol><p>构造方法又叫构造器(constructor)，是类的一种特殊的方法，它的主要作用是完成对新对象的初始化  </p><p><img src="/2023/08/04/JavaSE/18.png" alt="构造器使用注意事项"></p><p><img src="/2023/08/04/JavaSE/19.png" alt="构造器使用注意事项"></p><h3 id="this-关键字"><a href="#this-关键字" class="headerlink" title="this 关键字"></a>this 关键字</h3><p><img src="/2023/08/04/JavaSE/20.png" alt="This关键字"></p><ol><li><p>this 关键字可以用来访问本类的属性、方法、构造器  </p></li><li><p>this 用于区分当前类的属性和局部变量  </p></li><li><p>访问成员方法的语法：this.方法名(参数列表);    </p></li><li><p>访问构造器语法：this(参数列表); 注意只能在构造器中使用(即只能在构造器中访问另外一个构造器, 必须放在第一条语句)  </p></li><li><p>this 不能在类定义的外部使用，只能在类定义的方法中使用。</p></li></ol><h1 id="面向对象编程-中级部分"><a href="#面向对象编程-中级部分" class="headerlink" title="面向对象编程(中级部分)"></a>面向对象编程(中级部分)</h1><p>IDEA 常用快捷键  </p><ol><li><p>删除当前行, 默认是 ctrl + Y 自己配置 ctrl + d  </p></li><li><p>复制当前行, 自己配置 ctrl + alt + 向下光标  </p></li><li><p>补全代码 alt + &#x2F;  </p></li><li><p>添加注释和取消注释 ctrl + &#x2F; 【第一次是添加注释，第二次是取消注释】  </p></li><li><p>导入该行需要的类 先配置 auto import , 然后使用 alt+enter 即可  </p></li><li><p>快速格式化代码 ctrl + alt + L  </p></li><li><p>快速运行程序 自己定义 alt + R  </p></li><li><p>生成构造器等 alt + insert [提高开发效率]  </p></li><li><p>查看一个类的层级关系 ctrl + H [学习继承后，非常有用]  </p></li><li><p>将光标放在一个方法上，输入 ctrl + B , 可以定位到方法 [学继承后，非常有用]  </p></li><li><p>自动的分配变量名 , 通过 在后面假 .var [老师最喜欢的]  </p></li><li><p>还有很多其它的快捷键</p></li></ol><h2 id="包"><a href="#包" class="headerlink" title="包"></a>包</h2><p><img src="/2023/08/04/JavaSE/21.png" alt="包">  </p><p>包的本质 </p><p><img src="/2023/08/04/JavaSE/22.png">  </p><p>包的命名：  </p><p>com.公司名.项目名.业务模块名  </p><h3 id="Java常用的包"><a href="#Java常用的包" class="headerlink" title="Java常用的包"></a>Java常用的包</h3><p>一个包下,包含很多的类,java 中常用的包有:  </p><ol><li><p>java.lang.* &#x2F;&#x2F;lang 包是基本包，默认引入，不需要再引入.  </p></li><li><p>java.util.* &#x2F;&#x2F;util 包，系统提供的工具包, 工具类，使用 Scanner  </p></li><li><p>java.net.* &#x2F;&#x2F;网络包，网络开发  </p></li><li><p>java.awt.* &#x2F;&#x2F;是做 java 的界面开发，GUI</p></li></ol><p>引入包的语法：  </p><p>import 包;  </p><p>eg:  import java.util.*  &#x2F;&#x2F;表示将java.util包所有都引入    </p><p>我们需要使用到哪个类，就导入哪个类即可，不建议使用*导入   </p><h2 id="访问修饰符"><a href="#访问修饰符" class="headerlink" title="访问修饰符"></a>访问修饰符</h2><p>java 提供四种访问控制修饰符号，用于控制方法和属性(成员变量)的访问权限（范围）   </p><ol><li><p>公开级别:用 public 修饰,对外公开  </p></li><li><p>受保护级别:用 protected 修饰,对子类和同一个包中的类公开  </p></li><li><p>默认级别:没有修饰符号,向同一个包的类公开.   </p></li><li><p>私有级别:用 private 修饰,只有类本身可以访问,不对外公开</p></li></ol><p><img src="/2023/08/04/JavaSE/23.png" alt="访问控制符">  </p><p><img src="/2023/08/04/JavaSE/24.png" alt="访问控制符使用说明">   </p><h2 id="面向对象编程三大特征"><a href="#面向对象编程三大特征" class="headerlink" title="面向对象编程三大特征"></a>面向对象编程三大特征</h2><p>封装、继承和多态    </p><h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><p><img src="/2023/08/04/JavaSE/25.png" alt="封装"></p><p>封装的实现步骤    </p><p><img src="/2023/08/04/JavaSE/26.png" alt="封装的实现步骤">    </p><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>继承可以解决代码复用,让我们的编程更加靠近人类思维.当多个类存在相同的属性(变量)和方法时,可以从这些类中抽象出父类,在父类中定义这些相同的属性和方法，所有的子类不需要重新定义这些属性和方法，只需要通过 extends 来声明继承父类即可  </p><p><img src="/2023/08/04/JavaSE/27.png" alt="继承">  </p><p>继承的基本语法：  </p><p>class 子类 extends 父类 {</p><p>}<br>子类就会自动拥有父类定义的属性和方法<br>子类又叫超类，基类<br>子类又叫派生类  </p><p>继承给编程带来的便利  </p><ol><li><p>代码的复用性提高了  </p></li><li><p>代码的扩展性和维护性提高了</p></li></ol><p>**继承的细节问题： ** </p><ol><li><p>子类继承了所有的属性和方法，非私有的属性和方法可以在子类直接访问, 但是私有属性和方法不能在子类直接访问，要通过父类提供公共的方法去访问  </p></li><li><p>子类必须调用父类的构造器,完成父类的初始化  </p></li><li><p>当创建子类对象时，不管使用子类的哪个构造器，默认情况下总会去调用父类的无参构造器，如果父类没有提供无参构造器，则必须在子类的构造器中用 super 去指定使用父类的哪个构造器完成对父类的初始化工作，否则，编译不会通过(怎么理解。)   </p></li><li><p>如果希望指定去调用父类的某个构造器，则显式的调用一下 : super(参数列表)  </p></li><li><p>super 在使用时，必须放在构造器第一行(super 只能在构造器中使用)  </p></li><li><p>super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器  </p></li><li><p>java 所有类都是 Object 类的子类, Object 是所有类的基类.  </p></li><li><p>父类构造器的调用不限于直接父类！将一直往上追溯直到 Object 类(顶级父类)    </p></li><li><p>子类最多只能继承一个父类(指直接继承)，即 java 中是单继承机制。<br>思考：如何让 A 类继承 B 类和 C 类？ 【A 继承 B， B 继承 C】  </p></li><li><p>不能滥用继承，子类和父类之间必须满足 is-a 的逻辑关系</p></li></ol><p><strong>输入 ctrl + H 可以看到类的继承关系</strong></p><pre><code>public class Sub extends Base &#123; //子类    public Sub(String name, int age) &#123;    //1. 调用父类的无参构造器, 如下或者什么都不写,默认就是调用 super()    //super();//父类的无参构造器    //2. 调用父类的 Base(String name) 构造器    //super(&quot;hsp&quot;);    //调用父类的 Base(String name, int age) 构造器    super(&quot;king&quot;, 20);    //细节： super 在使用时，必须放在构造器第一行    //细节: super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器    //this() 不能再使用了    System.out.println(&quot;子类 Sub(String name, int age)构造器被调用....&quot;);    &#125;</code></pre><p>子类创建的内存布局  </p><p><img src="/2023/08/04/JavaSE/28.png" alt="子类创建的内存布局">    </p><h3 id="super-关键字"><a href="#super-关键字" class="headerlink" title="super 关键字"></a>super 关键字</h3><p>super 代表父类的引用，用于访问父类的属性、方法、构造器  </p><p><img src="/2023/08/04/JavaSE/29.png" alt="super关键字">  </p><p>&#x2F;&#x2F; (1)先找本类，如果有，则调用  </p><p>&#x2F;&#x2F; (2)如果没有，则找父类(如果有，并可以调用，则调用)  </p><p>&#x2F;&#x2F; (3)如果父类没有，则继续找父类的父类,整个规则，就是一样的,直到 Object 类  </p><p>&#x2F;&#x2F; 提示：如果查找方法的过程中，找到了，但是不能访问， 则报错, cannot access  </p><p>&#x2F;&#x2F; 如果查找方法的过程中，没有找到，则提示方法不存在    </p><p><img src="/2023/08/04/JavaSE/30.png" alt="Super关键字的用法细节">      </p><p><strong>super 和 this 的比较</strong>    </p><p><img src="/2023/08/04/JavaSE/31.png" alt="super和this的比较">    </p><h2 id="方法重写-覆盖-override"><a href="#方法重写-覆盖-override" class="headerlink" title="方法重写&#x2F;覆盖(override)"></a>方法重写&#x2F;覆盖(override)</h2><p><img src="/2023/08/04/JavaSE/32.png" alt="方法重写">  </p><p><img src="/2023/08/04/JavaSE/33.png" alt="方法重写的注意事项">    </p><p><strong>方法重载和方法重写的区别</strong>  </p><p><img src="/2023/08/04/JavaSE/34.png" alt="方法重载和方法重写的区别">  </p><h2 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h2><p>多态是建立在封装和继承基础之上的<br><img src="/2023/08/04/JavaSE/35.png" alt="多态">     </p><p>多态的具体体现  </p><ol><li>方法的多态</li></ol><p>重写和重载就体现多态    </p><ol start="2"><li>对象的多态 (核心，困难，重点)</li></ol><p><img src="/2023/08/04/JavaSE/36.png" alt="多态案例">     </p><p>多态的前提是：两个对象(类)存在继承关系  </p><p>多态的向上转型   </p><p><img src="/2023/08/04/JavaSE/37.png" alt="多态的向上转型">   </p><p>多态向下转型  </p><p><img src="/2023/08/04/JavaSE/38.png" alt="多态的向下转型">    </p><p>属性没有重写之说！属性的值看编译类型   </p><p>instanceOf 比较操作符，用于判断对象的运行类型是否为 XX 类型或 XX 类型的子类型  </p><p>java 的动态绑定机制(非常非常重要.)  page 365</p><p>JDBC page 1119  </p><p>正则表达式 page 1210</p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%A4%A9%E8%A1%8C%E5%81%A5%EF%BC%8C%E5%90%9B%E5%AD%90%E4%BB%A5%E8%87%AA%E5%BC%BA%E4%B8%8D%E6%81%AF/">天行健，君子以自强不息</category>
      
      
      <comments>http://example.com/2023/08/04/JavaSE/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数仓建模</title>
      <link>http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/</link>
      <guid>http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/</guid>
      <pubDate>Thu, 03 Aug 2023 10:25:47 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;人生在勤，不索何获&quot;&gt;&lt;a href=&quot;#人生在勤，不索何获&quot; class=&quot;headerlink&quot; title=&quot;人生在勤，不索何获&quot;&gt;&lt;/a&gt;人生在勤，不索何获&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://help.aliyun.com/zh/datawo</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="人生在勤，不索何获"><a href="#人生在勤，不索何获" class="headerlink" title="人生在勤，不索何获"></a>人生在勤，不索何获</h1><p><a href="https://help.aliyun.com/zh/dataworks/user-guide/dataworks-data-modeling/?spm=a2c4g.11186623.0.0.6cbf2c36NLQ1IR">阿里数仓建模理论</a></p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E4%BA%BA%E7%94%9F%E5%9C%A8%E5%8B%A4%EF%BC%8C%E4%B8%8D%E7%B4%A2%E4%BD%95%E8%8E%B7/">人生在勤，不索何获</category>
      
      
      <comments>http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>kafka3.0.0学习记录</title>
      <link>http://example.com/2023/08/03/kafka/</link>
      <guid>http://example.com/2023/08/03/kafka/</guid>
      <pubDate>Thu, 03 Aug 2023 02:18:49 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;黄沙百战穿金甲，不破楼兰终不还！   &lt;/p&gt;
&lt;p&gt;相关学习文档&lt;br&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw&quot;&gt;https://pan.baidu.com/s/1GAiGG8E6vI94YO</description>
        
      
      
      
      <content:encoded><![CDATA[<p>黄沙百战穿金甲，不破楼兰终不还！   </p><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw">https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw</a><br>提取码：85vn   </p><h1 id="1-Kafka概述"><a href="#1-Kafka概述" class="headerlink" title="1.Kafka概述"></a>1.Kafka概述</h1><h2 id="1-1定义："><a href="#1-1定义：" class="headerlink" title="1.1定义："></a>1.1定义：</h2><p>Kafka传 统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（Message<br>Queue），主要应用于大数据实时处理领域。    </p><p>发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息<br>分为不同的类别，订阅者只接收感兴趣的消息。  </p><h2 id="1-2消息队列"><a href="#1-2消息队列" class="headerlink" title="1.2消息队列"></a>1.2消息队列</h2><p>大数据场景主要采用 Kafka 作为消息队列。  </p><h3 id="1-2-1传统消息队列的应用场景"><a href="#1-2-1传统消息队列的应用场景" class="headerlink" title="1.2.1传统消息队列的应用场景"></a>1.2.1传统消息队列的应用场景</h3><p>传统的消息队列的主要应用场景包括：缓存&#x2F;消峰、解耦和异步通信。</p><p>缓冲&#x2F;消峰：解决生产消息和消费消息的处理速度不一致的情况。 </p><p>解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p><p><img src="/2023/08/03/kafka/kafka1.png" alt="解耦"> </p><p>异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。  </p><h3 id="1-2-2消息队列的两种模式"><a href="#1-2-2消息队列的两种模式" class="headerlink" title="1.2.2消息队列的两种模式"></a>1.2.2消息队列的两种模式</h3><p><img src="/2023/08/03/kafka/2.png" alt="消息队列的两种模式">  </p><h2 id="1-3kafka基础架构"><a href="#1-3kafka基础架构" class="headerlink" title="1.3kafka基础架构"></a>1.3kafka基础架构</h2><p><img src="/2023/08/03/kafka/3.png" alt="kafka基础架构">   </p><p>（1）Producer：消息生产者，就是向 Kafka broker 发消息的客户端。  </p><p>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。  </p><p>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。消费者组内每个消<br>费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不<br>影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。  </p><p>（4）Broker：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个broker 可以容纳多个 topic。  </p><p>（5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个 topic。  </p><p>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。  </p><p>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个Follower。  </p><p>（8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数<br>据的对象都是 Leader。  </p><p>（9）Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和<br>Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Leader。    </p><h2 id="2kafka快速入门"><a href="#2kafka快速入门" class="headerlink" title="2kafka快速入门"></a>2kafka快速入门</h2><p>kafka命令行操作    </p><p>1）查看操作主题命令参数    </p><pre><code> bin/kafka-topics.sh    </code></pre><p>2）查看当前服务器中的所有 topic  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list</code></pre><p>3）创建 first topic  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 1 --replication-factor 3 --topic first  </code></pre><p>选项说明：    </p><pre><code>--topic 定义 topic 名  --replication-factor 定义副本数  --partitions 定义分区数  </code></pre><p>4）查看 first 主题的详情  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first</code></pre><p>5）修改分区数（注意：分区数只能增加，不能减少）  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3 </code></pre><p>6）删除 topic    </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first</code></pre><h2 id="生产者命令行操作"><a href="#生产者命令行操作" class="headerlink" title="生产者命令行操作"></a>生产者命令行操作</h2><p>发送消息</p><pre><code>bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</code></pre><h2 id="消费者命令行操作"><a href="#消费者命令行操作" class="headerlink" title="消费者命令行操作"></a>消费者命令行操作</h2><h3 id="消费-first-主题中的数据"><a href="#消费-first-主题中的数据" class="headerlink" title="消费 first 主题中的数据"></a>消费 first 主题中的数据</h3><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first</code></pre><h3 id="把主题中所有的数据都读取出来（包括历史数据）"><a href="#把主题中所有的数据都读取出来（包括历史数据）" class="headerlink" title="把主题中所有的数据都读取出来（包括历史数据）"></a>把主题中所有的数据都读取出来（包括历史数据）</h3><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first  </code></pre><h2 id="生产者重要参数"><a href="#生产者重要参数" class="headerlink" title="生产者重要参数"></a>生产者重要参数</h2><p>acks </p><pre><code>1）0：生产者发送过来的数据，不需要等数据落盘应答。  2）1：生产者发送过来的数据，Leader 收到数据后应答。  3）-1（all）：生产者发送过来的数据，Leader+和 isr 队列里面的所有节点收齐数据后应答。默认值是-1，-1 和all 是等价的。  </code></pre><p>enable.idempotence  </p><p>是否开启幂等性，默认 true，开启幂等性。  </p><p>compression.type  </p><p>生产者发送的所有数据的压缩方式。默认是 none，也就是不压缩。  </p><p>支持压缩类型：none、gzip、snappy、lz4 和 zstd。    </p><h2 id="生产者分区"><a href="#生产者分区" class="headerlink" title="生产者分区"></a>生产者分区</h2><h3 id="1-分区好处"><a href="#1-分区好处" class="headerlink" title="1.分区好处"></a>1.分区好处</h3><p>（1）便于合理使用存储资源  </p><p>每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一<br>块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果   </p><p>（2）提高并行度  </p><p>生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。  </p><h2 id="生产经验——生产者如何提高吞吐量"><a href="#生产经验——生产者如何提高吞吐量" class="headerlink" title="生产经验——生产者如何提高吞吐量"></a>生产经验——生产者如何提高吞吐量</h2><p>batch.size：批次大小，默认16k   </p><p>linger.ms：等待时间，修改为5-100ms  </p><p>compression.type：压缩snappy   </p><p>RecordAccumulator：缓冲区大小，修改为64m  </p><h2 id="ack-应答原理"><a href="#ack-应答原理" class="headerlink" title="ack 应答原理"></a>ack 应答原理</h2><p><img src="/2023/08/03/kafka/4.png" alt="ack应答原理">  </p><p><img src="/2023/08/03/kafka/5.png" alt="ack应答原理"> </p><p>数据完全可靠条件 &#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2  </p><h3 id="可靠性总结："><a href="#可靠性总结：" class="headerlink" title="可靠性总结："></a>可靠性总结：</h3><p>acks&#x3D;0，生产者发送过来数据就不管了，可靠性差，效率高；  </p><p>acks&#x3D;1，生产者发送过来数据Leader应答，可靠性中等，效率中等；  </p><p>acks&#x3D;-1，生产者发送过来数据Leader和ISR队列里面所有Follwer应答，可靠性高，效率低；  </p><p>在生产环境中，acks&#x3D;0很少使用；acks&#x3D;1，一般用于传输普通日志，允许丢个别数据；acks&#x3D;-1，一般用于传输和钱相关的数据，对可靠性要求比较高的场景。    </p><h2 id="生产经验——数据去重"><a href="#生产经验——数据去重" class="headerlink" title="生产经验——数据去重"></a>生产经验——数据去重</h2><p>至少一次（At Least Once）&#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2  </p><p>最多一次（At Most Once）&#x3D; ACK级别设置为0  </p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>At Least Once可以保证数据不丢失，但是不能保证数据不重复；  </p><p>At Most Once可以保证数据不重复，但是不能保证数据不丢失。  </p><p>精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。  </p><p>Kafka 0.11版本以后，引入了一项重大特性：幂等性和事务。  </p><h3 id="幂等性原理"><a href="#幂等性原理" class="headerlink" title="幂等性原理"></a>幂等性原理</h3><p>幂等性就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。  </p><p>精确一次（Exactly Once） &#x3D; 幂等性 + 至少一次（ ack&#x3D;-1 + 分区副本数&gt;&#x3D;2 + ISR最小副本数量&gt;&#x3D;2）  </p><p>幂等性只能保证的是在单分区单会话内不重复。  </p><h4 id="如何使用幂等性"><a href="#如何使用幂等性" class="headerlink" title="如何使用幂等性"></a>如何使用幂等性</h4><p>开启参数 enable.idempotence 默认为 true，false 关闭。    </p><p>开启事务，必须开启幂等性。  </p><h2 id="生产经验——数据乱序"><a href="#生产经验——数据乱序" class="headerlink" title="生产经验——数据乱序"></a>生产经验——数据乱序</h2><p>）kafka在1.x版本之前保证数据单分区有序，条件如下：  </p><p>max.in.flight.requests.per.connection&#x3D;1（不需要考虑是否开启幂等性）。  </p><p>2）kafka在1.x及以后版本保证数据单分区有序，条件如下：    </p><p>（1）未开启幂等性  </p><p>max.in.flight.requests.per.connection需要设置为1。  </p><p>（2）开启幂等性   </p><p>max.in.flight.requests.per.connection需要设置小于等于5。    </p><h2 id="Kafka-副本"><a href="#Kafka-副本" class="headerlink" title="Kafka 副本"></a>Kafka 副本</h2><h3 id="副本基本信息"><a href="#副本基本信息" class="headerlink" title="副本基本信息"></a>副本基本信息</h3><p>（1）Kafka 副本作用：提高数据可靠性。  </p><p>（2）Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；  </p><p>（3）Kafka 中副本分为：Leader 和 Follower。    </p><p>（4）Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。AR &#x3D; ISR + OSR  </p><p>ISR，表示和 Leader 保持同步的 Follower 集合。如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值由 replica.lag.time.max.ms参数设定，默认 30s。Leader 发生故障之后，就会从 ISR 中选举新的 Leader    </p><p>OSR，表示 Follower 与 Leader 副本同步时，延迟过多的副本。    </p><h3 id="Leader-选举流程"><a href="#Leader-选举流程" class="headerlink" title="Leader 选举流程"></a>Leader 选举流程</h3><p>Kafka 集群中有一个 broker 的 Controller 会被选举为 Controller Leader，负责管理集群broker 的上下线，所有 topic 的分区副本分配和 Leader 选举等工作。  </p><p><img src="/2023/08/03/kafka/6.png" alt="Leader选举流程">  </p><h3 id="Leader-和-Follower-故障处理细节"><a href="#Leader-和-Follower-故障处理细节" class="headerlink" title="Leader 和 Follower 故障处理细节"></a>Leader 和 Follower 故障处理细节</h3><p><img src="/2023/08/03/kafka/7.png" alt="Follower故障处理细节">   </p><p><img src="/2023/08/03/kafka/8.png" alt="Leader故障处理细节"></p><h3 id="生产经验——Leader-Partition-负载平衡"><a href="#生产经验——Leader-Partition-负载平衡" class="headerlink" title="生产经验——Leader Partition 负载平衡"></a>生产经验——Leader Partition 负载平衡</h3><p>正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p><p>auto.leader.rebalance.enable 默认是 true。 自动 Leader Partition 平衡。生产环境中，leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭。   </p><p>leader.imbalance.per.broker.percentage 默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器<br>会触发 leader 的平衡。  </p><p>leader.imbalance.check.interval.seconds 默认值 300 秒。检查 leader 负载是否平衡的间隔时间。  </p><h3 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h3><p>1）Topic 数据的存储机制</p><p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。  </p><p><img src="/2023/08/03/kafka/9.png" alt="Kafka文件存储机制">   </p><p>Topic数据存储位置：每个broker节点的 kafka&#x2F;datas&#x2F;目录下</p><p>通过工具查看 index 和 log 信息<br>[atguigu@hadoop104 first-1]$ kafka-run-class.sh kafka.tools.DumpLogSegments –files .&#x2F;00000000000000000000.index   </p><pre><code>Dumping ./00000000000000000000.index  offset: 3 position: 152 </code></pre><h4 id="Log文件和Index文件详解"><a href="#Log文件和Index文件详解" class="headerlink" title="Log文件和Index文件详解"></a>Log文件和Index文件详解</h4><p><img src="/2023/08/03/kafka/10.png" alt="Log文件和Index文件详解">    </p><p>log.segment.bytes Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，默认值 1G。  </p><p>log.index.interval.bytes 默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。 稀疏索引。    </p><h3 id="文件清理策略"><a href="#文件清理策略" class="headerlink" title="文件清理策略"></a>文件清理策略</h3><p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。    </p><p>Kafka 中提供的日志清理策略有 delete 和 compact 两种。  </p><p>1）delete 日志删除：将过期数据删除<br> log.cleanup.policy &#x3D; delete 所有数据启用删除策略   </p><p>（1）基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳。  </p><p>（2）基于大小：默认关闭。超过设置的所有日志总大小，删除最早的 segment。log.retention.bytes，默认等于-1，表示无穷大。  </p><p>2）compact 日志压缩  </p><p>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。    </p><p>log.cleanup.policy &#x3D; compact 所有数据启用压缩策略</p><p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。  </p><p>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。  </p><p><img src="/2023/08/03/kafka/11.png" alt="compact日志压缩">  </p><h2 id="高效读写数据"><a href="#高效读写数据" class="headerlink" title="高效读写数据"></a>高效读写数据</h2><p>1）Kafka 本身是分布式集群，可以采用分区技术，并行度高  </p><p>2）读数据采用稀疏索引，可以快速定位要消费的数据  </p><p>3）顺序写磁盘  </p><p>   顺序写之所以快，是因为其省去了大量磁头寻址的时间。  </p><p>4）页缓存 + 零拷贝技术     </p><h2 id="Kafka-消费者"><a href="#Kafka-消费者" class="headerlink" title="Kafka 消费者"></a>Kafka 消费者</h2><h3 id="Kafka-消费方式"><a href="#Kafka-消费方式" class="headerlink" title="Kafka 消费方式"></a>Kafka 消费方式</h3><p>pull（拉）模 式：</p><p>consumer采用从broker中主动拉取数据。Kafka采用这种方式。</p><p>push（推）模式：  </p><p>Kafka没有采用这种方式，因为由broker决定消息发送速率，很难适应所有消费者的消费速率。    </p><p>pull模式不足之处是，如 果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据。  </p><p>Kafka 消费者总体工作流程  </p><p><img src="/2023/08/03/kafka/12.png" alt="kafka消费总体工作流程">    </p><p>消费者组原理    </p><p>Consumer Group（CG）：消费者组，由多个consumer组成。形成一个消费者组的条件，是所有消费者的groupid相同。  </p><p>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费。  </p><p>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。    </p><p><img src="/2023/08/03/kafka/13.png" alt="消费者组原理">  </p><h2 id="生产经验——分区的分配以及再平衡"><a href="#生产经验——分区的分配以及再平衡" class="headerlink" title="生产经验——分区的分配以及再平衡"></a>生产经验——分区的分配以及再平衡</h2><p>Kafka有四种主流的分区分配策略： Range、RoundRobin、Sticky、CooperativeSticky。可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略  </p><h3 id="Range-以及再平衡"><a href="#Range-以及再平衡" class="headerlink" title="Range 以及再平衡"></a>Range 以及再平衡</h3><p>Range 是对每个 topic 而言的。首先对同一个 topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。  </p><p>容易产生数据倾斜！    </p><p>Kafka 默认的分区分配策略就是 Range + CooperativeSticky   </p><h3 id="RoundRobin-以及再平衡"><a href="#RoundRobin-以及再平衡" class="headerlink" title="RoundRobin 以及再平衡"></a>RoundRobin 以及再平衡</h3><p>RoundRobin 轮询分区策略，是把所有的 partition 和所有的consumer 都列出来，然后按照 hashcode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。  </p><h3 id="Sticky-以及再平衡"><a href="#Sticky-以及再平衡" class="headerlink" title="Sticky 以及再平衡"></a>Sticky 以及再平衡</h3><p>粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，<br>考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。<br>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。   </p><h2 id="offset-位移"><a href="#offset-位移" class="headerlink" title="offset 位移"></a>offset 位移</h2><p>从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets</p><p>__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是group.id+topic+分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact，也就是每个 group.id+topic+分区号就保留最新数据。  </p><h3 id="自动提交-offset"><a href="#自动提交-offset" class="headerlink" title="自动提交 offset"></a>自动提交 offset</h3><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。  </p><p>自动提交offset的相关参数：  </p><p>enable.auto.commit：是否开启自动提交offset功能，默认是true  </p><p>auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s  </p><h3 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h3><p>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次提交的一批数据最高的偏移量提交；不同点是，同步提交阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故有可能提交失败。  </p><p>commitSync（同步提交）：必须等待offset提交完毕，再去消费下一批数据。  </p><p>commitAsync（异步提交） ：发送完提交offset请求后，就开始消费下一批数据了。  </p><h3 id="指定-Offset-消费"><a href="#指定-Offset-消费" class="headerlink" title="指定 Offset 消费"></a>指定 Offset 消费</h3><pre><code>auto.offset.reset = earliest | latest | none 默认是 latest</code></pre><p>（1）earliest：自动将偏移量重置为最早的偏移量，–from-beginning。  </p><p>（2）latest（默认值）：自动将偏移量重置为最新偏移量。  </p><p>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。  </p><p>（4）任意指定 offset 位移开始消费    </p><h2 id="漏消费和重复消费"><a href="#漏消费和重复消费" class="headerlink" title="漏消费和重复消费"></a>漏消费和重复消费</h2><p>重复消费：已经消费了数据，但是 offset 没提交。 </p><p>漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。  </p><h3 id="生产经验——消费者事务"><a href="#生产经验——消费者事务" class="headerlink" title="生产经验——消费者事务"></a>生产经验——消费者事务</h3><p>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交<br>offset过程做原子绑定。  </p><h3 id="生产经验——数据积压（消费者如何提高吞吐量）"><a href="#生产经验——数据积压（消费者如何提高吞吐量）" class="headerlink" title="生产经验——数据积压（消费者如何提高吞吐量）"></a>生产经验——数据积压（消费者如何提高吞吐量）</h3><p><img src="/2023/08/03/kafka/14.png" alt="数据积压">    </p><h2 id="Kafka-Kraft-模式"><a href="#Kafka-Kraft-模式" class="headerlink" title="Kafka-Kraft 模式"></a>Kafka-Kraft 模式</h2><p><img src="/2023/08/03/kafka/15.png" alt="kafka-kraft架构">  </p><p>右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。  </p><p>这样做的好处有以下几个：  </p><p>Kafka 不再依赖外部框架，而是能够独立运行；    </p><p>controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升；  </p><p>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制；  </p><p>controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强  </p><p>controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。  </p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E9%BB%84%E6%B2%99%E7%99%BE%E6%88%98%E7%A9%BF%E9%87%91%E7%94%B2%EF%BC%8C%E4%B8%8D%E7%A0%B4%E6%A5%BC%E5%85%B0%E7%BB%88%E4%B8%8D%E8%BF%98/">黄沙百战穿金甲，不破楼兰终不还</category>
      
      
      <comments>http://example.com/2023/08/03/kafka/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
