<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>第五门徒</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>初级以内我无敌，中级以上我一换一</description>
    <pubDate>Wed, 09 Aug 2023 04:18:18 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Java数据结构和算法</title>
      <link>http://example.com/2023/08/07/Java_datastrcut/</link>
      <guid>http://example.com/2023/08/07/Java_datastrcut/</guid>
      <pubDate>Mon, 07 Aug 2023 03:10:06 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;#君子藏器于身，待时而动   &lt;/p&gt;
&lt;p&gt;##线性结构和非线性结构    &lt;/p&gt;
&lt;p&gt;###线性结构  &lt;/p&gt;
&lt;p&gt;线性结构作为最常用的数据结构，其特点是数据元素之间存在一对一的线性关系  &lt;/p&gt;
&lt;p&gt;线性结构有两种不同的存储结构，即顺序存储结构和链式存储结</description>
        
      
      
      
      <content:encoded><![CDATA[<p>#君子藏器于身，待时而动   </p><p>##线性结构和非线性结构    </p><p>###线性结构  </p><p>线性结构作为最常用的数据结构，其特点是数据元素之间存在一对一的线性关系  </p><p>线性结构有两种不同的存储结构，即顺序存储结构和链式存储结构。顺序存储的线性表称为顺序表，顺序表中的存储元素是连续的  </p><p>链式存储的线性表称为链表，链表中的存储元素不一定是连续的，元素节点中存放数据元素以及相邻元素的地址信息  </p><p>线性结构常见的有：数组、队列、链表和栈，后面我们会详细讲解   </p><p>非线性结构包括：二维数组，多维数组，广义表，树结构，图结构  </p><p>#稀疏数组  </p><p>当一个数组中大部分元素为０，或者为同一个值的数组时，可以使用稀疏数组来保存该数组。  </p><p>稀疏数组的处理方法是:  </p><p>记录数组一共有几行几列，有多少个不同的值  </p><p>把具有不同值的元素的行列及值记录在一个小规模的数组中，从而缩小程序的规模  </p><p><img src="/2023/08/07/Java_datastrcut/1.png" alt="稀疏数组">   </p><p><img src="/2023/08/07/Java_datastrcut/2.png" alt="稀疏数组转换思路">    </p><p>二维表转稀疏数组代码实现：    </p><pre><code>package com.zyy;public class SparseArray &#123;    public static void main(String[] args)&#123;        //创建一个原始的二维数组 11*11        // 0: 表示没有棋子，1表示黑子 2表示蓝子        int chessArr1[][] = new int[11][11];        chessArr1[1][2] = 1; //第二行第三列 有一颗黑子        chessArr1[2][3] = 2; //第三行第四列 有一颗蓝子        chessArr1[4][5] = 2; //第五行第六列 有一颗蓝子        //输出原始的二维数组        System.out.println(&quot;原始的二维数组~~&quot;);        //从二维数组中拿出每一行数据，返回为一维数组int[] row        for(int[] row:chessArr1) &#123;            //从拿到的每一行数据中拿到每一个值            for (int data:row)&#123;                System.out.printf(&quot;%d\t&quot;,data);            &#125;            System.out.println();        &#125;        // 将二维数组转稀疏数组的思想        //1.先遍历二维数组，得到非0数据的个数        int sum = 0;        for (int i = 0; i &lt; 11; i++)&#123;            for (int j = 0;j &lt; 11; j++)&#123;                if (chessArr1[i][j] != 0 )&#123;                    sum++;                &#125;            &#125;        &#125;        //2.创建对应的稀疏数组        //由统计出来的非0数个数+1，构成稀疏数组的行数        //稀疏数组的列数固定为3，记录行坐标，列坐标，值        int sparseArr[][] = new int[sum+1][3];        //给稀疏数组赋值        sparseArr[0][0] = 11;        sparseArr[0][1] = 11;        sparseArr[0][2] = sum;        //遍历二维数组，将非0的值存放到sparseArr中        int count = 0;  //count用于记录是第几个非0数据        for(int i = 0;i &lt; 11; i++)&#123;            for (int j = 0 ;j &lt; 11; j++)&#123;                if(chessArr1[i][j] != 0 )&#123;                    count++;                    //记录第count个非0数据的行i                    sparseArr[count][0] = i;                    //记录第count个非0数据的列j                    sparseArr[count][1] = j;                    //记录第count个非0数据的值                    sparseArr[count][2] = chessArr1[i][j];                &#125;            &#125;        &#125;        //输出稀疏数组的形式        System.out.println();        System.out.println(&quot;得到稀疏数组为~~~&quot;);        //二维数组结构为[[数组1],[数组2],[数组3]]        //所以sparseArr.length实际上是在统计外层一维数组的长度        for(int i = 0; i &lt; sparseArr.length;i++)&#123;            System.out.printf(&quot;%d\t%d\t%d\t\n&quot;,sparseArr[i][0],sparseArr[i][1],sparseArr[i][2]);        &#125;        System.out.println();        //将稀疏数组 -》 恢复成 原始的二维数组        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组，比如上面的chessArr2 = int[11][11]        //2.在读取稀疏数组后几行的数据，并赋值给原始的二维数组，即可        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组        int chessArr2[][] = new int[sparseArr[0][0]][sparseArr[0][1]];        //2.在读取稀疏数组后几行的数据（从第二行开始），并赋值给原始的二维数组即可        for (int i = 1; i &lt; sparseArr.length; i++)&#123;            chessArr2[sparseArr[i][0]][sparseArr[i][1]] = sparseArr[i][2];        &#125;        //输出恢复后的二维数组        System.out.println();        System.out.println(&quot;恢复后的二维数组&quot;);        for (int[] row:chessArr2)&#123;            for(int data:row)&#123;                System.out.printf(&quot;%d\t&quot;,data);            &#125;            //每行数据打印完之后，执行换行            System.out.println();        &#125;    &#125;&#125;原始的二维数组~~0000000000000100000000000200000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000得到稀疏数组为~~~11113121232452恢复后的二维数组0000000000000100000000000200000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000</code></pre><p>#队列  </p><p>队列是一个有序列表，可以用数组或者链表来实现。遵循先进先出原则（FIFO）</p><p><img src="/2023/08/07/Java_datastrcut/3.png" alt="数组模拟队列">  </p><p>##数组模拟队列  </p><pre><code>package com.zyy;import java.util.Scanner;public class ArrayQueueDemo &#123;public static void main(String[] args) &#123;    // 测试一把    // 创建一个队列    ArrayQueue queue = new ArrayQueue(5);    char key ;//接收用户输入    Scanner scanner = new Scanner(System.in);    boolean loop = true;    //输出一个菜单    while(loop)&#123;        System.out.println(&quot;s(show):显示队列&quot;);        System.out.println(&quot;e(exit):退出程序&quot;);        System.out.println(&quot;a(add):添加数据到队列&quot;);        System.out.println(&quot;g(get):从队列取出数据&quot;);        System.out.println(&quot;h(head):查看队列头的数据&quot;);        key = scanner.next().charAt(0);//接收一个字符        switch(key)&#123;            case &#39;s&#39;:                queue.showQueue();                break;            case &#39;a&#39;:                System.out.println(&quot;输入一个数&quot;);                int value = scanner.nextInt();                queue.addQueue(value);                break;            case &#39;g&#39;://取出数据                try&#123;                    int res = queue.getQueue();                    System.out.printf(&quot;取出的数据是%d\n&quot;,res);                &#125;catch (Exception e)&#123;                    //TODO:handle exception                    System.out.println(e.getMessage());                &#125;                break;            case &#39;h&#39;://查看队列头的数据                try&#123;                    int res = queue.headQueue();                    System.out.printf(&quot;队列头的数据是%d\n&quot;,res);                &#125;catch(Exception e)&#123;                    //TODO:handle exception                    System.out.println(e.getMessage());                &#125;                break;            case &#39;e&#39;://退出                scanner.close();                loop = false;                break;            default:                break;        &#125;    &#125;    System.out.println(&quot;程序退出~~&quot;);&#125;    //使用数组模拟队列-编写一个ArrayQueue类static class ArrayQueue&#123;        private int maxSize;// 表示数组的最大容量        private int front;//队列头        private int rear;//队列尾        private int[] arr;//该数据用于存放数据，模拟队列        //创建队列的构造器        public ArrayQueue(int arrMaxSize)&#123;            maxSize = arrMaxSize;            arr = new int[maxSize];            front = -1;//指向队列头部，分析出front是指向队列头的前一个位置            rear = -1;//指向队列尾，指向队列尾的数据（即就是队列最后一个数据）        &#125;        //判断队列是否满        public boolean isFull()&#123;            return rear == maxSize - 1;        &#125;        //判断队列是否为空        public boolean isEmpty()&#123;            return rear == front;        &#125;        //添加数据到队列        public void addQueue(int n)&#123;            //判断队列是否满            if(isFull())&#123;                System.out.println(&quot;队列满，不能加入数据~&quot;);                return;            &#125;            rear++;//让rear后移            arr[rear] = n;        &#125;        //获取队列的数据，出队列        public int getQueue()&#123;            //判断队列是否为空            if(isEmpty())&#123;                //通过抛出异常                throw new RuntimeException(&quot;队列空，不能取数据&quot;);            &#125;            front++;//front后移            return arr[front];        &#125;        //显示队列的所有数据        public void showQueue()&#123;            //遍历            if(isEmpty())&#123;                System.out.println(&quot;队列空的，没有数据~~&quot;);                return;            &#125;            for(int i=0;i&lt;arr.length;i++)&#123;                System.out.printf(&quot;arr[%d]=%d\n&quot;,i,arr[i]);            &#125;        &#125;        //显示队列的头数据，注意不是取出数据        public int headQueue()&#123;            //判断            if(isEmpty())&#123;                throw new RuntimeException(&quot;队列空的，没有数据~~&quot;);            &#125;            return arr[front + 1];        &#125;    &#125;&#125;</code></pre><p>###使用数组模拟队列存在的问题：  </p><p>数组只能使用一次，因为front和rear指针无法再回头指向已经走过的数组位置  </p><p>###优化方案：</p><p>通过取模运算，让front和rear指针能循环指向已经走过的数组位置，让数组复用  </p><p>分析说明:  </p><p>1):尾索引的下一个为头索引时表示队列满，即将队列容量空出一个作为约定，这个在做判断队列满的时候需要注意（rear+1）%maxSize &#x3D;&#x3D; front 满</p><p>2):rear &#x3D;&#x3D; front(空)</p><p>3):与数组模拟队列不同，数组模拟环形队列时，front指向队列的第一个元素，front的初始值为0 ，rear指向队列的最后一个元素的最后一个位置，因为希望空出一个空间作为约定，rear的初始值为0</p><p>4):队列中的有效数据个数计算方法:  </p><p>(rear+maxSize-front)%maxSize</p><p><img src="/2023/08/07/Java_datastrcut/4.png" alt="循环队列相关判断条件"></p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%90%9B%E5%AD%90%E8%97%8F%E5%99%A8%E4%BA%8E%E8%BA%AB%EF%BC%8C%E5%BE%85%E6%97%B6%E8%80%8C%E5%8A%A8/">君子藏器于身，待时而动</category>
      
      
      <comments>http://example.com/2023/08/07/Java_datastrcut/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hadoop学习笔记</title>
      <link>http://example.com/2023/08/06/hadoop/</link>
      <guid>http://example.com/2023/08/06/hadoop/</guid>
      <pubDate>Sun, 06 Aug 2023 02:45:20 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;#博观而约取，厚积而薄发     &lt;/p&gt;
&lt;p&gt;相关学习文档&lt;br&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA&quot;&gt;https://pan.baidu.com/s/1WLK6GP99XAgJcX3F</description>
        
      
      
      
      <content:encoded><![CDATA[<p>#博观而约取，厚积而薄发     </p><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA">https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA</a><br>提取码：mvcs   </p><p>#免密登录原理   </p><p><img src="/2023/08/06/hadoop/4.png" alt="免密登录原理">    </p><p>#HDFS架构概述  </p><p><img src="/2023/08/06/hadoop/1.png" alt="HDFS架构概述">  </p><p>HDFS适合一次写入，多次读出的场景，且不支持文件的修改  </p><p>HDFS的缺点：仅支持数据append，不支持文件的随机修改  </p><p><img src="/2023/08/06/hadoop/5.png" alt="HDFS组成架构">  </p><p><img src="/2023/08/06/hadoop/6.png" alt="HDFS组成架构">   </p><p>HDFS文件块大小：在Hadoop2.x版本中是128M  </p><p>寻址时间为传输时间的1%时，是最佳状态  </p><p>HDFS块的大小设置主要取决于磁盘传输速率  </p><p>###HDFS写数据流程  </p><p><img src="/2023/08/06/hadoop/7.png" alt="HDFS写数据流程">    </p><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据    </p><p>###HDFS副本节点选择  </p><p><img src="/2023/08/06/hadoop/8.png" alt="HDFS副本节点选择">       </p><p>###HDFS读数据流程  </p><p><img src="/2023/08/06/hadoop/9.png" alt="HDFS读数据流程">    </p><p>#NN和2NN工作机制  </p><p>NameNode中的元数据是存储在内存中，在内存上维护一个Edits文件，磁盘上维护一个FsImage文件，每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据  </p><p>需要定期进行FsImage和Edits的合并，由SecondaryNamenode完成，专门负责FsImage和Edits的合并  </p><p>##NameNode工作机制  </p><p><img src="/2023/08/06/hadoop/10.png" alt="NameNode工作机制">    </p><p>##NameNode故障处理  </p><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录    </p><p>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中  </p><p><img src="/2023/08/06/hadoop/11.png" alt="集群安全模式">    </p><p>（1）bin&#x2F;hdfs dfsadmin -safemode get（功能描述：查看安全模式状态）  </p><p>（2）bin&#x2F;hdfs dfsadmin -safemode enter  （功能描述：进入安全模式状态）  </p><p>（3）bin&#x2F;hdfs dfsadmin -safemode leave（功能描述：离开安全模式状态）  </p><p>（4）bin&#x2F;hdfs dfsadmin -safemode wait（功能描述：等待安全模式状态）  </p><p>##DataNode工作机制   </p><p><img src="/2023/08/06/hadoop/12.png" alt="DataNode工作机制">   </p><p>###DataNode如何保证数据完整性？    </p><p>1）当DataNode读取Block的时候，它会计算CheckSum。  </p><p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。  </p><p>3）Client读取其他DataNode上的Block。  </p><p>4）DataNode在其文件创建后周期验证CheckSum    </p><p><img src="/2023/08/06/hadoop/13.png" alt="DataNode数据完整性">    </p><p>HDFS中默认DataNode掉线的超时时长为10分钟+30秒    </p><p>###DataNode配置多目录  </p><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本   </p><p>hdfs-site.xml  </p><pre><code>&lt;property&gt;    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;    &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt;</code></pre><p>##HDFS-HA故障转移机制  </p><p><img src="/2023/08/06/hadoop/14.png" alt="HDFS-HA故障转移机制">   </p><p>#YARN架构概述  </p><p><img src="/2023/08/06/hadoop/2.png" alt="YARN架构概述">    </p><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序  </p><p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成</p><p>###Yarn工作机制  </p><p><img src="/2023/08/06/hadoop/25.png" alt="Yarn工作机制">    </p><p>工作机制详解  </p><pre><code>（1）MR程序提交到客户端所在的节点。  （2）YarnRunner向ResourceManager申请一个Application。  （3）RM将该应用程序的资源路径返回给YarnRunner。（4）该程序将运行所需资源提交到HDFS上。（5）程序资源提交完毕后，申请运行mrAppMaster。（6）RM将用户的请求初始化成一个Task。（7）其中一个NodeManager领取到Task任务。（8）该NodeManager创建容器Container，并产生MRAppmaster。（9）Container从HDFS上拷贝资源到本地。（10）MRAppmaster向RM 申请运行MapTask资源。（11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。（12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。  （14）ReduceTask向MapTask获取相应分区的数据。（15）程序运行完毕后，MR会向RM申请注销自己</code></pre><p>总结：程序提交之后，找RM申请Application,告知运行程序需要的资源。RM生成一个资源分配Task任务放进yarn队列。这个Task会随机分配给Nodemanager，NodeManager领取任务后会根据资源要求创建Container容器，开始运行程序，运行完毕后向RM报告，注销资源占用和Task任务。  </p><p>###资源调度器  </p><p>1.先进先出调度器（FIFO）  </p><p>2.容量调度器（Capacity Scheduler）  </p><p>3．公平调度器（Fair Scheduler）  </p><p>#MapReduce架构概述  </p><p><img src="/2023/08/06/hadoop/3.png" alt="MapReduce架构概述">     </p><p>###MapReduce核心思想  </p><p><img src="/2023/08/06/hadoop/15.png" alt="MapReduce核心思想">   </p><p>MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行  </p><p>###MapTask并行度决定机制  </p><p>数据块：Block是HDFS物理上把数据分成一块一块。  </p><p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储  </p><p><img src="/2023/08/06/hadoop/16.png" alt="MapTask并行度决定机制">  </p><p>##MapReduce工作流程  </p><p><img src="/2023/08/06/hadoop/17.png" alt="MapReduce工作流程">  </p><p><img src="/2023/08/06/hadoop/18.png" alt="MapReduce工作流程">  </p><p>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快,默认100M   </p><p>##Shuffle机制  </p><p><img src="/2023/08/06/hadoop/19.png" alt="Shuffle机制">    </p><p>##MapTask工作机制  </p><p><img src="/2023/08/06/hadoop/20.png" alt="MapTask工作机制">    </p><p>##ReduceTask工作机制</p><p><img src="/2023/08/06/hadoop/21.png" alt="ReduceTask工作机制">  </p><p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置  </p><p>#Hadoop文件压缩    </p><p><img src="/2023/08/06/hadoop/22.png" alt="Hadoop文件压缩">  </p><p>###Hadoop文件压缩性能比较  </p><p><img src="/2023/08/06/hadoop/23.png" alt="Hadoop文件压缩性能比较">  </p><p>Gzip压缩：每个文件压缩后都在130M以内的（一个块大小内），都可以考虑Gzip压缩格式  ，不支持Split  </p><p>Bzip压缩：支持Split，压缩率很高，但压缩&#x2F;解压缩 速度慢  </p><p>LZO压缩：支持Split，合理的压缩率，是Hadoop中最流行的压缩格式    </p><p>Snappy压缩：不支持Split，压缩率比Gzip低，但Hadoop本身不支持，需要安装  </p><p>###压缩位置选择  </p><p><img src="/2023/08/06/hadoop/24.png" alt="压缩位置选择">   </p><p>压缩可以在MapReduce作用的任意阶段启用    </p><p>速度是最优先考虑的因素，而不是压缩率  </p><p>#ZooKeeper    </p><p>是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。   </p><p>ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功 能稳定的系统提供给用户    </p><p>##Zookeeper工作机制  </p><p><img src="/2023/08/06/hadoop/26.png" alt="Zookeeper工作机制">  </p><p>###Zookeeper特点    </p><p><img src="/2023/08/06/hadoop/27.png" alt="Zookeeper特点">    </p><p>###Zookeeper的数据结构  </p><p><img src="/2023/08/06/hadoop/28.png" alt="Zookeeper的数据结构">    </p><p>###软负载均衡  </p><p>在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求  </p><p>##选举机制（面试重点）   </p><p><img src="/2023/08/06/hadoop/29.png" alt="Zookeeper选举机制-第一次启动">    </p><p><img src="/2023/08/06/hadoop/30.png" alt="Zookeeper选举机制-非第一次启动">   </p><p>###客户端命令行操作 </p><p>1）启动客户端    </p><pre><code>[atguigu@hadoop102 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop102:2181</code></pre><p>2）显示所有操作命令  </p><pre><code>[zk: hadoop102:2181(CONNECTED) 1] help  </code></pre><p>####znode 节点数据信息  </p><p>1）查看当前znode中所包含的内容   </p><pre><code>[zk: hadoop102:2181(CONNECTED) 0] ls /  </code></pre><p>2）查看当前节点详细数据  </p><pre><code>[zk: hadoop102:2181(CONNECTED) 5] ls -s /[zookeeper]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x0cversion = -1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1    </code></pre><p>（1）czxid：创建节点的事务 zxid  </p><p>每次修改 ZooKeeper 状态都会产生一个 ZooKeeper 事务 ID。事务ID是 ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1在zxid2之前发生。  </p><p>（2）ctime：znode 被创建的毫秒数（从 1970 年开始）  </p><p>（3）mzxid：znode 最后更新的事务 zxid  </p><p>（4）mtime：znode 最后修改的毫秒数（从 1970 年开始）  </p><p>（5）pZxid：znode 最后更新的子节点 zxid    </p><p>（6）cversion：znode 子节点变化号，znode子节点修改次数  </p><p>（7）dataversion：znode 数据变化号  </p><p>（8）aclVersion：znode 访问控制列表的变化号  </p><p>（9）ephemeralOwner：如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。  </p><p>（10）dataLength：znode 的数据长度  </p><p>（11）numChildren：znode 子节点数量  </p><p>####节点类型（持久&#x2F;短暂&#x2F;有序号&#x2F;无序号）  </p><p><img src="/2023/08/06/hadoop/31.png" alt="Zookeeper节点类型">   </p><p>1）分别创建2个普通节点（永久节点 + 不带序号）   </p><pre><code>[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;diaochan&quot; 注意：创建节点时，要赋值  </code></pre><p>2）获得节点的值  </p><pre><code>[zk: localhost:2181(CONNECTED) 5] get -s /sanguo  </code></pre><p>3）创建带序号的节点（永久节点 + 带序号）  </p><p>（1）先创建一个普通的根节点&#x2F;sanguo&#x2F;weiguo    </p><pre><code>[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot;</code></pre><p>（2）创建带序号的节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/zhangliao &quot;zhangliao&quot;  </code></pre><p>如果原来没有序号节点，序号从 0 开始依次递增。如果原节点下已有 2 个节点，则再排序时从 2 开始，以此类推。    </p><p>4）创建短暂节点（短暂节点 + 不带序号 or 带序号）   </p><p>（1）创建短暂的不带序号的节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot;</code></pre><p>（2）创建短暂的带序号的节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 2] create -e -s /sanguo/wuguo &quot;zhouyu&quot;    </code></pre><p>（3）在当前客户端是能查看到的  </p><pre><code>[zk: localhost:2181(CONNECTED) 3] ls /sanguo   </code></pre><p>（4）修改节点数据值   </p><pre><code>[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo &quot;simayi&quot;  </code></pre><p>##监听器原理      </p><p>监听机制保证 ZooKeeper 保存的任何的数据的任何改变都能快速的响应到监听了该节点的应用程序。  </p><p><img src="/2023/08/06/hadoop/32.png" alt="监听器原理">      </p><p>####节点删除与查看    </p><p>1）删除节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin</code></pre><p>2）递归删除节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 15] deleteall /sanguo/shuguo</code></pre><p>3）查看节点状态  </p><pre><code>[zk: localhost:2181(CONNECTED) 17] stat /sanguo</code></pre><p>####客户端向服务端写数据流程  </p><p><img src="/2023/08/06/hadoop/33.png" alt="客户端向服务端写数据流程">  </p><p>##ZooKeeper 分布式锁    </p><p>“进程1” 在使用该资源的时候，会先去获得锁，保持独占，这样其他进程就无法访问该资源,用完该资源以后就将锁释放掉,保证了分布式系统中多个进程能够有序的访问该临界资源。  </p><p><img src="/2023/08/06/hadoop/34.png" alt="Zookeeper分布式锁">      </p><p>##Zookeeper企业面试真题（面试重点）总结    </p><p>###选举机制  </p><p>半数机制，超过半数的投票通过，即通过。  </p><p>（1）第一次启动选举规则：  </p><pre><code>投票过半数时，服务器 id 大的胜出  </code></pre><p>（2）第二次启动选举规则： </p><pre><code>①EPOCH 大的直接胜出  ②EPOCH 相同，事务 id 大的胜出  ③事务 id 相同，服务器 id 大的胜出  </code></pre><p>生产集群安装多少 zk 合适？  </p><pre><code>安装奇数台  </code></pre><p>生产经验：  </p><pre><code>10 台服务器：3 台 zk  20 台服务器：5 台 zk  100 台服务器：11 台 zk  200 台服务器：11 台 zk    </code></pre><p>服务器台数多：好处，提高可靠性；坏处：提高通信延时  </p><p>常用命令  </p><pre><code>ls、get、create、delete</code></pre>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%8D%9A%E8%A7%82%E8%80%8C%E7%BA%A6%E5%8F%96%EF%BC%8C%E5%8E%9A%E7%A7%AF%E8%80%8C%E8%96%84%E5%8F%91/">博观而约取，厚积而薄发</category>
      
      
      <comments>http://example.com/2023/08/06/hadoop/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>JavaSE</title>
      <link>http://example.com/2023/08/04/JavaSE/</link>
      <guid>http://example.com/2023/08/04/JavaSE/</guid>
      <pubDate>Fri, 04 Aug 2023 09:50:53 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;天行健，君子以自强不息~  &lt;/p&gt;
&lt;p&gt;##Java 核心机制-Java 虚拟机 [JVM java virtual machine]  &lt;/p&gt;
&lt;p&gt;JVM 是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器，包含在JDK中  </description>
        
      
      
      
      <content:encoded><![CDATA[<p>天行健，君子以自强不息~  </p><p>##Java 核心机制-Java 虚拟机 [JVM java virtual machine]  </p><p>JVM 是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器，包含在JDK中  </p><p>对于不同的平台，有不同的虚拟机    </p><p>##什么是 JDK，JRE  </p><p>###JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等]     </p><p>JDK 是提供给 Java 开发人员使用的，其中包含了 java 的开发工具，也包括了 JRE。所以安装了 JDK，就不用在单独安装JRE了  </p><p>###JRE &#x3D; JVM + Java 的核心类库[类]  </p><p>包括 Java 虚拟机(JVM Java Virtual Machine)和 Java 程序所需的核心类库等，如果想要运行一个开发好的 Java 程序，计算机中只需要安装 JRE 即可    </p><p>####Java 转义字符    </p><p>\t ：一个制表位，实现对齐的功能  </p><p>\n ：换行符  </p><p>\ ：一个\  </p><p>&quot; :一个”  </p><p>&#39; ：一个’ \r :一个回车 System.out.println(“韩顺平教育\r 北京”);  </p><p>####Java 中的注释类型  </p><ol><li><p>单行注释 &#x2F;&#x2F;  </p></li><li><p>多行注释 &#x2F;* *&#x2F;    </p><p> 多行注释里面不允许有多行注释嵌套</p></li><li><p>文档注释 &#x2F;** *&#x2F;</p></li></ol><p><strong>文档注释：</strong>  </p><p>&#x2F;**  </p><ul><li>@author 韩顺平  </li><li>@version 1.0<br>*&#x2F;</li></ul><p>####Java 代码规范    </p><p><img src="/2023/08/04/JavaSE/1.png" alt="Java代码规范">    </p><p>###DOS 介绍 </p><p>Dos： Disk Operating System 磁盘操作系统  </p><p>####常用的 dos 命令  </p><ol><li><p>查看当前目录是有什么内容 dir  </p><p> dir dir d:\abc2\test200</p></li><li><p>切换到其他盘下：盘符号 cd : change directory</p></li></ol><p>案例演示：切换到 c 盘 cd &#x2F;D c:  </p><ol start="3"><li>切换到当前盘的其他目录下 (使用相对路径和绝对路径演示), ..\表示上一级目录</li></ol><p>案例演示： cd d:\abc2\test200 cd ....\abc2\test200  </p><ol start="4"><li>切换到上一级：</li></ol><p>案例演示： cd .. 5) 切换到根目录：cd \  </p><p>案例演示：cd \  </p><ol start="6"><li><p>查看指定的目录下所有的子级目录 tree  </p></li><li><p>清屏 cls [苍老师]  </p></li><li><p>退出 DOS exit</p></li></ol><p>##变量  </p><p><strong>变量三要素：变量名+值+数据类型</strong>  </p><p>变量相当于内存中一个数据存储空间的表示，你可以把变量看做是一个房间的门牌号，通过门牌号我们可以找到房间，而通过变量名可以访问到变量(值)    </p><ol><li><p>声明变量  </p><p> int a;  </p></li><li><p>赋值  </p><p> a &#x3D; 60; &#x2F;&#x2F;应该这么说: 把 60 赋给 a</p></li></ol><p>###数据类型</p><p>每一种数据都定义了明确的数据类型，在内存中分配了不同大小的内存空间(字节)。  </p><p><img src="/2023/08/04/JavaSE/2.png" alt="数据类型">    </p><p>####整型的类型    </p><p><img src="/2023/08/04/JavaSE/3.png" alt="数据类型">     </p><p>&#x2F;&#x2F;Java 的整型常量（具体值）默认为 int 型，声明 long 型常量须后加‘l’或‘L’  </p><pre><code>int n1 = 1;//4 个字节long n3 = 1L;//长整型  </code></pre><p>####浮点类型</p><p><img src="/2023/08/04/JavaSE/4.png" alt="数据类型">    </p><p>&#x2F;&#x2F;Java 的浮点型常量(具体值)默认为 double 型，声明 float 型常量，须后加‘f’或‘F’  </p><pre><code>float num2 = 1.1F;    double num3 = 1.1; double num4 = 1.1f; </code></pre><p>十进制数形式：如：5.12 512.0f .512 (必须有小数点）   </p><p>Java类的组织形式  </p><p><img src="/2023/08/04/JavaSE/5.png" alt="Java类的组织形式">   </p><p>####字符编码  </p><p><img src="/2023/08/04/JavaSE/6.png" alt="字符编码">  </p><p>####基本数据类型转换</p><p>#####自动类型转换    </p><p><img src="/2023/08/04/JavaSE/7.png" alt="自动类型转换">    </p><p>#####强制类型转换   </p><p>自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符 ( )，但可能造成精度降低或溢出,格外要注意  </p><p><img src="/2023/08/04/JavaSE/8.png" alt="强制类型转换">  </p><p>##运算符  </p><p>###逻辑运算符  </p><p>说明逻辑运算规则：  </p><ol><li><p>a&amp;b : &amp; 叫逻辑与：规则：当 a 和 b 同时为 true ,则结果为 true, 否则为 false  </p></li><li><p>a&amp;&amp;b : &amp;&amp; 叫短路与：规则：当 a 和 b 同时为 true ,则结果为 true,否则为 false  </p></li><li><p>a|b : | 叫逻辑或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p></li><li><p>a||b : || 叫短路或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p></li><li><p>!a : 叫取反，或者非运算。当 a 为 true, 则结果为 false, 当 a 为 false 是，结果为 true  </p></li><li><p>a^b: 叫逻辑异或，当 a 和 b 不同时，则结果为 true, 否则为 false</p></li></ol><p><strong>&amp;&amp; 和 &amp; 使用区别</strong>  </p><ol><li><p>&amp;&amp;短路与：如果第一个条件为 false，则第二个条件不会判断，最终结果为 false，效率高  </p></li><li><p>&amp; 逻辑与：不管第一个条件是否为 false，第二个条件都要判断，效率低</p></li></ol><p><strong>|| 和 | 使用区别</strong>  </p><ol><li><p>||短路或：如果第一个条件为 true，则第二个条件不会判断，最终结果为 true，效率高  </p></li><li><p>| 逻辑或：不管第一个条件是否为 true，第二个条件都要判断，效率低</p></li></ol><p>###三元运算符    </p><p>条件表达式 ? 表达式 1: 表达式 2;  </p><p>运算规则：  </p><p>1.如果条件表达式为 true，运算后的结果是表达式 1；  </p><p>2.如果条件表达式为 false，运算后的结果是表达式 2；  </p><p>口诀: [一灯大师：一真大师]  </p><p>#####接收控制台输入Scanner</p><pre><code>import java.util.Scanner;    Scanner myScanner = new Scanner(System.in);  </code></pre><p>#####原码、反码、补码(重点 难点)    </p><p><img src="/2023/08/04/JavaSE/9.png">    </p><p>##程序控制结构    </p><p>主要有三大流程控制语句。  </p><ol><li><p>顺序控制  </p></li><li><p>分支控制    </p><ol><li>单分支 if  </li><li>双分支 if-else  </li><li>多分支 if-else if -….-else  </li><li>switch分支<br> switch(表达式){<br>         case xxx<br>                 }</li></ol></li><li><p>循环控制</p></li></ol><p><strong>for 循环控制</strong><br>    for(循环变量初始化;循环条件;循环变量迭代){<br>        循环操作(可以多条语句)<br>            }  </p><pre><code>eg:  for(int i = 1;i&lt;=10;i++)&#123;    System.out.println(&quot;Hello World ~ ！&quot;)&#125;</code></pre><p><strong>while 循环控制</strong>  </p><pre><code>循环变量初始化;  while（循环条件）&#123;    循环体（语句）；    循环变量迭代；&#125;  eg:int i = 1;while (i &lt;= 10)&#123;    System.out.println(&quot;Hello World ~ !&quot;)      i ++ &#125;  </code></pre><p><strong>do..while 循环控制</strong>  </p><pre><code>循环变量初始化;do&#123;    循环体(语句);    循环变量迭代;&#125;while(循环条件);  </code></pre><p>先执行，再判断，也就是说，一定会至少执行一次    </p><p>##跳转控制语句-break    </p><p>break 语句用于终止某个语句块的执行，一般使用在 switch 或者循环[for , while , do-while]中  </p><p>##跳转控制语句-continue</p><p>continue 语句用于结束本次循环，继续执行下一次循环  </p><p>##跳转控制语句-return    </p><p>return 使用在方法，表示跳出所在的方法  </p><p>##数组</p><p>数组可以存放多个同一类型的数据。数组也是一种数据类型，是<strong>引用类型</strong>  </p><p><img src="/2023/08/04/JavaSE/10.png" alt="数组的使用">  </p><pre><code>方式一：int a[] = new Int[5]   方式二：       int[] a;       a = new Int[10];   方式三：      int a[] = &#123;2,3,4,5,6&#125;  </code></pre><p>###数组赋值机制  </p><p><img src="/2023/08/04/JavaSE/11.png" alt="数组的使用">      </p><p>#面向对象编程(基础部分)    </p><p>类和对象的区别和联系</p><ol><li><p>类是抽象的，概念的，代表一类事物,比如人类,猫类.., 即它是数据类型  </p></li><li><p>对象是具体的，实际的，代表一个具体事物, 即 是实例   </p></li><li><p>类是对象的模板，对象是类的一个个体，对应一个实例</p></li></ol><p>##对象在内存中存在形式(重要的)必须搞清楚</p><p><img src="/2023/08/04/JavaSE/12.png" alt="对象在内存中存在形式">     </p><p>###类和对象的内存分配机制     </p><p>Java 内存的结构分析  </p><ol><li><p>栈： 一般存放基本数据类型(局部变量)  </p></li><li><p>堆： 存放对象(Cat cat , 数组等)  </p></li><li><p>方法区：常量池(常量，比如字符串)， 类加载信息</p></li></ol><p>##属性&#x2F;成员变量&#x2F;字段  </p><p>成员变量 &#x3D; 属性 &#x3D; field(字段)  </p><p>属性是类的一个组成部分，一般是基本数据类型,也可是引用类型(对象，数组)    </p><p>##创建对象  </p><p>Cat cat1 &#x3D; new Cat();  </p><p>###成员方法  </p><pre><code>class Person &#123;    String name;    int age;      //方法(成员方法)      public void speak() &#123;        System.out.println(&quot;我是一个好人&quot;);        &#125;    &#125;  </code></pre><p>###方法的调用机制原理    </p><p><img src="/2023/08/04/JavaSE/13.png">  </p><p>###成员方法的好处    </p><ol><li><p>提高代码的复用性  </p></li><li><p>可以将实现的细节封装起来，然后供其他用户来调用即可</p></li></ol><p>###成员方法的定义</p><pre><code>访问修饰符 返回数据类型 方法名（形参列表..） &#123;    //方法体语句；    return 返回值;&#125;</code></pre><p>###传参 </p><p>引用类型传递的是地址（传递也是值，但是值是地址），可以通过形参影响实参！  </p><p>###方法递归调用(非常非常重要，比较难)  </p><p>递归就是方法自己调用自己  </p><p>递归重要规则  </p><p><img src="/2023/08/04/JavaSE/14.png"></p><p>##方法重载(OverLoad)    </p><p>java 中允许同一个类中，多个同名方法的存在，但要求 形参列表不一致！  </p><p>案例：类：MyCalculator 方法：calculate  </p><ol><li>calculate(int n1, int n2) &#x2F;&#x2F;两个整数的和  </li><li>calculate(int n1, double n2) &#x2F;&#x2F;一个整数，一个 double 的和  </li><li>calculate(double n2, int n1)&#x2F;&#x2F;一个 double ,一个 Int 和  </li><li>calculate(int n1, int n2,int n3)&#x2F;&#x2F;三个 int 的和</li></ol><p><img src="/2023/08/04/JavaSE/15.png" alt="方法重载">  </p><p>###可变参数   </p><p>java 允许将同一个类中多个同名同功能但参数个数不同的方法，封装成一个方法。<br>就可以通过可变参数实现</p><p>eg:方法 sum 【可以计算 2 个数的和，3 个数的和 ， 4. 5， 。。】  </p><pre><code>//1. int... 表示接受的是可变参数，类型是 int ,即可以接收多个 int(0-多)//2. 使用可变参数时，可以当做数组来使用 即 nums 可以当做数组//3. 遍历 nums 求和即可public int sum(int... nums) &#123;//System.out.println(&quot;接收的参数个数=&quot; + nums.length);int res = 0;for(int i = 0; i &lt; nums.length; i++) &#123;res += nums[i];&#125;return res;&#125;&#125;  </code></pre><p>###变量作用域<br>变量：  </p><p>1.全局变量（属性）</p><p>2.局部变量（局部变量一般是指在成员方法中定义的变量）</p><p>全局变量和局部变量可以重名</p><p><img src="/2023/08/04/JavaSE/16.png" alt="变量作用域">  </p><p>全局变量和局部变量的区别  </p><p><img src="/2023/08/04/JavaSE/17.png" alt="全局变量和局部变量的区别">    </p><p>###构造方法&#x2F;构造器    </p><p>在创建人类的对象时，就直接指定这个对象的年龄和姓名，该怎么做? 这时就可以使用构造器  </p><p>[修饰符] 方法名(形参列表){<br>方法体;<br>}   </p><ol><li><p>构造器的修饰符可以默认， 也可以是 public protected private  </p></li><li><p>构造器没有返回值   </p></li><li><p>方法名 和类名字必须一样  </p></li><li><p>参数列表 和 成员方法一样的规则  </p></li><li><p>构造器的调用, 由系统完成</p></li></ol><p>构造方法又叫构造器(constructor)，是类的一种特殊的方法，它的主要作用是完成对新对象的初始化  </p><p><img src="/2023/08/04/JavaSE/18.png" alt="构造器使用注意事项"></p><p><img src="/2023/08/04/JavaSE/19.png" alt="构造器使用注意事项"></p><p>###this 关键字  </p><p><img src="/2023/08/04/JavaSE/20.png" alt="This关键字"></p><ol><li><p>this 关键字可以用来访问本类的属性、方法、构造器  </p></li><li><p>this 用于区分当前类的属性和局部变量  </p></li><li><p>访问成员方法的语法：this.方法名(参数列表);    </p></li><li><p>访问构造器语法：this(参数列表); 注意只能在构造器中使用(即只能在构造器中访问另外一个构造器, 必须放在第一条语句)  </p></li><li><p>this 不能在类定义的外部使用，只能在类定义的方法中使用。</p></li></ol><p>#面向对象编程(中级部分)  </p><p>IDEA 常用快捷键  </p><ol><li><p>删除当前行, 默认是 ctrl + Y 自己配置 ctrl + d  </p></li><li><p>复制当前行, 自己配置 ctrl + alt + 向下光标  </p></li><li><p>补全代码 alt + &#x2F;  </p></li><li><p>添加注释和取消注释 ctrl + &#x2F; 【第一次是添加注释，第二次是取消注释】  </p></li><li><p>导入该行需要的类 先配置 auto import , 然后使用 alt+enter 即可  </p></li><li><p>快速格式化代码 ctrl + alt + L  </p></li><li><p>快速运行程序 自己定义 alt + R  </p></li><li><p>生成构造器等 alt + insert [提高开发效率]  </p></li><li><p>查看一个类的层级关系 ctrl + H [学习继承后，非常有用]  </p></li><li><p>将光标放在一个方法上，输入 ctrl + B , 可以定位到方法 [学继承后，非常有用]  </p></li><li><p>自动的分配变量名 , 通过 在后面假 .var [老师最喜欢的]  </p></li><li><p>还有很多其它的快捷键</p></li></ol><p>##包  </p><p><img src="/2023/08/04/JavaSE/21.png" alt="包">  </p><p>包的本质 </p><p><img src="/2023/08/04/JavaSE/22.png">  </p><p>包的命名：  </p><p>com.公司名.项目名.业务模块名  </p><p>###Java常用的包  </p><p>一个包下,包含很多的类,java 中常用的包有:  </p><ol><li><p>java.lang.* &#x2F;&#x2F;lang 包是基本包，默认引入，不需要再引入.  </p></li><li><p>java.util.* &#x2F;&#x2F;util 包，系统提供的工具包, 工具类，使用 Scanner  </p></li><li><p>java.net.* &#x2F;&#x2F;网络包，网络开发  </p></li><li><p>java.awt.* &#x2F;&#x2F;是做 java 的界面开发，GUI</p></li></ol><p>引入包的语法：  </p><p>import 包;  </p><p>eg:  import java.util.*  &#x2F;&#x2F;表示将java.util包所有都引入    </p><p>我们需要使用到哪个类，就导入哪个类即可，不建议使用*导入   </p><p>##访问修饰符   </p><p>java 提供四种访问控制修饰符号，用于控制方法和属性(成员变量)的访问权限（范围）   </p><ol><li><p>公开级别:用 public 修饰,对外公开  </p></li><li><p>受保护级别:用 protected 修饰,对子类和同一个包中的类公开  </p></li><li><p>默认级别:没有修饰符号,向同一个包的类公开.   </p></li><li><p>私有级别:用 private 修饰,只有类本身可以访问,不对外公开</p></li></ol><p><img src="/2023/08/04/JavaSE/23.png" alt="访问控制符">  </p><p><img src="/2023/08/04/JavaSE/24.png" alt="访问控制符使用说明">   </p><p>##面向对象编程三大特征  </p><p>封装、继承和多态    </p><p>###封装<br><img src="/2023/08/04/JavaSE/25.png" alt="封装"></p><p>封装的实现步骤    </p><p><img src="/2023/08/04/JavaSE/26.png" alt="封装的实现步骤">    </p><p>###继承   </p><p>继承可以解决代码复用,让我们的编程更加靠近人类思维.当多个类存在相同的属性(变量)和方法时,可以从这些类中抽象出父类,在父类中定义这些相同的属性和方法，所有的子类不需要重新定义这些属性和方法，只需要通过 extends 来声明继承父类即可  </p><p><img src="/2023/08/04/JavaSE/27.png" alt="继承">  </p><p>继承的基本语法：  </p><p>class 子类 extends 父类 {</p><p>}<br>子类就会自动拥有父类定义的属性和方法<br>子类又叫超类，基类<br>子类又叫派生类  </p><p>继承给编程带来的便利  </p><ol><li><p>代码的复用性提高了  </p></li><li><p>代码的扩展性和维护性提高了</p></li></ol><p>**继承的细节问题： ** </p><ol><li><p>子类继承了所有的属性和方法，非私有的属性和方法可以在子类直接访问, 但是私有属性和方法不能在子类直接访问，要通过父类提供公共的方法去访问  </p></li><li><p>子类必须调用父类的构造器,完成父类的初始化  </p></li><li><p>当创建子类对象时，不管使用子类的哪个构造器，默认情况下总会去调用父类的无参构造器，如果父类没有提供无参构造器，则必须在子类的构造器中用 super 去指定使用父类的哪个构造器完成对父类的初始化工作，否则，编译不会通过(怎么理解。)   </p></li><li><p>如果希望指定去调用父类的某个构造器，则显式的调用一下 : super(参数列表)  </p></li><li><p>super 在使用时，必须放在构造器第一行(super 只能在构造器中使用)  </p></li><li><p>super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器  </p></li><li><p>java 所有类都是 Object 类的子类, Object 是所有类的基类.  </p></li><li><p>父类构造器的调用不限于直接父类！将一直往上追溯直到 Object 类(顶级父类)    </p></li><li><p>子类最多只能继承一个父类(指直接继承)，即 java 中是单继承机制。<br>思考：如何让 A 类继承 B 类和 C 类？ 【A 继承 B， B 继承 C】  </p></li><li><p>不能滥用继承，子类和父类之间必须满足 is-a 的逻辑关系</p></li></ol><p><strong>输入 ctrl + H 可以看到类的继承关系</strong></p><pre><code>public class Sub extends Base &#123; //子类    public Sub(String name, int age) &#123;    //1. 调用父类的无参构造器, 如下或者什么都不写,默认就是调用 super()    //super();//父类的无参构造器    //2. 调用父类的 Base(String name) 构造器    //super(&quot;hsp&quot;);    //调用父类的 Base(String name, int age) 构造器    super(&quot;king&quot;, 20);    //细节： super 在使用时，必须放在构造器第一行    //细节: super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器    //this() 不能再使用了    System.out.println(&quot;子类 Sub(String name, int age)构造器被调用....&quot;);    &#125;</code></pre><p>子类创建的内存布局  </p><p><img src="/2023/08/04/JavaSE/28.png" alt="子类创建的内存布局">    </p><p>###super 关键字  </p><p>super 代表父类的引用，用于访问父类的属性、方法、构造器  </p><p><img src="/2023/08/04/JavaSE/29.png" alt="super关键字">  </p><p>&#x2F;&#x2F; (1)先找本类，如果有，则调用  </p><p>&#x2F;&#x2F; (2)如果没有，则找父类(如果有，并可以调用，则调用)  </p><p>&#x2F;&#x2F; (3)如果父类没有，则继续找父类的父类,整个规则，就是一样的,直到 Object 类  </p><p>&#x2F;&#x2F; 提示：如果查找方法的过程中，找到了，但是不能访问， 则报错, cannot access  </p><p>&#x2F;&#x2F; 如果查找方法的过程中，没有找到，则提示方法不存在    </p><p><img src="/2023/08/04/JavaSE/30.png" alt="Super关键字的用法细节">      </p><p><strong>super 和 this 的比较</strong>    </p><p><img src="/2023/08/04/JavaSE/31.png" alt="super和this的比较">    </p><p>##方法重写&#x2F;覆盖(override) </p><p><img src="/2023/08/04/JavaSE/32.png" alt="方法重写">  </p><p><img src="/2023/08/04/JavaSE/33.png" alt="方法重写的注意事项">    </p><p><strong>方法重载和方法重写的区别</strong>  </p><p><img src="/2023/08/04/JavaSE/34.png" alt="方法重载和方法重写的区别">  </p><p>##多态  </p><p>多态是建立在封装和继承基础之上的<br><img src="/2023/08/04/JavaSE/35.png" alt="多态">     </p><p>多态的具体体现  </p><ol><li>方法的多态</li></ol><p>重写和重载就体现多态    </p><ol start="2"><li>对象的多态 (核心，困难，重点)</li></ol><p><img src="/2023/08/04/JavaSE/36.png" alt="多态案例">     </p><p>多态的前提是：两个对象(类)存在继承关系  </p><p>多态的向上转型   </p><p><img src="/2023/08/04/JavaSE/37.png" alt="多态的向上转型">   </p><p>多态向下转型  </p><p><img src="/2023/08/04/JavaSE/38.png" alt="多态的向下转型">    </p><p>属性没有重写之说！属性的值看编译类型   </p><p>instanceOf 比较操作符，用于判断对象的运行类型是否为 XX 类型或 XX 类型的子类型  </p><p>java 的动态绑定机制(非常非常重要.)  page 365</p><p>JDBC page 1119  </p><p>正则表达式 page 1210</p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%A4%A9%E8%A1%8C%E5%81%A5%EF%BC%8C%E5%90%9B%E5%AD%90%E4%BB%A5%E8%87%AA%E5%BC%BA%E4%B8%8D%E6%81%AF/">天行健，君子以自强不息</category>
      
      
      <comments>http://example.com/2023/08/04/JavaSE/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数仓建模</title>
      <link>http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/</link>
      <guid>http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/</guid>
      <pubDate>Thu, 03 Aug 2023 10:25:47 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;a href=&quot;https://help.aliyun.com/zh/dataworks/user-guide/dataworks-data-modeling/?spm=a2c4g.11186623.0.0.6cbf2c36NLQ1IR&quot;&gt;阿里数仓建模理论&lt;/a&gt;&lt;/p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<p><a href="https://help.aliyun.com/zh/dataworks/user-guide/dataworks-data-modeling/?spm=a2c4g.11186623.0.0.6cbf2c36NLQ1IR">阿里数仓建模理论</a></p>]]></content:encoded>
      
      
      
      
      <comments>http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>kafka3.0.0学习记录</title>
      <link>http://example.com/2023/08/03/kafka/</link>
      <guid>http://example.com/2023/08/03/kafka/</guid>
      <pubDate>Thu, 03 Aug 2023 02:18:49 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;黄沙百战穿金甲，不破楼兰终不还！   &lt;/p&gt;
&lt;p&gt;相关学习文档&lt;br&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw&quot;&gt;https://pan.baidu.com/s/1GAiGG8E6vI94YO</description>
        
      
      
      
      <content:encoded><![CDATA[<p>黄沙百战穿金甲，不破楼兰终不还！   </p><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw">https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw</a><br>提取码：85vn   </p><p>#1.Kafka概述   </p><p>##1.1定义：</p><p>Kafka传 统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（Message<br>Queue），主要应用于大数据实时处理领域。    </p><p>发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息<br>分为不同的类别，订阅者只接收感兴趣的消息。  </p><p>##1.2消息队列  </p><p>大数据场景主要采用 Kafka 作为消息队列。  </p><p>###1.2.1传统消息队列的应用场景  </p><p>传统的消息队列的主要应用场景包括：缓存&#x2F;消峰、解耦和异步通信。</p><p>缓冲&#x2F;消峰：解决生产消息和消费消息的处理速度不一致的情况。 </p><p>解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p><p><img src="/2023/08/03/kafka/kafka1.png" alt="解耦"> </p><p>异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。  </p><p>###1.2.2消息队列的两种模式    </p><p><img src="/2023/08/03/kafka/2.png" alt="消息队列的两种模式">  </p><p>##1.3kafka基础架构<br><img src="/2023/08/03/kafka/3.png" alt="kafka基础架构">   </p><p>（1）Producer：消息生产者，就是向 Kafka broker 发消息的客户端。  </p><p>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。  </p><p>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。消费者组内每个消<br>费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不<br>影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。  </p><p>（4）Broker：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个broker 可以容纳多个 topic。  </p><p>（5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个 topic。  </p><p>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。  </p><p>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个Follower。  </p><p>（8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数<br>据的对象都是 Leader。  </p><p>（9）Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和<br>Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Leader。    </p><p>##2kafka快速入门    </p><p>kafka命令行操作    </p><p>1）查看操作主题命令参数    </p><pre><code> bin/kafka-topics.sh    </code></pre><p>2）查看当前服务器中的所有 topic  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list</code></pre><p>3）创建 first topic  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 1 --replication-factor 3 --topic first  </code></pre><p>选项说明：    </p><pre><code>--topic 定义 topic 名  --replication-factor 定义副本数  --partitions 定义分区数  </code></pre><p>4）查看 first 主题的详情  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first</code></pre><p>5）修改分区数（注意：分区数只能增加，不能减少）  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3 </code></pre><p>6）删除 topic    </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first</code></pre><p>##生产者命令行操作</p><p>发送消息</p><pre><code>bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</code></pre><p>##消费者命令行操作  </p><p>###消费 first 主题中的数据</p><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first</code></pre><p>###把主题中所有的数据都读取出来（包括历史数据）  </p><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first  </code></pre><p>##生产者重要参数</p><p>acks </p><pre><code>1）0：生产者发送过来的数据，不需要等数据落盘应答。  2）1：生产者发送过来的数据，Leader 收到数据后应答。  3）-1（all）：生产者发送过来的数据，Leader+和 isr 队列里面的所有节点收齐数据后应答。默认值是-1，-1 和all 是等价的。  </code></pre><p>enable.idempotence  </p><p>是否开启幂等性，默认 true，开启幂等性。  </p><p>compression.type  </p><p>生产者发送的所有数据的压缩方式。默认是 none，也就是不压缩。  </p><p>支持压缩类型：none、gzip、snappy、lz4 和 zstd。    </p><p>##生产者分区    </p><p>###1.分区好处  </p><p>（1）便于合理使用存储资源  </p><p>每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一<br>块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果   </p><p>（2）提高并行度  </p><p>生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。  </p><p>##生产经验——生产者如何提高吞吐量    </p><p>batch.size：批次大小，默认16k   </p><p>linger.ms：等待时间，修改为5-100ms  </p><p>compression.type：压缩snappy   </p><p>RecordAccumulator：缓冲区大小，修改为64m  </p><p>##ack 应答原理  </p><p><img src="/2023/08/03/kafka/4.png" alt="ack应答原理">  </p><p><img src="/2023/08/03/kafka/5.png" alt="ack应答原理"> </p><p>数据完全可靠条件 &#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2  </p><p>###可靠性总结：  </p><p>acks&#x3D;0，生产者发送过来数据就不管了，可靠性差，效率高；  </p><p>acks&#x3D;1，生产者发送过来数据Leader应答，可靠性中等，效率中等；  </p><p>acks&#x3D;-1，生产者发送过来数据Leader和ISR队列里面所有Follwer应答，可靠性高，效率低；  </p><p>在生产环境中，acks&#x3D;0很少使用；acks&#x3D;1，一般用于传输普通日志，允许丢个别数据；acks&#x3D;-1，一般用于传输和钱相关的数据，对可靠性要求比较高的场景。    </p><p>##生产经验——数据去重    </p><p>至少一次（At Least Once）&#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2  </p><p>最多一次（At Most Once）&#x3D; ACK级别设置为0  </p><p>###总结：  </p><p>At Least Once可以保证数据不丢失，但是不能保证数据不重复；  </p><p>At Most Once可以保证数据不重复，但是不能保证数据不丢失。  </p><p>精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。  </p><p>Kafka 0.11版本以后，引入了一项重大特性：幂等性和事务。  </p><p>###幂等性原理  </p><p>幂等性就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。  </p><p>精确一次（Exactly Once） &#x3D; 幂等性 + 至少一次（ ack&#x3D;-1 + 分区副本数&gt;&#x3D;2 + ISR最小副本数量&gt;&#x3D;2）  </p><p>幂等性只能保证的是在单分区单会话内不重复。  </p><p>####如何使用幂等性    </p><p>开启参数 enable.idempotence 默认为 true，false 关闭。    </p><p>开启事务，必须开启幂等性。  </p><p>##生产经验——数据乱序</p><p>）kafka在1.x版本之前保证数据单分区有序，条件如下：  </p><p>max.in.flight.requests.per.connection&#x3D;1（不需要考虑是否开启幂等性）。  </p><p>2）kafka在1.x及以后版本保证数据单分区有序，条件如下：    </p><p>（1）未开启幂等性  </p><p>max.in.flight.requests.per.connection需要设置为1。  </p><p>（2）开启幂等性   </p><p>max.in.flight.requests.per.connection需要设置小于等于5。    </p><p>##Kafka 副本  </p><p>###副本基本信息  </p><p>（1）Kafka 副本作用：提高数据可靠性。  </p><p>（2）Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；  </p><p>（3）Kafka 中副本分为：Leader 和 Follower。    </p><p>（4）Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。AR &#x3D; ISR + OSR  </p><p>ISR，表示和 Leader 保持同步的 Follower 集合。如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值由 replica.lag.time.max.ms参数设定，默认 30s。Leader 发生故障之后，就会从 ISR 中选举新的 Leader    </p><p>OSR，表示 Follower 与 Leader 副本同步时，延迟过多的副本。    </p><p>###Leader 选举流程  </p><p>Kafka 集群中有一个 broker 的 Controller 会被选举为 Controller Leader，负责管理集群broker 的上下线，所有 topic 的分区副本分配和 Leader 选举等工作。  </p><p><img src="/2023/08/03/kafka/6.png" alt="Leader选举流程">  </p><p>###Leader 和 Follower 故障处理细节<br><img src="/2023/08/03/kafka/7.png" alt="Follower故障处理细节">   </p><p><img src="/2023/08/03/kafka/8.png" alt="Leader故障处理细节"></p><p>###生产经验——Leader Partition 负载平衡  </p><p>正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p><p>auto.leader.rebalance.enable 默认是 true。 自动 Leader Partition 平衡。生产环境中，leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭。   </p><p>leader.imbalance.per.broker.percentage 默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器<br>会触发 leader 的平衡。  </p><p>leader.imbalance.check.interval.seconds 默认值 300 秒。检查 leader 负载是否平衡的间隔时间。  </p><p>###文件存储    </p><p>1）Topic 数据的存储机制</p><p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。  </p><p><img src="/2023/08/03/kafka/9.png" alt="Kafka文件存储机制">   </p><p>Topic数据存储位置：每个broker节点的 kafka&#x2F;datas&#x2F;目录下</p><p>通过工具查看 index 和 log 信息<br>[atguigu@hadoop104 first-1]$ kafka-run-class.sh kafka.tools.DumpLogSegments –files .&#x2F;00000000000000000000.index   </p><pre><code>Dumping ./00000000000000000000.index  offset: 3 position: 152 </code></pre><p>####Log文件和Index文件详解  </p><p><img src="/2023/08/03/kafka/10.png" alt="Log文件和Index文件详解">    </p><p>log.segment.bytes Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，默认值 1G。  </p><p>log.index.interval.bytes 默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。 稀疏索引。    </p><p>###文件清理策略    </p><p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。    </p><p>Kafka 中提供的日志清理策略有 delete 和 compact 两种。  </p><p>1）delete 日志删除：将过期数据删除<br> log.cleanup.policy &#x3D; delete 所有数据启用删除策略   </p><p>（1）基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳。  </p><p>（2）基于大小：默认关闭。超过设置的所有日志总大小，删除最早的 segment。log.retention.bytes，默认等于-1，表示无穷大。  </p><p>2）compact 日志压缩  </p><p>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。    </p><p>log.cleanup.policy &#x3D; compact 所有数据启用压缩策略</p><p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。  </p><p>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。  </p><p><img src="/2023/08/03/kafka/11.png" alt="compact日志压缩">  </p><p>##高效读写数据  </p><p>1）Kafka 本身是分布式集群，可以采用分区技术，并行度高  </p><p>2）读数据采用稀疏索引，可以快速定位要消费的数据  </p><p>3）顺序写磁盘  </p><p>   顺序写之所以快，是因为其省去了大量磁头寻址的时间。  </p><p>4）页缓存 + 零拷贝技术     </p><p>##Kafka 消费者  </p><p>###Kafka 消费方式  </p><p>pull（拉）模 式：</p><p>consumer采用从broker中主动拉取数据。Kafka采用这种方式。</p><p>push（推）模式：  </p><p>Kafka没有采用这种方式，因为由broker决定消息发送速率，很难适应所有消费者的消费速率。    </p><p>pull模式不足之处是，如 果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据。  </p><p>Kafka 消费者总体工作流程  </p><p><img src="/2023/08/03/kafka/12.png" alt="kafka消费总体工作流程">    </p><p>消费者组原理    </p><p>Consumer Group（CG）：消费者组，由多个consumer组成。形成一个消费者组的条件，是所有消费者的groupid相同。  </p><p>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费。  </p><p>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。    </p><p><img src="/2023/08/03/kafka/13.png" alt="消费者组原理">  </p><p>##生产经验——分区的分配以及再平衡</p><p>Kafka有四种主流的分区分配策略： Range、RoundRobin、Sticky、CooperativeSticky。可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略  </p><p>###Range 以及再平衡 </p><p>Range 是对每个 topic 而言的。首先对同一个 topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。  </p><p>容易产生数据倾斜！    </p><p>Kafka 默认的分区分配策略就是 Range + CooperativeSticky   </p><p>###RoundRobin 以及再平衡  </p><p>RoundRobin 轮询分区策略，是把所有的 partition 和所有的consumer 都列出来，然后按照 hashcode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。  </p><p>###Sticky 以及再平衡  </p><p>粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，<br>考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。<br>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。   </p><p>##offset 位移       </p><p>从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets</p><p>__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是group.id+topic+分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact，也就是每个 group.id+topic+分区号就保留最新数据。  </p><p>###自动提交 offset   </p><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。  </p><p>自动提交offset的相关参数：  </p><p>enable.auto.commit：是否开启自动提交offset功能，默认是true  </p><p>auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s  </p><p>###手动提交offset  </p><p>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次提交的一批数据最高的偏移量提交；不同点是，同步提交阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故有可能提交失败。  </p><p>commitSync（同步提交）：必须等待offset提交完毕，再去消费下一批数据。  </p><p>commitAsync（异步提交） ：发送完提交offset请求后，就开始消费下一批数据了。  </p><p>###指定 Offset 消费</p><pre><code>auto.offset.reset = earliest | latest | none 默认是 latest</code></pre><p>（1）earliest：自动将偏移量重置为最早的偏移量，–from-beginning。  </p><p>（2）latest（默认值）：自动将偏移量重置为最新偏移量。  </p><p>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。  </p><p>（4）任意指定 offset 位移开始消费    </p><p>##漏消费和重复消费</p><p>重复消费：已经消费了数据，但是 offset 没提交。 </p><p>漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。  </p><p>###生产经验——消费者事务     </p><p>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交<br>offset过程做原子绑定。  </p><p>###生产经验——数据积压（消费者如何提高吞吐量）    </p><p><img src="/2023/08/03/kafka/14.png" alt="数据积压">    </p><p>##Kafka-Kraft 模式    </p><p><img src="/2023/08/03/kafka/15.png" alt="kafka-kraft架构">  </p><p>右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。  </p><p>这样做的好处有以下几个：  </p><p>Kafka 不再依赖外部框架，而是能够独立运行；    </p><p>controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升；  </p><p>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制；  </p><p>controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强  </p><p>controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。  </p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E9%BB%84%E6%B2%99%E7%99%BE%E6%88%98%E7%A9%BF%E9%87%91%E7%94%B2%EF%BC%8C%E4%B8%8D%E7%A0%B4%E6%A5%BC%E5%85%B0%E7%BB%88%E4%B8%8D%E8%BF%98/">黄沙百战穿金甲，不破楼兰终不还</category>
      
      
      <comments>http://example.com/2023/08/03/kafka/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark学习笔记</title>
      <link>http://example.com/2023/08/02/Spark-Core/</link>
      <guid>http://example.com/2023/08/02/Spark-Core/</guid>
      <pubDate>Wed, 02 Aug 2023 02:51:06 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;纵浪大化中，不喜亦不惧&quot;&gt;&lt;a href=&quot;#纵浪大化中，不喜亦不惧&quot; class=&quot;headerlink&quot; title=&quot;纵浪大化中，不喜亦不惧 ~&quot;&gt;&lt;/a&gt;纵浪大化中，不喜亦不惧 ~&lt;/h1&gt;&lt;p&gt;相关学习文档&lt;br&gt;链接：&lt;a href=&quot;https://</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="纵浪大化中，不喜亦不惧"><a href="#纵浪大化中，不喜亦不惧" class="headerlink" title="纵浪大化中，不喜亦不惧 ~"></a>纵浪大化中，不喜亦不惧 ~</h1><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g">https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g</a><br>提取码：mg5w   </p><h2 id="Spark-是什么？"><a href="#Spark-是什么？" class="headerlink" title="Spark 是什么？"></a>Spark 是什么？</h2><p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎     </p><p>Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎  </p><p>Spark Core 中提供了 Spark 最基础与最核心的功能  </p><p>Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据   </p><p>Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API  </p><p>Spark 一直被认为是 Hadoop 框架的升级版。  </p><h2 id="Mapreduce是什么？"><a href="#Mapreduce是什么？" class="headerlink" title="Mapreduce是什么？"></a>Mapreduce是什么？</h2><p>MapReduce 是一种编程模型，作为 Hadoop 的分布式计算模型，是 Hadoop 的核心    </p><h2 id="HBase是什么？"><a href="#HBase是什么？" class="headerlink" title="HBase是什么？"></a>HBase是什么？</h2><p>HBase 是一个基于 HDFS 的分布式数据库，擅长实时地随机读&#x2F;写超大规模数据集  </p><h2 id="Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）"><a href="#Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）" class="headerlink" title="Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）"></a>Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）</h2><p>Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘  </p><p>Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互  </p><p>Spark 的缓存机制比 HDFS 的缓存机制高效  </p><h2 id="什么时候选用Spark什么时候选用Mapreduce？"><a href="#什么时候选用Spark什么时候选用Mapreduce？" class="headerlink" title="什么时候选用Spark什么时候选用Mapreduce？"></a>什么时候选用Spark什么时候选用Mapreduce？</h2><p>Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark并不能完全替代 MR。  </p><h2 id="提交Spark应用的代码示例"><a href="#提交Spark应用的代码示例" class="headerlink" title="提交Spark应用的代码示例"></a>提交Spark应用的代码示例</h2><h3 id="local模式"><a href="#local模式" class="headerlink" title="local模式"></a>local模式</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ./examples/jars/spark-examples_2.12-3.0.0.jar 101) --class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序  2) --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟 CPU 核数量  3) spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包  4) 数字 10 表示程序的入口参数，用于设定当前应用的任务数量  </code></pre><h3 id="Yarn模式-（生产：Cluster模式）"><a href="#Yarn模式-（生产：Cluster模式）" class="headerlink" title="Yarn模式  （生产：Cluster模式）"></a>Yarn模式  （生产：Cluster模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2.12-3.0.0.jar 10  </code></pre><h3 id="Yarn模式-（测试：Client模式）"><a href="#Yarn模式-（测试：Client模式）" class="headerlink" title="Yarn模式  （测试：Client模式）"></a>Yarn模式  （测试：Client模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.12-3.0.0.jar 10  </code></pre><h4 id="Spark的端口号"><a href="#Spark的端口号" class="headerlink" title="Spark的端口号"></a>Spark的端口号</h4><pre><code>➢ Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）  ➢ Spark Master 内部通信服务端口号：7077  ➢ Standalone 模式下，Spark Master Web 端口号：8080（资源）  ➢ Spark 历史服务器端口号：18080  ➢ Hadoop YARN 任务运行情况查看端口号：8088     </code></pre><h3 id="Spark运行架构"><a href="#Spark运行架构" class="headerlink" title="Spark运行架构"></a>Spark运行架构</h3><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构  </p><p><img src="/2023/08/02/Spark-Core/1.png" alt="Spark运行架构">  </p><p>Driver表示Master，负责管理整个集群中的作业调度  </p><p>Executor表示slave，负责实际执行任务  </p><h3 id="Spark核心组件"><a href="#Spark核心组件" class="headerlink" title="Spark核心组件"></a>Spark核心组件</h3><p>Driver和Executor &amp; Master 和 Worker </p><h4 id="Driver的作用"><a href="#Driver的作用" class="headerlink" title="Driver的作用"></a>Driver的作用</h4><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。  </p><p>Driver 在 Spark 作业执行时主要负责：  </p><pre><code>➢ 将用户程序转化为作业（job）  ➢ 在 Executor 之间调度任务(task)  ➢ 跟踪 Executor 的执行情况  ➢ 通过 UI 展示查询运行情况      </code></pre><p>所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。  </p><h3 id="Executor的作用"><a href="#Executor的作用" class="headerlink" title="Executor的作用"></a>Executor的作用</h3><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立  </p><p>Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。  </p><h3 id="Executor-有两个核心功能："><a href="#Executor-有两个核心功能：" class="headerlink" title="Executor 有两个核心功能："></a>Executor 有两个核心功能：</h3><pre><code>➢ 负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程➢ 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。  </code></pre><h3 id="Master-和-Worker-Local模式时"><a href="#Master-和-Worker-Local模式时" class="headerlink" title="Master 和 Worker  (Local模式时)"></a>Master 和 Worker  (Local模式时)</h3><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker    </p><p>在Yarn模式时，Master 就是 RM ,Worker 就是 NM</p><h4 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h4><p>这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM  </p><h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>Worker也是进程，一个 Worker运行在集群中的一台服务器上，由 Master分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NM。    </p><h3 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h3><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。<br>说的简单点就是，ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是ApplicationMaster。    </p><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Executor-与-Core"><a href="#Executor-与-Core" class="headerlink" title="Executor 与 Core"></a>Executor 与 Core</h3><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。  </p><p>应用程序相关启动参数如下：<br>–num-executors 配置 Executor 的数量<br>–executor-memory 配置每个 Executor 的内存大小<br>–executor-cores 配置每个 Executor 的虚拟 CPU core 数量    </p><h3 id="并行度（Parallelism）"><a href="#并行度（Parallelism）" class="headerlink" title="并行度（Parallelism）"></a>并行度（Parallelism）</h3><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，是并行，而不是并发。  </p><p>将整个集群并行执行任务的数量称之为并行度。</p><p>一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。   </p><h3 id="有向无环图（DAG）"><a href="#有向无环图（DAG）" class="headerlink" title="有向无环图（DAG）"></a>有向无环图（DAG）</h3><p><img src="/2023/08/02/Spark-Core/2.png" alt="有向无环图">  </p><p>这里所谓的有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。    </p><p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。    </p><h3 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h3><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。  </p><p><img src="/2023/08/02/Spark-Core/3.png" alt="基于Yarn的Spark任务提交流程">   </p><p>Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client和 Cluster。两种模式主要区别在于：<strong>Driver 程序的运行节点位置</strong>。  </p><h4 id="Yarn-Client-模式"><a href="#Yarn-Client-模式" class="headerlink" title="Yarn Client 模式"></a>Yarn Client 模式</h4><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。  </p><pre><code>➢ Driver 在任务提交的本地机器上运行➢ Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster➢ ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存➢ ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程  ➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。  </code></pre><h4 id="Yarn-Cluster-模式"><a href="#Yarn-Cluster-模式" class="headerlink" title="Yarn Cluster 模式"></a>Yarn Cluster 模式</h4><p>Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。  </p><pre><code>➢ 在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster➢ 随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。➢ Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程  ➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行    </code></pre><h2 id="Spark-核心编程"><a href="#Spark-核心编程" class="headerlink" title="Spark 核心编程"></a>Spark 核心编程</h2><p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。<strong>三大数据结构</strong>分别是：  </p><pre><code>➢ RDD : 弹性分布式数据集➢ 累加器：分布式共享只写变量➢ 广播变量：分布式共享只读变量</code></pre><h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合  </p><pre><code>➢ 弹性    存储的弹性：内存与磁盘的自动切换；    容错的弹性：数据丢失可以自动恢复；    计算的弹性：计算出错重试机制；    分片的弹性：可根据需要重新分片。➢ 分布式：数据存储在大数据集群不同节点上➢ 数据集：RDD 封装了计算逻辑，并不保存数据➢ 数据抽象：RDD 是一个抽象类，需要子类具体实现➢ 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑➢ 可分区、并行计算  </code></pre><h4 id="执行原理"><a href="#执行原理" class="headerlink" title="执行原理"></a>执行原理</h4><p>从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。  </p><p>执行时，需要将计算资源和计算模型进行协调和整合。  </p><p>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。  </p><p>在 Yarn 环境中，RDD的工作原理:  </p><p>1）启动Yarn集群环境  </p><p><img src="/2023/08/02/Spark-Core/5.png" alt="启动Yarn集群环境">   </p><p>2）Spark通过申请资源创建调度节点和计算节点   </p><p><img src="/2023/08/02/Spark-Core/6.png" alt="创建调度节点和计算节点">   </p><p>3）Spark框架根据需求将计算逻辑根据分区划分成不同的任务  </p><p><img src="/2023/08/02/Spark-Core/7.png" alt="根据分区划分成不同的任务">    </p><p>4）调度节点将任务根据计算节点状态发送到对应的计算节点进行计算  </p><p><img src="/2023/08/02/Spark-Core/8.png" alt="将任务分发给对应的计算节点进行计算"> </p><p>RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算  </p><h1 id="RDD算子总结"><a href="#RDD算子总结" class="headerlink" title="RDD算子总结"></a>RDD算子总结</h1><h2 id="Value类型总结"><a href="#Value类型总结" class="headerlink" title="Value类型总结"></a>Value类型总结</h2><p>map</p><pre><code>｜ def map[U: ClassTag](f: T =&gt; U): RDD[U]｜ 将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。｜ val dataRDD1: RDD[Int] = dataRDD.map(｜  num =&gt;｜        &#123;   num * 2 &#125;｜ )｜ val dataRDD2: RDD[String] = dataRDD1.map(｜  num =&gt; &#123;｜  &quot;&quot; + num｜  &#125;｜ )｜ ​val mapRDD: RDD[Int] = rdd.map(_*2)对传入的数据，一个一个的进行转换，再返回给结果集</code></pre><p>mapPartitions</p><pre><code>｜ def mapPartitions[U: ClassTag](｜  f: Iterator[T] =&gt; Iterator[U],｜  preservesPartitioning: Boolean = false): RDD[U]｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。｜ val dataRDD1: RDD[Int] = dataRDD.mapPartitions(｜  datas =&gt; &#123;｜  datas.filter(_==2)｜  &#125;｜ )｜ ｜ val mpRDD: RDD[Int] = rdd.mapPartitions(    iter =&gt; &#123;println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;)        iter.map(_ * 2) &#125;)一个分区一个分区的数据进行转换，再返回给结果集</code></pre><p>map 和 mapPartitions 的区别：</p><pre><code>数据处理角度：    Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作功能的角度：    Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。    MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据性能的角度：    Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高    但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作</code></pre><p>mapPartitionsWithIndex</p><pre><code>｜ def mapPartitionsWithIndex[U: ClassTag](｜  f: (Int, Iterator[T]) =&gt; Iterator[U],｜  preservesPartitioning: Boolean = false): RDD[U]｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引｜ val dataRDD1 = dataRDD.mapPartitionsWithIndex(｜  (index, datas) =&gt; &#123;｜  datas.map(index, _)｜  &#125;｜ )｜ ｜ mapPartitionsWithIndex在mapPartitions基础上加上了分区indexval mpiRDD = rdd.mapPartitionsWithIndex(  (index,iter) =&gt; &#123;// 1 ,     2 ,     3 ,    4    // (0,1)   (2,2)   (4,3)  (6,4)    iter.map(      num =&gt; &#123; (index,num) &#125;  ）&#125; )</code></pre><p>flatMap</p><p>｜ def flatMap[U: ClassTag](f: T &#x3D;&gt; TraversableOnce[U]): RDD[U]<br>｜ 将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射<br>｜ val dataRDD1 &#x3D; dataRDD.flatMap( list &#x3D;&gt; list)<br>｜<br>    val flatRDD:RDD[String] &#x3D; rdd.flatMap( s &#x3D;&gt; { s.split(“ “)  })<br>        Hello<br>        Scala<br>        Hello<br>        Spark</p><pre><code>如果使用rdd.map( s =&gt; &#123; s.split(&quot; &quot;)  &#125;)，会发现打印的结果是    [Ljava.lang.String;@f1a45f8    [Ljava.lang.String;@5edf2821所以切割等扁平映射操作，选用flatMap</code></pre><p>glom </p><p>｜ def glom(): RDD[Array[T]]<br>｜ 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变<br>｜ val dataRDD1:RDD[Array[Int]] &#x3D; dataRDD.glom()<br>    将同一个分区的数据直接转换为相同类型的内存数组进行处理<br>    List[Int] &#x3D;&gt; Array[Int]</p><p>groupBy</p><pre><code>｜ def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]｜ 将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。极限情况下，数据可能被分在同一个分区中｜ ​val groupRDD:RDD[(Int,Iterable[Int])] = rdd.groupBy(num % 2)    (0,CompactBuffer(2, 4))    (1,CompactBuffer(1, 3))    会输出一个RDD[(K, Iterable[T])]</code></pre><p>filter<br>    ｜ 将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。<br>    val dataRDD1 &#x3D; dataRDD.filter(_%2 &#x3D;&#x3D; 0)</p><p>sample</p><p>｜ def sample(<br>｜ withReplacement: Boolean,<br>｜  fraction: Double,<br>｜  seed: Long &#x3D; Utils.random.nextLong): RDD[T]<br>｜ 根据指定的规则从数据集中抽取数据<br>｜<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4<br>    ｜ ),1)<br>｜ &#x2F;&#x2F; 抽取数据不放回（伯努利算法）<br>｜ &#x2F;&#x2F; 伯努利算法：又叫 0、1 分布。例如扔硬币，要么正面，要么反面。<br>｜ &#x2F;&#x2F; 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不<br>｜ 要<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD1 &#x3D; dataRDD.sample(false, 0.5)<br>｜ &#x2F;&#x2F; 抽取数据放回（泊松算法）<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，true：放回；false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：重复数据的几率，范围大于等于 0.表示每一个元素被期望抽取到的次数<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD2 &#x3D; dataRDD.sample(true, 2)<br>｜ ​<br>｜<br>    相同的seed种子，多次运行依旧是相同的抽样结果,修改withReplacement也不会发生变化，&#x2F;&#x2F; 修改fraction后结果会发生改变<br>    rdd.sample(true,0.5,101)</p><p>distinct</p><p>｜ def distinct()(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>｜ def distinct(numPartitions: Int)(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>    ｜ 将数据集中重复的数据去重<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4,1,2<br>    ｜ ),1)<br>    ｜ val dataRDD1 &#x3D; dataRDD.distinct()<br>    ｜ val dataRDD2 &#x3D; dataRDD.distinct(2)<br>｜ ​<br>｜<br>coalesce<br>｜ def coalesce(numPartitions: Int, shuffle: Boolean &#x3D; false,<br>｜  partitionCoalescer: Option[PartitionCoalescer] &#x3D; Option.empty)<br>｜  (implicit ord: Ordering[T] &#x3D; null)<br>｜  : RDD[T]<br>｜ 根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率<br>｜ 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少<br>｜ 分区的个数，减小任务调度成本<br>｜ ​<br>｜<br>    &#x2F;&#x2F; coalesce 方法默认情况下不会将分区的数据打乱重新组合<br>    &#x2F;&#x2F; 这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜<br>    &#x2F;&#x2F; 如果想要让数据倾斜，可以进行shuffle处理<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2,shuffle &#x3D; false)<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2) 默认不进行shuffle<br>    &#x2F;&#x2F; 进行shuffle处理后会出现数据倾斜val newRDD &#x3D; rdd.coalesce(2, true)</p><p>repartition<br>    ｜ def repartition(numPartitions: Int)(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>    repartition 操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true</p><p>sortBy<br>｜ def sortBy[K](<br>｜  f: (T) &#x3D;&gt; K,<br>｜ ascending: Boolean &#x3D; true,<br>｜  numPartitions: Int &#x3D; this.partitions.length)<br>｜  (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]<br>｜ 该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，之后按照 f 函数处理<br>｜ 的结果进行排序，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一<br>｜ 致。中间存在 shuffle 的过程<br>｜ ​<br>｜ ​<br>    默认升序排序<br>    val dataRDD1 &#x3D; dataRDD.sortBy(num&#x3D;&gt;num, false, 4)<br>    val newRDD &#x3D; rdd.sortBy(t &#x3D;&gt; t._1.toInt, true)</p><h2 id="双-Value-类型总结"><a href="#双-Value-类型总结" class="headerlink" title="双 Value 类型总结"></a>双 Value 类型总结</h2><p>intersection </p><pre><code>def intersection(other: RDD[T]): RDD[T]val dataRDD = dataRDD1.intersection(dataRDD2)求交集</code></pre><p>union </p><pre><code>def union(other: RDD[T]): RDD[T]val dataRDD = dataRDD1.union(dataRDD2)求并集</code></pre><p>subtract</p><pre><code>def subtract(other: RDD[T]): RDD[T]val dataRDD = dataRDD1.subtract(dataRDD2)求差集</code></pre><p>zip</p><pre><code>def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]val dataRDD = dataRDD1.zip(dataRDD2)将两个 RDD 中的元素，以键值对的形式进行合并。其中，键值对中的 Key 为第 1 个 RDD中的元素，Value 为第 2 个 RDD 中的相同位置的元素</code></pre><h2 id="Key-Value类型总结"><a href="#Key-Value类型总结" class="headerlink" title="Key-Value类型总结"></a>Key-Value类型总结</h2><p>partitionBy </p><pre><code>def partitionBy(partitioner: Partitioner): RDD[(K, V)]将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitionerval rdd2: RDD[(Int, String)] = rdd.partitionBy(new HashPartitioner(2))</code></pre><p>reduceByKey</p><pre><code>def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]可以将数据按照相同的 Key 对 Value 进行聚合val dataRDD2 = dataRDD1.reduceByKey(_+_)</code></pre><p>groupByKey </p><pre><code>def groupByKey(): RDD[(K, Iterable[V])]def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]将数据源的数据根据 key 对 value 进行分组val dataRDD2 = dataRDD1.groupByKey()val dataRDD3 = dataRDD1.groupByKey(2)val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2))</code></pre><p>reduceByKey 和 groupByKey 的区别：</p><pre><code>从 shuffle 的角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。从功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey</code></pre><p>aggregateByKey</p><pre><code>def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): RDD[(K, U)]将数据根据不同的规则进行分区内计算和分区间计算dataRDD1.aggregateByKey(0)(_+_,_+_)｜ // 1. 第一个参数列表中的参数表示初始值｜ // 2. 第二个参数列表中含有两个参数｜ // 2.1 第一个参数表示分区内的计算规则｜ // 2.2 第二个参数表示分区间的计算规则｜ </code></pre><p>foldByKey</p><pre><code>def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey</code></pre><p>combineByKey</p><pre><code>def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]最通用的对 key-value 型 rdd 进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致val combineRdd: RDD[(String, (Int, Int))] = input.combineByKey( (_, 1), (acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1), (acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2))</code></pre><p>reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别：</p><pre><code>｜ reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同｜ FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同｜ AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同｜ CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同｜ </code></pre><p>sortByKey</p><pre><code>def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length): RDD[(K, V)]val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(true)val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(false)</code></pre><p>join</p><pre><code>def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的(K,(V,W))的 RDDrdd.join(rdd1).collect().foreach(println)</code></pre><p>leftOuterJoin</p><pre><code>def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]类似于 SQL 语句的左外连接val rdd: RDD[(String, (Int, Option[Int]))] = dataRDD1.leftOuterJoin(dataRDD2)</code></pre><p>rightOuterJoin</p><pre><code>类似于 SQL 语句的右外连接</code></pre><p>cogroup </p><pre><code>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))类型的 RDDval value: RDD[(String, (Iterable[Int], Iterable[Int]))] = dataRDD1.cogroup(dataRDD2)(a,(CompactBuffer(1),CompactBuffer(4)))(b,(CompactBuffer(2),CompactBuffer(5)))(c,(CompactBuffer(3),CompactBuffer(6, 7)))</code></pre><h1 id="Spark行动算子"><a href="#Spark行动算子" class="headerlink" title="Spark行动算子"></a>Spark行动算子</h1><p>collect   </p><p>在驱动程序中，以数组 Array 的形式返回数据集的所有元素  </p><pre><code>// 收集数据到 Driverrdd.collect().foreach(println)  </code></pre><p>count  </p><p>返回 RDD 中元素的个数  </p><pre><code>// 返回 RDD 中元素的个数val countResult: Long = rdd.count()  </code></pre><p>first   </p><p>返回 RDD 中的第一个元素  </p><pre><code>// 返回 RDD 中元素的第1个元素val firstResult: Int = rdd.first()  </code></pre><p>take  </p><p>返回一个由 RDD 的前 n 个元素组成的数组  </p><pre><code>// 返回 RDD 中元素的个数val takeResult: Array[Int] = rdd.take(2)println(takeResult.mkString(&quot;,&quot;))  </code></pre><p>takeOrdered  </p><p>返回该 RDD 排序后的前 n 个元素组成的数组  </p><pre><code>// 返回 RDD 中元素的个数val result: Array[Int] = rdd.takeOrdered(2)  </code></pre><p>aggregate   </p><p>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合  </p><pre><code>val result: Int = rdd.aggregate(10)(_ + _, _ + _)  </code></pre><p>fold  </p><p>折叠操作，aggregate 的简化版操作  </p><pre><code>val foldResult: Int = rdd.fold(0)(_+_)  </code></pre><p>countByKey  </p><p>统计每种 key 的个数  </p><pre><code>// 统计每种 key 的个数val result: collection.Map[Int, Long] = rdd.countByKey()  </code></pre><p>save 相关算子  </p><p>将数据保存到不同格式的文件中  </p><pre><code>// 保存成 Text 文件rdd.saveAsTextFile(&quot;output&quot;)  </code></pre><p>foreach  </p><pre><code>// 收集后打印rdd.map(num=&gt;num).collect().foreach(println)  </code></pre><h2 id="RDD-序列化"><a href="#RDD-序列化" class="headerlink" title="RDD 序列化"></a>RDD 序列化</h2><h3 id="闭包检查"><a href="#闭包检查" class="headerlink" title="闭包检查"></a>闭包检查</h3><p>从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。</p><h2 id="RDD-依赖关系"><a href="#RDD-依赖关系" class="headerlink" title="RDD 依赖关系"></a>RDD 依赖关系</h2><p>1）RDD血缘关系  </p><p>RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage（血统）记录下来，以便恢复丢失的分区。    </p><p>RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。   </p><pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)println(fileRDD.toDebugString)</code></pre><p>2）RDD依赖关系  </p><p>所谓的依赖关系，其实就是两个相邻 RDD 之间的关系。  </p><pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)println(fileRDD.dependencies)   </code></pre><p>3）RDD 窄依赖（没有Shuffle）     </p><p>窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，窄依赖我们形象的比喻为独生子女。</p><p>4）RDD宽依赖 （有Shuffle）</p><p>宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。    </p><h4 id="RDD-任务划分"><a href="#RDD-任务划分" class="headerlink" title="RDD 任务划分"></a>RDD 任务划分</h4><p>RDD 任务切分中间分为：Application、Job、Stage 和 Task  </p><pre><code>Application：初始化一个 SparkContext 即生成一个 Application；Job：一个 Action 算子就会生成一个 Job；Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数。  </code></pre><p>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系</p><p><img src="/2023/08/02/Spark-Core/9.png" alt="Spark任务划分流程">     </p><h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><ol><li>RDD Cache 缓存</li></ol><p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用。  </p><pre><code>// cache 操作会增加血缘关系，不改变原有的血缘关系  // 可以更改存储级别//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)  </code></pre><h3 id="RDD持久化可选存储级别"><a href="#RDD持久化可选存储级别" class="headerlink" title="RDD持久化可选存储级别"></a>RDD持久化可选存储级别</h3><pre><code>object StorageLevel &#123; val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(true, true, true, false, 1)  </code></pre><p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD 的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。  </p><p>Spark 会自动对一些 Shuffle 操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点 Shuffle 失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用 persist 或 cache。  </p><p>2）RDD CheckPoint 检查点  </p><p>所谓的检查点其实就是通过将 RDD 中间结果写入磁盘  </p><p>对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发。  </p><ol start="3"><li><p>缓存和检查点区别  </p><p> 1）Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖。<br> 2）Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高。<br> 3）建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD</p></li></ol><h3 id="RDD-分区器"><a href="#RDD-分区器" class="headerlink" title="RDD 分区器"></a>RDD 分区器</h3><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认分区。   </p><p>分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。   </p><pre><code>➢ 只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None  ➢ 每个 RDD 的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。  </code></pre><ol><li><p>Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余  </p></li><li><p>Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p></li></ol><h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge。  </p><h3 id="系统累加器"><a href="#系统累加器" class="headerlink" title="系统累加器"></a>系统累加器</h3><pre><code>val rdd = sc.makeRDD(List(1,2,3,4,5))// 声明累加器var sum = sc.longAccumulator(&quot;sum&quot;);rdd.foreach( num =&gt; &#123; // 使用累加器 sum.add(num) &#125;)// 获取累加器的值println(&quot;sum = &quot; + sum.value)</code></pre><h3 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h3><pre><code>// 自定义累加器// 1. 继承 AccumulatorV2，并设定泛型// 2. 重写累加器的抽象方法class WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, Long]]&#123;var map : mutable.Map[String, Long] = mutable.Map()// 累加器是否为初始状态override def isZero: Boolean = &#123; map.isEmpty&#125;// 复制累加器override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = &#123; new WordCountAccumulator&#125;// 重置累加器override def reset(): Unit = &#123; map.clear()&#125;// 向累加器中增加数据 (In)override def add(word: String): Unit = &#123; // 查询 map 中是否存在相同的单词 // 如果有相同的单词，那么单词的数量加 1 // 如果没有相同的单词，那么在 map 中增加这个单词 map(word) = map.getOrElse(word, 0L) + 1L&#125;// 合并累加器override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): Unit = &#123; val map1 = map val map2 = other.value // 两个 Map 的合并 map = map1.foldLeft(map2)( ( innerMap, kv ) =&gt; &#123; innerMap(kv._1) = innerMap.getOrElse(kv._1, 0L) + kv._2 innerMap &#125; )&#125;// 返回累加器的结果 （Out）override def value: mutable.Map[String, Long] = map&#125;</code></pre><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。  </p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E7%BA%B5%E6%B5%AA%E5%A4%A7%E5%8C%96%E4%B8%AD%EF%BC%8C%E4%B8%8D%E5%96%9C%E4%BA%A6%E4%B8%8D%E6%83%A7/">纵浪大化中，不喜亦不惧</category>
      
      
      <comments>http://example.com/2023/08/02/Spark-Core/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>SQLServer查询体系学习记录</title>
      <link>http://example.com/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</link>
      <guid>http://example.com/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</guid>
      <pubDate>Tue, 01 Aug 2023 03:07:31 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;#SQLServer查询处理体系结构指南&lt;/p&gt;
&lt;p&gt;##执行模式&lt;br&gt;行执行模式&lt;br&gt;批执行模式 &lt;/p&gt;
&lt;p&gt;###行执行模式&lt;br&gt;行模式执行是用于传统 RDBMS 表（其中数据以行格式存储）的查询处理方法&lt;/p&gt;
&lt;p&gt;###批执行模式&lt;br&gt;批模式执行是一</description>
        
      
      
      
      <content:encoded><![CDATA[<p>#SQLServer查询处理体系结构指南</p><p>##执行模式<br>行执行模式<br>批执行模式 </p><p>###行执行模式<br>行模式执行是用于传统 RDBMS 表（其中数据以行格式存储）的查询处理方法</p><p>###批执行模式<br>批模式执行是一种查询处理方法，用于统一处理多个行（因此采用“批”一词）    </p><p>批中的每列都作为一个矢量存储在单独的内存区域中，因此批模式处理是基于矢量的</p><p>当在批模式下执行查询并且查询访问列存储索引中的数据时，执行树运算符和子运算符会一次读取列段中的多行。 SQL Server 仅读取结果所需的列，即 SELECT 语句、JOIN 谓词或筛选谓词引用的列</p><p><strong>批执行模式VS行执行模式的优势</strong>：一次读取多行，再筛选。避免了行执行模式的多次读取，提高运行效率  </p><p>##SQL 语句处理  </p><p>处理单个 Transact-SQL 语句是 SQL Server 执行 Transact-SQL 语句的最基本方法    </p><p>###逻辑运算符优先级  </p><p>计算顺序依次为：NOT、AND最后是 OR。算术运算符和位运算符优先于逻辑运算符处理  </p><p>##优化 SELECT 语句    </p><p>语句 SELECT 是非过程性的，数据库服务器必须分析语句，以决定提取所请求数据的最有效方法。  </p><p>执行此操作的组件称为查询优化器，可以使数据库服务器针对数据库内的更改情况进行动态调整，而无需程序员或数据库管理员输入  </p><p>order 排序字段上应该建索引  </p><p>从潜在的多个可能的计划中选择一个执行计划的过程称为“优化”  </p><p>SQL Server 查询优化器是基于成本的优化器（CBO）</p><p>当执行复杂的SQL语句时，不会去分析所有的执行计划成本，会根据算法选一个执行计划，其成本合理地接近最低可能成本的执行计划    </p><p><strong>除了CBO，还要考虑运行效率</strong>：SQL Server查询优化器不会仅选择资源成本最低的执行计划;它选择以合理的资源成本向用户返回结果的计划，并且以最快的速度返回结果  </p><p>SQL Server 查询优化器总能针对数据库的状态生成一个有效的执行计划    </p><p>SQL Server Management Studio 有三个选项可用于显示执行计划</p><pre><code>1.估计的执行计划    2.实际执行计划    3.实时查询统计信息，这与编译的计划及其执行上下文相同      这包括执行过程中的运行时信息，每秒更新一次</code></pre><p>##密度  </p><p>密度定义数据中存在的唯一值的分布，或给定列的重复值平均数。 密度与值的选择性成反比，密度越小，值的选择性越大  </p><p>##处理 SELECT 语句 </p><p>SQL Server 处理单个 SELECT 语句的基本步骤包括如下内容：  </p><p>1.解析select语句</p><p>2.生成查询树</p><p>3.生成执行计划，并选择合理的执行计划</p><p>4.运行执行计划  </p><p>5.返回结果</p><p>##常量折叠和表达式计算</p><p>###可折叠表达式    </p><p>仅包含常量的算术表达式  </p><p>仅包含常量的逻辑表达式  </p><p>被 SQL Server 认为可折叠的内置函数包括 CAST 和 CONVERT   </p><p>CLR 用户定义类型的确定性方法和确定性的标量值 CLR 用户定义函数  </p><p>###不可折叠的表达式    </p><p>所有其他表达式类型都是不可折叠的。 特别是下列类型的表达式是不可折叠的：  </p><p>非常量表达式    </p><p>结果取决于局部变量或参数的表达式   </p><p>不确定性函数     </p><p>用户定义的 Transact-SQL 函数  </p><p>结果取决于语言设置的表达式   </p><p>结果取决于 SET 选项的表达式  </p><p>结果取决于服务器配置选项的表达式  </p><p>##常量折叠的优点  </p><p>表达式不必在运行时重复计算  </p><p>查询优化器可使用计算表达式后所得的值来估计 TotalDue &gt; 117.00 + 1000.00 查询部分的结果集的大小  </p><p>##工作表<br>工作表是用于保存中间结果的内部表。 某些 GROUP BY、 ORDER BY或 UNION 查询会生成工作表  </p><p>工作表在 tempdb 中生成，并在不再需要时自动删除  </p><p>SQL Server 查询处理器对索引视图和非索引视图将区别对待  </p><p>索引视图的行以表的格式存储在数据库中   </p><p>只有非索引视图的定义才存储，而不存储视图的行  </p><p> 如果索引视图中的数据包括所有或部分 Transact-SQL 语句，而且查询优化器确定视图的某个索引是低成本的访问路径，则不论查询中是否引用了该视图的名称，查询优化器都将选择此索引  </p><p>视图没有单独的执行计划</p><h2 id="存储过程和触发器执行"><a href="#存储过程和触发器执行" class="headerlink" title="存储过程和触发器执行"></a>存储过程和触发器执行</h2><p>SQL Server 仅存储存储过程和触发器的源  </p><p>第一次执行存储过程或触发器时，源被编译为执行计划，在内存中被释放后需要重新运行存储过程再次生成  </p><p>执行计划缓存和重用  </p><p>SQL Server 有一个用于存储执行计划和数据缓冲区的内存池，池内分配给执行计划或数据缓冲区的百分比随系统状态动态波动    </p><p>计划缓存有两个不用于存储计划的附加存储：  </p><p>“对象计划”缓存存储 (OBJCP)  </p><pre><code>用于与持久化对象（存储过程、函数和触发器）相关的计划   </code></pre><p>“SQL 计划”缓存存储 (SQLCP)   </p><pre><code>用于与自动参数化、动态或已准备的查询相关的计划  </code></pre><p>##从计划缓存中删除执行计划   </p><p>只要计划缓存中有足够的存储空间，执行计划就会保留在其中   </p><p>当存在内存不足的情况时，SQL Server 数据库引擎将使用基于开销的方法来确定从计划缓存中删除哪些执行计划  </p>]]></content:encoded>
      
      
      
      
      <comments>http://example.com/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Hive on mr调优</title>
      <link>http://example.com/2023/07/31/Hive-on-mr/</link>
      <guid>http://example.com/2023/07/31/Hive-on-mr/</guid>
      <pubDate>Mon, 31 Jul 2023 07:57:39 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;#我于人间全无敌，不与天战与shui战？  &lt;/p&gt;
&lt;p&gt;本文测试数据和Explain可视化工具资料包&lt;br&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1Q8Zt7gWBF6JkW_Kg6sIWzw&quot;&gt;https://pan.baidu.co</description>
        
      
      
      
      <content:encoded><![CDATA[<p>#我于人间全无敌，不与天战与shui战？  </p><p>本文测试数据和Explain可视化工具资料包<br>链接：<a href="https://pan.baidu.com/s/1Q8Zt7gWBF6JkW_Kg6sIWzw">https://pan.baidu.com/s/1Q8Zt7gWBF6JkW_Kg6sIWzw</a><br>提取码：2khx     </p><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1Wdc2f38csrGbC3GA1s0H_w">https://pan.baidu.com/s/1Wdc2f38csrGbC3GA1s0H_w</a><br>提取码：888i   </p><p>永久有效，失效来打我~<br><img src="/2023/07/31/Hive-on-mr/1.gif"></p><p>Hive on mr  </p><p>即Hive引擎选用mapreduce。（目前Hive引擎可选项为Mapreduce&#x2F;Tez&#x2F;Spark）  </p><p>调优主要分为下面三个方向 </p><p>1)：组件资源调优  </p><p>通过控制任务运行的组件资源，实现任务的高效运行</p><p>2)：Explain执行计划调优  </p><p>通过优化执行计划,保证相同资源配置的情况下，任务运行更流畅  </p><p>3): 常有调优参数设置  </p><p>开启Hive内置的一些有助于任务高效运行的设置,保障任务流畅运行  </p><p>##1:组件资源调优</p><p>###Yarn资源配置  </p><p>需要调整的Yarn参数均与CPU、内存等资源有关，核心配置参数如下  </p><p>（1）yarn.nodemanager.resource.memory-mb  </p><p>该参数的含义是，一个NodeManager节点分配给Container使用的内存。该参数的配置，<strong>取决于NodeManager所在节点的总内存容量和该节点运行的其他服务的数量</strong>。  </p><p>（2）yarn.nodemanager.resource.cpu-vcores  </p><p>该参数的含义是，一个NodeManager节点分配给Container使用的CPU核数。该参数的配置，<strong>同样取决于NodeManager所在节点的总CPU核数和该节点运行的其他服务</strong>。</p><p>通常是一个核4个G  </p><pre><code>即（1）yarn.nodemanager.resource.memory-mb/（2）yarn.nodemanager.resource.cpu-vcores  = 4  </code></pre><p>（3）yarn.scheduler.maximum-allocation-mb  </p><p>该参数的含义是，单个Container能够使用的最大内存</p><pre><code>（1）yarn.nodemanager.resource.memory-mb /（3）yarn.scheduler.maximum-allocation-mb  = 整数</code></pre><p>（4）yarn.scheduler.minimum-allocation-mb  </p><p>该参数的含义是，单个Container能够使用的最小内存，推荐配置(512M)如下：  </p><pre><code>&lt;property&gt;    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;    &lt;value&gt;512&lt;/value&gt;&lt;/property&gt;</code></pre><p>###MapReduce资源配置  </p><p>MapReduce资源配置主要包括Map Task的内存和CPU核数，以及Reduce Task的内存和CPU核数  </p><p>1）mapreduce.map.memory.mb  </p><p>该参数的含义是，单个Map Task申请的container容器内存大小，其默认值为1024。该值不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围  </p><p>该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置： </p><pre><code>set  mapreduce.map.memory.mb=2048;  </code></pre><p>2）mapreduce.map.cpu.vcores  </p><p>该参数的含义是，单个Map Task申请的container容器cpu核数，其默认值为1。该值一般无需调整</p><p>3）mapreduce.reduce.memory.mb</p><p>该参数的含义是，单个Reduce Task申请的container容器内存大小，其默认值为1024。该值同样不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围  </p><p>该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置  </p><pre><code>set  mapreduce.reduce.memory.mb=2048;  </code></pre><p>4）mapreduce.reduce.cpu.vcores  </p><p>该参数的含义是，单个Reduce Task申请的container容器cpu核数，其默认值为1。该值一般无需调整  </p><p>#2.Explain执行计划调优 </p><p>##测试用表</p><p>###1.订单表(2000w条数据)</p><p>####建表语句</p><pre><code>hive (default)&gt;drop table if exists order_detail;create table order_detail(id           string comment &#39;订单id&#39;,user_id      string comment &#39;用户id&#39;,product_id   string comment &#39;商品id&#39;,province_id  string comment &#39;省份id&#39;,create_time  string comment &#39;下单时间&#39;,product_num  int comment &#39;商品件数&#39;,total_amount decimal(16, 2) comment &#39;下单金额&#39;)partitioned by (dt string)row format delimited fields terminated by &#39;\t&#39;;</code></pre><p>####数据装载  </p><p>将order_detail.txt文件上传到hiveserver2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。</p><p>注：文件较大，请耐心等待。  </p><pre><code>hive (default)&gt; load data local inpath &#39;/opt/module/hive/datas/order_detail.txt&#39; overwrite into table order_detail partition(dt=&#39;2020-06-14&#39;); </code></pre><p>###2.支付表(600w条数据)</p><p>####建表语句</p><pre><code>hive (default)&gt;drop table if exists payment_detail;create table payment_detail(id              string comment &#39;支付id&#39;,order_detail_id string comment &#39;订单明细id&#39;,user_id         string comment &#39;用户id&#39;,payment_time    string comment &#39;支付时间&#39;,total_amount    decimal(16, 2) comment &#39;支付金额&#39;)partitioned by (dt string)row format delimited fields terminated by &#39;\t&#39;;</code></pre><p>####数据装载  </p><p>将payment_detail.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。</p><p>注：文件较大，请耐心等待。  </p><pre><code>hive (default)&gt; load data local inpath &#39;/opt/module/hive/datas/payment_detail.txt&#39; overwrite into table payment_detail partition(dt=&#39;2020-06-14&#39;);  </code></pre><p>###3.商品信息表(100w条数据)  </p><p>####建表语句    </p><pre><code>hive (default)&gt; drop table if exists product_info;create table product_info(id           string comment &#39;商品id&#39;,product_name string comment &#39;商品名称&#39;,price        decimal(16, 2) comment &#39;价格&#39;,category_id  string comment &#39;分类id&#39;)row format delimited fields terminated by &#39;\t&#39;;  </code></pre><p>####数据装载    </p><p>将product_info.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。  </p><pre><code>hive (default)&gt; load data local inpath &#39;/opt/module/hive/datas/product_info.txt&#39; overwrite into table product_info;  </code></pre><p>###4.省份信息表(34条数据)    </p><p>####建表语句    </p><pre><code>hive (default)&gt; drop table if exists province_info;create table province_info(id            string comment &#39;省份id&#39;,province_name string comment &#39;省份名称&#39;)row format delimited fields terminated by &#39;\t&#39;;  </code></pre><p>####数据装载  </p><p>将province_info.txt文件上传到HiveServer2所在节点的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;目录，并执行以下导入语句。  </p><pre><code>hive (default)&gt; load data local inpath &#39;/opt/module/hive/datas/province_info.txt&#39; overwrite into table province_info;  </code></pre><p>##Explain查看执行计划（重点）    </p><p>Explain呈现的执行计划，由一系列Stage组成，这一系列Stage具有依赖关系，每个Stage对应一个MapReduce Job，或者一个文件系统操作等。  </p><p>若某个Stage对应的一个MapReduce Job，其Map端和Reduce端的计算逻辑分别由Map Operator Tree和Reduce Operator Tree进行描述，Operator Tree由一系列的Operator组成，一个Operator代表在Map或Reduce阶段的一个单一的逻辑操作，例如TableScan Operator，Select Operator，Join Operator等</p><p>###常见的Operator及其作用如下：  </p><p>TableScan：表扫描操作，通常map端第一个操作肯定是表扫描操作  </p><p>Select Operator：选取操作   </p><p>Group By Operator：分组聚合操作  </p><p>Reduce Output Operator：输出到 reduce 操作  </p><p>Filter Operator：过滤操作  </p><p>Join Operator：join 操作  </p><p>File Output Operator：文件输出操作  </p><p>Fetch Operator 客户端获取数据操作  </p><p>###Explain查看执行计划基本语法  </p><p>EXPLAIN [FORMATTED | EXTENDED | DEPENDENCY] query-sql  </p><pre><code>FORMATTED：将执行计划以JSON字符串的形式输出  EXTENDED：输出执行计划中的额外信息，通常是读写的文件名等信息  DEPENDENCY：输出执行计划读取的表及分区  </code></pre><p>###Explain执行计划可视化工具使用方法  </p><p>1.将本文开头百度网盘共享的dist文件压缩包上传至linux服务器中  </p><p>2.unzip dist.zip 解压  </p><p>3.进入解压后的dist文件夹下  </p><p>4.python -m SimpleHTTPServer 8901  启动可视化工具（此处的8901是我指定的可用端口号，可以按自己的想法设置）<br><img src="/2023/07/31/Hive-on-mr/1.png" alt="Explain可视化工具">  </p><p>5.将Explain执行计划制作成json格式粘贴到可视化工具里即可查看  </p><p><img src="/2023/07/31/Hive-on-mr/2.png" alt="Explain执行计划json格式">  </p><p>6.Explain可视化工具示例  </p><p><img src="/2023/07/31/Hive-on-mr/3.png" alt="示例">  </p><p>##HQL语法优化之分组聚合优化 （map-site） </p><p>Hive对分组聚合的优化主要围绕着减少Shuffle数据量进行，具体做法是map-side聚合  </p><p>在map端维护一个hash table进行预聚合，按照分组字段分区，发送至reduce端，完成最终的聚合。有效的减少shuffle操作的数量，达到提高运行效率的目的。  </p><p>###map-side 聚合相关的参数如下：</p><p>–启用map-side聚合 </p><pre><code>set hive.map.aggr=true;  </code></pre><p>–用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。  </p><pre><code>set hive.map.aggr.hash.min.reduction=0.5;  </code></pre><p>–用于检测源表是否适合map-side聚合的条数  </p><pre><code>set hive.groupby.mapaggr.checkinterval=100000;  </code></pre><p>–map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。  </p><pre><code>set hive.map.aggr.hash.force.flush.memory.threshold=0.9;  </code></pre><p>###HQL语法优化之分组聚合优化 （map-site）优化案例  </p><pre><code>selectproduct_id,count(*)from order_detailgroup by product_id;  </code></pre><p><img src="/2023/07/31/Hive-on-mr/4.png" alt="启用map-site调优前后Explain执行计划对比">  </p><p>####优化思路   </p><p>开启map-side聚合，配置以下参数：  </p><pre><code>set hive.map.aggr=true;  set hive.map.aggr.hash.min.reduction=0.5;   set hive.groupby.mapaggr.checkinterval=100000;  set hive.map.aggr.hash.force.flush.memory.threshold=0.9;  </code></pre><p>###HQL语法优化之Join优化  </p><p>Join算法概述</p><p>Common Join : 常规join，不做优化    </p><p><img src="/2023/07/31/Hive-on-mr/5.png" alt="Common Join原理图">  </p><p>Map Join：<strong>适用于大表join小表</strong>，将小表数据缓存为hash table（内存表），然后扫描大表数据，这样在map端即可完成关联操作  </p><p><img src="/2023/07/31/Hive-on-mr/6.png" alt="Map Join原理图"></p><p>Bucket Map Join：<strong>适用于大表join大表</strong>  通过分桶对数据进行切分，让有限的内存缓存一部分分桶数据，再对另一个大表进行遍历操作     </p><pre><code> Bucket Map Join的使用要求：若能保证参与**join的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍**</code></pre><p><img src="/2023/07/31/Hive-on-mr/7.png" alt="Bucket Map Join原理图"></p><p>SMB Map Join：<strong>适用于大表join大表</strong>，两个分桶之间的join实现原理为Sort Merge Join算法。前提条件是两个大表分桶数据都要排好序，这样就无需缓存在内存中，通过Sort Merge Join算法直接完成逐条遍历计算。  </p><pre><code>SMB Map Join的使用要求:参与join的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍    </code></pre><p><img src="/2023/07/31/Hive-on-mr/8.png" alt="SMB Map Join原理图"></p><p>Bucket Map Join 和 SMB Map Join 的区别：  </p><p>1.SMB Map Join在Bucket Map join的基础上，要求分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段</p><p>2.两个分桶之间的join实现算法不一样  </p><pre><code>Bucket Map Join，两个分桶之间的join实现原理为Hash Join算法    SMB Map Join，两个分桶之间的join实现原理为Sort Merge Join算法   </code></pre><p>3.相较于Bucket Map Join，SMB Map Join对分桶大小是没有要求的,因为SMB Map Join不需要缓存数据  </p><p>####Map Join  </p><p>优化说明 </p><p><img src="/2023/07/31/Hive-on-mr/9.png" alt="Map join原理解析">   </p><p>寻找大表候选人：  </p><p>a inner join b时，a,b都可以作为大表候选人，只返回a,b都能连接上的数据  </p><p>A Left join  b时，只有b才可以作为大表候选人，这样才会遍历A表数据，输出A的所有数据  </p><p>A right join b时，只有A才可以作为大表候选人，这样才会遍历B表的数据，输出B的所有数据  </p><p>A full join b时，没有大表候选人，因为无论选a还是B作为大表候选人，都无法输出a和b的所有数据  </p><p>Conditionaltask：条件任务  </p><p>图中涉及到的参数如下：    </p><p>–启动Map Join自动转换  </p><pre><code>set hive.auto.convert.join=true;  </code></pre><p>–一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的已知大小总和&lt;&#x3D;该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。 </p><pre><code>set hive.mapjoin.smalltable.filesize=250000;    </code></pre><p>注意此处的内存大小参数与实际读取磁盘文件的大小是有差别的，考虑到磁盘文件解压缩，反序列化和对象信息，相同文件在内存中要比在磁盘中占用的空间放大大概10倍  </p><p>所以该参数设置为1G，就表明拿取磁盘中1G大小的文件，但内存需要占用10G  </p><p>实际生产中，将参数配置到hive-site等配置文件中。设置size参数时，通常配置为map端内存的1&#x2F;2 ~2&#x2F;3范围内作为缓存，记得size的值应该是map_memory*2&#x2F;3 的十分之一大小才行，否则磁盘文件读取到内存，会oom  </p><p>生产中，配置文件中的调优参数生效后，大部分sql语句性能提高了，如果极少部分任务还是慢sql，就需要单独调优，在语句中加入set参数的方式进行针对性局部调优  </p><p>–开启无条件转Map Join</p><pre><code>set hive.auto.convert.join.noconditionaltask=true;  </code></pre><p>–无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和&lt;&#x3D;该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。  </p><pre><code>set hive.auto.convert.join.noconditionaltask.size=10000000;  </code></pre><p>#####Map Join优化案例  </p><pre><code>select  *from order_detail odjoin product_info product on od.product_id = product.idjoin province_info province on od.province_id = province.id;  </code></pre><p>优化前：设置 set hive.auto.convert.join&#x3D;true &#x3D; false  </p><p><img src="/2023/07/31/Hive-on-mr/10.png" alt="Map Join优化前">    </p><p>对参与关联的三张表进行分析，发现各自大小如下   </p><p><img src="/2023/07/31/Hive-on-mr/11.png" alt="参与关联的三张表大小"></p><p>######Map Join方案一：  </p><p>启用Map Join自动转换 </p><pre><code>set hive.auto.convert.join=true;  </code></pre><p>不使用无条件转Map Join  </p><pre><code>set hive.auto.convert.join.noconditionaltask=false;  </code></pre><p>调整hive.mapjoin.smalltable.filesize参数，使其大于等于product_info  </p><pre><code>set hive.mapjoin.smalltable.filesize=25285707;  </code></pre><p>这样可保证将两个Common Join operator均可转为Map Join operator，并保留Common Join作为后备计划，保证计算任务的稳定  </p><p><img src="/2023/07/31/Hive-on-mr/12.png" alt="Map Jion优化方案一">  </p><p>######Map Join方案二： </p><p>启用Map Join自动转换  </p><pre><code>set hive.auto.convert.join=true;  </code></pre><p>使用无条件转Map Join  </p><pre><code>set hive.auto.convert.join.noconditionaltask=true;  </code></pre><p>调整hive.auto.convert.join.noconditionaltask.size参数，使其大于等于product_info和province_info之和  </p><pre><code>set hive.auto.convert.join.noconditionaltask.size=25286076;  </code></pre><p>这样可直接将两个Common Join operator转为两个Map Join operator，并且由于两个Map Join operator的小表大小之和小于等于hive.auto.convert.join.noconditionaltask.size，故两个Map Join operator任务可合并为同一个。这个方案计算效率最高，但需要的内存也是最多的</p><p><img src="/2023/07/31/Hive-on-mr/13.png" alt="Map Jion优化方案二"> </p><p>######Map Join方案三：  </p><p>启用Map Join自动转换    </p><pre><code>set hive.auto.convert.join=true;   </code></pre><p>使用无条件转Map Join  </p><pre><code>set hive.auto.convert.join.noconditionaltask=true;    </code></pre><p>调整hive.auto.convert.join.noconditionaltask.size参数，使其等于product_info</p><pre><code>set hive.auto.convert.join.noconditionaltask.size=25285707;  </code></pre><p>这样可直接将两个Common Join operator转为Map Join operator，但不会将两个Map Join的任务合并。该方案计算效率比方案二低，但需要的内存也更少</p><p><img src="/2023/07/31/Hive-on-mr/14.png" alt="Map Join优化方案三"></p><p>####Bucket Map Join  </p><p>Bucket Map Join不支持自动转换，发须通过用户在SQL语句中提供如下Hint提示，并配置如下相关参数，方可使用  </p><pre><code>select /*+ mapjoin(ta) */ta.id,tb.idfrom table_a tajoin table_b tb on ta.id=tb.id; </code></pre><p>相关参数  </p><p>–关闭cbo优化，cbo会导致hint信息被忽略  </p><pre><code>set hive.cbo.enable=false;</code></pre><p>–map join hint默认会被忽略(因为已经过时)，需将如下参数设置为false  </p><pre><code>set hive.ignore.mapjoin.hint=false;  </code></pre><p>–启用bucket map join优化功能  </p><pre><code>set hive.optimize.bucketmapjoin = true;    </code></pre><p>#####Bucket Map Join优化案例   </p><pre><code>select    *from(    select        *    from order_detail    where dt=&#39;2020-06-14&#39;)odjoin(    select        *    from payment_detail    where dt=&#39;2020-06-14&#39;)pdon od.id=pd.order_detail_id;  </code></pre><p>######Bucket Map Join优化前    </p><pre><code>set hive.auto.convert.join=false;  </code></pre><p><img src="/2023/07/31/Hive-on-mr/15.png" alt="Bucket Map Join优化前"></p><p>######Bucket Map Join优化思路<br>经分析，参与join的两张表，数据量如下  </p><p>order_detail  1176009934（约1122M）<br>payment_detail  334198480（约319M）  </p><p>可以认为是大表join大表，可尝试采用Bucket Map Join优化方案  </p><p>首先需要依据源表创建两个分桶表，order_detail建议分16个bucket  </p><p>payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段  </p><p>–订单表 </p><pre><code>hive (default)&gt; drop table if exists order_detail_bucketed;create table order_detail_bucketed(id           string comment &#39;订单id&#39;,user_id      string comment &#39;用户id&#39;,product_id   string comment &#39;商品id&#39;,province_id  string comment &#39;省份id&#39;,create_time  string comment &#39;下单时间&#39;,product_num  int comment &#39;商品件数&#39;,total_amount decimal(16, 2) comment &#39;下单金额&#39;)clustered by (id) into 16 bucketsrow format delimited fields terminated by &#39;\t&#39;;</code></pre><p>–支付表  </p><pre><code>hive (default)&gt; drop table if exists payment_detail_bucketed;create table payment_detail_bucketed(id              string comment &#39;支付id&#39;,order_detail_id string comment &#39;订单明细id&#39;,user_id         string comment &#39;用户id&#39;,payment_time    string comment &#39;支付时间&#39;,total_amount    decimal(16, 2) comment &#39;支付金额&#39;)clustered by (order_detail_id) into 8 bucketsrow format delimited fields terminated by &#39;\t&#39;;  </code></pre><p>然后向两个分桶表导入数据。  </p><p>–订单表  </p><pre><code>hive (default)&gt; insert overwrite table order_detail_bucketedselectid,user_id,product_id,province_id,create_time,product_num,total_amount   from order_detailwhere dt=&#39;2023-07-28&#39;;</code></pre><p>–分桶表  </p><pre><code>hive (default)&gt; insert overwrite table payment_detail_bucketedselectid,order_detail_id,user_id,payment_time,total_amountfrom payment_detailwhere dt=&#39;2020-07-28&#39;;</code></pre><p>然后设置以下参数：  </p><p>–关闭cbo优化，cbo会导致hint信息被忽略，需将如下参数修改为false  </p><pre><code>set hive.cbo.enable=false;  </code></pre><p>–map join hint默认会被忽略(因为已经过时)，需将如下参数修改为false  </p><pre><code>set hive.ignore.mapjoin.hint=false;  </code></pre><p>–启用bucket map join优化功能,默认不启用，需将如下参数修改为true </p><pre><code>set hive.optimize.bucketmapjoin = true;</code></pre><p>最后在重写SQL语句，如下：  </p><pre><code>select /*+ mapjoin(pd) */    *from order_detail_bucketed odjoin payment_detail_bucketed pd on od.id = pd.order_detail_id; </code></pre><p><img src="/2023/07/31/Hive-on-mr/16.png">  </p><p>由于bucket map join和map join的执行计划非常像，如何确定该执行计划是否属于bucket map join ? </p><p><img src="/2023/07/31/Hive-on-mr/17.png"></p><p>####Sort Merge Bucket Map Join   </p><p>优化说明  </p><p>Sort Merge Bucket Map Join有两种触发方式，包括Hint提示和自动转换。Hint提示已过时，不推荐使用。下面是自动转换的相关参数:  </p><p>–启动Sort Merge Bucket Map Join优化  </p><pre><code>set hive.optimize.bucketmapjoin.sortedmerge=true;  </code></pre><p>–使用自动转换SMB Join  </p><pre><code>set hive.auto.convert.sortmerge.join=true;   </code></pre><p>#####Sort Merge Bucket Map Join优化案例  </p><pre><code>select        *from(    select            *    from order_detail    where dt=&#39;2020-06-14&#39;)odjoin(    select        *    from payment_detail    where dt=&#39;2020-06-14&#39;)pdon od.id=pd.order_detail_id;  </code></pre><p>#####Sort Merge Bucket Map Join优化思路<br>order_detail1176009934（约1122M）<br>payment_detail334198480（约319M）</p><p>两张表都相对较大，除了可以考虑采用Bucket Map Join算法，还可以考虑SMB Join。相较于Bucket Map Join，SMB Map Join对分桶大小是没有要求的    </p><p>首先需要依据源表创建两个的有序的分桶表，order_detail建议分16个bucket，payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段和排序字段</p><p>–订单表  </p><pre><code>hive (default)&gt; drop table if exists order_detail_sorted_bucketed;create table order_detail_sorted_bucketed(id           string comment &#39;订单id&#39;,user_id      string comment &#39;用户id&#39;,product_id   string comment &#39;商品id&#39;,province_id  string comment &#39;省份id&#39;,create_time  string comment &#39;下单时间&#39;,product_num  int comment &#39;商品件数&#39;,total_amount decimal(16, 2) comment &#39;下单金额&#39;)clustered by (id) sorted by(id) into 16 bucketsrow format delimited fields terminated by &#39;\t&#39;;</code></pre><p>–支付表  </p><pre><code>hive (default)&gt; drop table if exists payment_detail_sorted_bucketed;create table payment_detail_sorted_bucketed(id              string comment &#39;支付id&#39;,order_detail_id string comment &#39;订单明细id&#39;,user_id         string comment &#39;用户id&#39;,payment_time    string comment &#39;支付时间&#39;,total_amount    decimal(16, 2) comment &#39;支付金额&#39;)clustered by (order_detail_id) sorted by(order_detail_id) into 8 bucketsrow format delimited fields terminated by &#39;\t&#39;;  </code></pre><p>然后向两个分桶表导入数据。  </p><p>–订单表  </p><pre><code>hive (default)&gt; insert overwrite table order_detail_sorted_bucketedselectid,user_id,product_id,province_id,create_time,product_num,total_amount   from order_detailwhere dt=&#39;2023-07-28&#39;;</code></pre><p>–分桶表  </p><pre><code>hive (default)&gt; insert overwrite table payment_detail_sorted_bucketedselectid,order_detail_id,user_id,payment_time,total_amountfrom payment_detailwhere dt=&#39;2023-07-28&#39;;  </code></pre><p>–启动Sort Merge Bucket Map Join优化  </p><pre><code>set hive.optimize.bucketmapjoin.sortedmerge=true;  </code></pre><p>–使用自动转换SMB Join  </p><pre><code>set hive.auto.convert.sortmerge.join=true; </code></pre><p>最后在重写SQL语句，如下：  </p><pre><code>hive (default)&gt; select    *from order_detail_sorted_bucketed odjoin payment_detail_sorted_bucketed pdon od.id = pd.order_detail_id;  </code></pre><p><img src="/2023/07/31/Hive-on-mr/18.png">  </p><p>###HQL语法优化之数据倾斜    </p><p>数据倾斜概述</p><p>数据倾斜问题，通常是指参与计算的数据分布不均，即某个key或者某些key的数据量远超其他key，导致在shuffle阶段，大量相同key的数据被发往同一个Reduce，进而导致该Reduce所需的时间远超其他Reduce，成为整个任务的瓶颈</p><p>Hive中的数据倾斜常出现在<strong>分组聚合</strong>和<strong>join操作</strong>的场景中  </p><p>####分组聚合导致的数据倾斜 （Map-Side聚合&#x2F;Skew-GroupBy优化）</p><p>如果group by分组字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题   </p><p>由分组聚合导致的数据倾斜问题，有以下两种解决思路    </p><p>#####Map-Side聚合   </p><p>开启Map-Side聚合后，数据会现在Map端完成部分聚合工作。这样一来即便原始数据是倾斜的，经过Map端的初步聚合后，发往Reduce的数据也就不再倾斜了，最佳状态下，Map-端聚合能完全屏蔽数据倾斜问题。  </p><p>相关参数如下：  </p><pre><code>set hive.map.aggr=true;set hive.map.aggr.hash.min.reduction=0.5;  set hive.groupby.mapaggr.checkinterval=100000;  set hive.map.aggr.hash.force.flush.memory.threshold=0.9;   </code></pre><p>#####Skew-GroupBy优化 </p><p>Skew-GroupBy的原理是启动两个MR任务，第一个MR按照随机数分区，将数据分散发送到Reduce，完成部分聚合，第二个MR按照分组字段分区，完成最终聚合  </p><p>–启用分组聚合数据倾斜优化  </p><pre><code>set hive.groupby.skewindata=true;  </code></pre><p>–关闭map-side聚合  </p><pre><code>set hive.map.aggr=false;    </code></pre><p>####Join导致的数据倾斜    </p><p>如果关联字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题。  </p><p>由join导致的数据倾斜问题，有如下三种解决方案：<br>map join<br>skew join<br>调整sql，通过sql语句将倾斜数据打散成更小的块  </p><p>#####map join（<strong>适用于大表join小表时发生数据倾斜的场景</strong>）      </p><p> 使用map join算法，join操作仅在map端就能完成，没有shuffle操作，没有reduce阶段，自然不会产生reduce端的数据倾斜     </p><p>相关参数参照上文中map join部分内容  </p><pre><code>set hive.auto.convert.join=true;  set hive.mapjoin.smalltable.filesize=250000;  set hive.auto.convert.join.noconditionaltask=true;set hive.auto.convert.join.noconditionaltask.size=10000000;</code></pre><p>#####skew join（<strong>对两表中倾斜的key的数据量有要求</strong>）   </p><p>skew join的原理是，为倾斜的大key单独启动一个map join任务进行计算，其余key进行正常的common join  </p><p><img src="/2023/07/31/Hive-on-mr/19.png" alt="Skew Join原理图">  </p><p>相关参数如下：  </p><p>–启用skew join优化  </p><pre><code>set hive.optimize.skewjoin=true;  </code></pre><p>–触发skew join的阈值，若某个key的行数超过该参数值，则触发  </p><pre><code>set hive.skewjoin.key=100000;    </code></pre><p>对两表中倾斜的key的数据量有要求，要求一张表中的倾斜key的数据量比较小（方便走mapjoin）  </p><p>#####SQL打散    </p><pre><code>select    *from(    select --打散操作    concat(id,&#39;_&#39;,cast(rand()*2 as int)) id,    valuefrom A)tajoin(    select --扩容操作        concat(id,&#39;_&#39;,0) id,        value    from B    union all    select        concat(id,&#39;_&#39;,1) id,           value    from B)tbon ta.id=tb.id;  </code></pre><p><img src="/2023/07/31/Hive-on-mr/20.png" alt="SQL打散"></p><p>####HQL语法优化之任务并行度  </p><p>对于一个分布式的计算任务而言，设置一个合适的并行度十分重要。Hive的计算任务由MapReduce完成，故并行度的调整需要分为Map端和Reduce端</p><p>#####Map端并行度  </p><p>Map端的并行度，也就是Map的个数。是由输入文件的切片数决定的。一般情况下，Map端的并行度无需手动调整  </p><p>以下特殊情况可考虑调整map端并行度：  </p><p>1）查询的表中存在大量小文件    </p><pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  </code></pre><p>2）map端有复杂的查询逻辑  </p><p>在计算资源充足的情况下，可考虑增大map端的并行度，令map task多一些，每个map task计算的数据少一些  </p><p>–一个切片的最大值  </p><pre><code>set mapreduce.input.fileinputformat.split.maxsize=256000000;  </code></pre><p>#####Reduce端并行度    </p><p>Reduce端的并行度，也就是Reduce个数。相对来说，更需要关注。Reduce端的并行度，可由用户自己指定，也可由Hive自行根据该MR Job输入的文件大小进行估算    </p><p>Reduce端的并行度的相关参数如下：  </p><p>–指定Reduce端并行度，默认值为-1，表示用户未指定  </p><pre><code>set mapreduce.job.reduces;  </code></pre><p>–Reduce端并行度最大值  </p><pre><code>set hive.exec.reducers.max;  </code></pre><p>–单个Reduce Task计算的数据量，用于估算Reduce并行度  </p><pre><code>set hive.exec.reducers.bytes.per.reducer;</code></pre><p>######估算逻辑   </p><p>假设Job输入的文件大小为totalInputBytes  </p><p>参数hive.exec.reducers.bytes.per.reducer的值为bytesPerReducer。  </p><p>参数hive.exec.reducers.max的值为maxReducers。  </p><p>则Reduce端的并行度为：  </p><pre><code>min(ceil(totalInputBytes/bytesPerReducer),maxReducers)  </code></pre><p>####HQL语法优化之小文件合并    </p><p>Map端输入文件合并   </p><p>合并Map端输入的小文件，是指将多个小文件划分到一个切片中，进而由一个Map Task去处理。目的是防止为单个小文件启动一个Map Task，浪费计算资源  </p><p>–可将多个小文件切片，合并为一个切片，进而由一个map任务处理  </p><pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  </code></pre><p>Reduce输出文件合并  </p><p>合并Reduce端输出的小文件，是指将多个小文件合并成大文件。目的是减少HDFS小文件数量。其原理是根据计算任务输出文件的平均大小进行判断，若符合条件，则单独启动一个额外的任务进行合并  </p><p>–开启合并map only任务输出的小文件  </p><pre><code>set hive.merge.mapfiles=true;</code></pre><p>–开启合并map reduce任务输出的小文件  </p><pre><code>set hive.merge.mapredfiles=true;</code></pre><p>–合并后的文件大小</p><pre><code>set hive.merge.size.per.task=256000000;</code></pre><p>–触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并 </p><pre><code>set hive.merge.smallfiles.avgsize=16000000;  </code></pre><p>###其他优化</p><p>####1.CBO优化     </p><p>CBO是指Cost based Optimizer，即基于计算成本的优化</p><p>在Hive中，计算成本模型考虑到了：数据的行数、CPU、本地IO、HDFS IO、网络IO等方面  </p><p>目前CBO在hive的MR引擎下主要用于join的优化，例如多表join的join顺序  </p><p>–是否启用cbo优化   </p><pre><code>set hive.cbo.enable=true;    </code></pre><p>####2.谓词下推    </p><p>谓词下推（predicate pushdown）是指，尽量将过滤操作前移，以减少后续计算步骤的数据量</p><p>–是否启动谓词下推（predicate pushdown）优化  </p><pre><code>set hive.optimize.ppd = true;  </code></pre><p>CBO优化也会完成一部分的谓词下推优化工作，因为在执行计划中，谓词越靠前，整个计划的计算成本就会越低</p><p>####3.矢量化查询  </p><p>Hive的矢量化查询优化，依赖于CPU的矢量化计算，CPU的矢量化计算的基本原理如下图  </p><p><img src="/2023/07/31/Hive-on-mr/21.png" alt="矢量化计算原理">  </p><pre><code>set hive.vectorized.execution.enabled=true;  </code></pre><p>若执行计划中，出现“Execution mode: vectorized”字样，即表明使用了矢量化计算。  </p><p><a href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution#VectorizedQueryExecution-Limitations">矢量化计算官方文档</a> </p><p>####4.Fetch抓取    </p><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算  </p><p>Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果到控制台</p><p>–是否在特定场景转换为fetch 任务  </p><p>–设置为none表示不转换  </p><p>–设置为minimal表示支持select *，分区字段过滤，Limit等  </p><p>–设置为more表示支持select 任意字段,包括函数，过滤，和limit等  </p><pre><code>set hive.fetch.task.conversion=more;  </code></pre><p>####5.本地模式（不上yarn）  </p><p>Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短</p><p>–开启自动转换为本地模式  </p><pre><code>set hive.exec.mode.local.auto=true;  </code></pre><p>–设置local MapReduce的最大输入数据量，当输入数据量小于这个值时采用local  MapReduce的方式，默认为134217728，即128M  </p><pre><code>set hive.exec.mode.local.auto.inputbytes.max=50000000;</code></pre><p>–设置local MapReduce的最大输入文件个数，当输入文件个数小于这个值时采用local MapReduce的方式，默认为4  </p><pre><code>set hive.exec.mode.local.auto.input.files.max=10;</code></pre><p>####6.并行执行  </p><p>Hive会将一个SQL语句转化成一个或者多个Stage，每个Stage对应一个MR Job。默认情况下，Hive同时只会执行一个Stage。但是某SQL语句可能会包含多个Stage，但这多个Stage可能并非完全互相依赖，也就是说有些Stage是可以并行执行的。此处提到的并行执行就是指这些Stage的并行执行   </p><p>–启用并行执行优化  </p><pre><code>set hive.exec.parallel=true;       </code></pre><p>–同一个sql允许最大并行度，默认为8  </p><pre><code>set hive.exec.parallel.thread.number=8;   </code></pre><p>####7.严格模式   </p><p>Hive可以通过设置某些参数防止危险操作：   </p><p>1）分区表不使用分区过滤  </p><p>将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行  </p><p>2）使用order by没有limit过滤   </p><p>将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句    </p><p>3）笛卡尔积    </p><p>将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询</p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E6%88%91%E4%BA%8E%E4%BA%BA%E9%97%B4%E5%85%A8%E6%97%A0%E6%95%8C%EF%BC%8C%E4%B8%8D%E4%B8%8E%E5%A4%A9%E6%88%98%E4%B8%8E%E8%B0%81%E6%88%98%EF%BC%9F/">我于人间全无敌，不与天战与谁战？</category>
      
      
      <comments>http://example.com/2023/07/31/Hive-on-mr/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hive-udf</title>
      <link>http://example.com/2023/07/31/hive-udf/</link>
      <guid>http://example.com/2023/07/31/hive-udf/</guid>
      <pubDate>Mon, 31 Jul 2023 03:40:34 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;img src=&quot;/2023/07/31/hive-udf/2.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;#Hive自定义UDF函数案例    &lt;/p&gt;
&lt;p&gt;##0）需求  &lt;/p&gt;
&lt;p&gt;自定义一个UDF实现计算给定基本数据类型的长度，例如：  &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hiv</description>
        
      
      
      
      <content:encoded><![CDATA[<p><img src="/2023/07/31/hive-udf/2.png"></p><p>#Hive自定义UDF函数案例    </p><p>##0）需求  </p><p>自定义一个UDF实现计算给定基本数据类型的长度，例如：  </p><pre><code>hive(default)&gt; select my_len(&quot;abcd&quot;);  4  </code></pre><p>##1）创建一个Maven工程Hive</p><p>##2）导入依赖</p><pre><code>&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;        &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;        &lt;version&gt;3.1.3&lt;/version&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;  </code></pre><p>##3）创建一个类  </p><pre><code>package com.atguigu.hive.udf;import org.apache.hadoop.hive.ql.exec.UDFArgumentException;import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;/* 我们需计算一个要给定基本数据类型的长度 */public class MyUDF extends GenericUDF &#123;    /*** 判断传进来的参数的类型和长度* 约定返回的数据类型*/      @Override    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123;    if (arguments.length !=1) &#123;        throw  new UDFArgumentLengthException(&quot;please give me  only one arg&quot;);    &#125;    if (!arguments[0].getCategory().equals(ObjectInspector.Category.PRIMITIVE))&#123;        throw  new UDFArgumentTypeException(1, &quot;i need primitive type arg&quot;);    &#125;    return PrimitiveObjectInspectorFactory.javaIntObjectInspector;&#125;/** * 解决具体逻辑的 */@Overridepublic Object evaluate(DeferredObject[] arguments) throws HiveException &#123;    Object o = arguments[0].get();    if(o==null)&#123;        return 0;    &#125;    return o.toString().length();&#125;@Override// 用于获取解释的字符串public String getDisplayString(String[] children) &#123;    return &quot;&quot;;&#125;&#125;</code></pre><p>##4）创建临时函数  </p><p>###（1）打成jar包上传到服务器&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;myudf.jar</p><p>###（2）将jar包添加到hive的classpath，临时生效</p><pre><code>hive (default)&gt; add jar /opt/module/hive/datas/myudf.jar;</code></pre><p>###（3）创建临时函数与开发好的java class关联</p><pre><code>hive (default)&gt; create temporary function my_len as &quot;com.atguigu.hive.udf.MyUDF&quot;;</code></pre><p>###（4）即可在hql中使用自定义的临时函数</p><pre><code>hive (default)&gt; select ename,my_len(ename) ename_len from emp;</code></pre><p>###（5）删除临时函数</p><pre><code>hive (default)&gt; drop temporary function my_len;注意：临时函数只跟会话有关系，跟库没有关系。只要创建临时函数的会话不断，在当前会话下，任意一个库都可以使用，其他会话全都不能使用。</code></pre><p>##5）创建永久函数</p><p>###（1）创建永久函数</p><p>注意：因为add jar本身也是临时生效，所以在创建永久函数的时候，需要制定路径（并且因为元数据的原因，这个路径还得是HDFS上的路径）</p><pre><code>hive (default)&gt; create function my_len2 as &quot;com.atguigu.hive.udf.MyUDF&quot; using jar &quot;hdfs://hadoop102:8020/udf/myudf.jar&quot;;</code></pre><p>###（2）即可在hql中使用自定义的永久函数 </p><pre><code>hive (default)&gt; select ename,my_len2(ename) ename_len from emp;</code></pre><p>###（3）删除永久函数 </p><pre><code>hive (default)&gt; drop function my_len2;  </code></pre><p>注意：永久函数跟会话没有关系，创建函数的会话断了以后，其他会话也可以使用。  </p><p>永久函数创建的时候，在函数名之前需要自己加上库名，如果不指定库名的话，会默认把当前库的库名给加上。  </p><p>永久函数使用的时候，需要在指定的库里面操作，或者在其他库里面使用的话加上，库名.函数名。  </p><p>#啊席八，Hive自定义UDF函数你都学会了~！</p><p>#啪啪啪~ 你很厉害喔~</p><p><img src="/2023/07/31/hive-udf/1.png" alt="歪嘴猫"></p>]]></content:encoded>
      
      
      
      
      <comments>http://example.com/2023/07/31/hive-udf/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hive学习笔记</title>
      <link>http://example.com/2023/07/30/hive_learn/</link>
      <guid>http://example.com/2023/07/30/hive_learn/</guid>
      <pubDate>Sun, 30 Jul 2023 10:56:09 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1g84y147sX?p=78&amp;vd_source=326368ccf929b51406b17a280e53c102&quot;&gt;尚硅谷大数据Hive 3.x教程全新升级版（基于hive3.1.3）&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<p><a href="https://www.bilibili.com/video/BV1g84y147sX?p=78&vd_source=326368ccf929b51406b17a280e53c102">尚硅谷大数据Hive 3.x教程全新升级版（基于hive3.1.3）</a><br>#手握日月摘星辰，世间无我这般人！  </p><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1vdjJdb5hZtWMDK6hoH1R5g">https://pan.baidu.com/s/1vdjJdb5hZtWMDK6hoH1R5g</a><br>提取码：uce2   </p><p><img src="/2023/07/30/hive_learn/8.png">  </p><p>#一：Hive的基础知识<br>##1.什么是Hive？  </p><p>Hive是由Facebook开源，基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能</p><p>##2.Hive本质  </p><p>Hive是一个Hadoop客户端，用于将HQL（Hive SQL）转化成MapReduce程序。<br>（1）Hive中每张表的数据存储在HDFS<br>（2）Hive分析数据底层的实现是MapReduce（也可配置为Spark或者Tez）<br>（3）执行程序运行在Yarn上</p><p>##3.用户接口：Client  </p><p>CLI（command-line interface）、JDBC&#x2F;ODBC  </p><p>###JDBC和ODBC的区别:  </p><p>（1）JDBC的移植性比ODBC好 </p><p>（2）两者使用的语言不同，JDBC在Java编程时使用，ODBC一般在C&#x2F;C++编程时使用  </p><p>##4:元数据：Metastore  </p><p>元数据包括：数据库（默认是default）、表名、表的拥有者、列&#x2F;分区字段、表的类型（是否是外部表）、表的数据所在目录等  </p><p>默认存储在自带的derby数据库中，由于derby数据库只支持单客户端访问，生产环境中为了多人开发，推荐使用MySQL存储Metastore  </p><p>##5.hive的存储和计算  </p><p>使用HDFS进行存储，可以选择MapReduce&#x2F;Tez&#x2F;Spark进行计算</p><p>##6.hiveserver2服务  </p><p>Hive的hiveserver2服务的作用是提供jdbc&#x2F;odbc接口，为用户提供远程访问Hive数据的功能，例如用户期望在个人电脑中访问远程服务中的Hive数据，就需要用到Hiveserver2  </p><p>##7.用户说明  </p><p>在远程访问Hive数据时，客户端并未直接访问Hadoop集群，而是由Hivesever2代理访问,那么访问Hadoop集群的用户身份是谁？  </p><p>具体是谁，由Hiveserver2的hive.server2.enable.doAs参数决定，该参数的含义是是否启用Hiveserver2用户模拟的功能  </p><p>若启用，则Hiveserver2会模拟成客户端的登录用户去访问Hadoop集群的数据，不启用，则Hivesever2会直接使用启动用户访问Hadoop集群数据  </p><p>默认为开启</p><p>生产环境，推荐开启用户模拟功能，因为开启后才能保证各用户之间的权限隔离</p><p>hivesever2的模拟用户功能，依赖于Hadoop提供的proxy user（代理用户功能），只有Hadoop中的代理用户才能模拟其他用户的身份访问Hadoop集群。因此，需要将hiveserver2的启动用户设置为Hadoop的代理用户 </p><p>##8.metastore服务  </p><p>Hive的metastore服务的作用是为Hive CLI或者Hiveserver2提供元数据访问接口  </p><p>##9.metastore运行模式  </p><p>metastore有两种运行模式，分别为嵌入式模式和独立服务模式</p><p><img src="/2023/07/30/hive_learn/1.png" alt="&quot;metastore运行模式&quot;"></p><p>生产环境中，不推荐使用嵌入式模式  </p><p>（1）嵌入式模式下，每个Hive CLI都需要直接连接元数据库，当Hive CLI较多时，数据库压力会比较大。  </p><p>（2）每个客户端都需要用户元数据库的读写权限，元数据库的安全得不到很好的保证</p><p>##10.编写Hive服务启动脚本  </p><p>nohup：放在命令开头，表示不挂起，也就是关闭终端进程也继续保持运行状态  </p><p>&#x2F;dev&#x2F;null：是Linux文件系统中的一个文件，被称为黑洞，所有写入该文件的内容都会被自动丢弃  </p><p>2&gt;&amp;1：表示将错误重定向到标准输出上  </p><p>&amp;：放在命令结尾，表示后台运行  </p><p>一般会组合使用：nohup  [xxx命令操作]&gt; file  2&gt;&amp;1 &amp;，表示将xxx命令运行的结果输出到file中，并保持命令启动的进程在后台运行。  </p><p>##11.hive -e 和 hive -f  </p><p>“-e”不进入hive的交互窗口执行hql语句  </p><p><code>bin/hive -e &quot;select id from student;&quot;</code></p><p>“-f”执行脚本中的hql语句  </p><p><code>bin/hive -f /opt/module/hive/datas/hivef.sql</code>  </p><p>##12.Hive参数配置方式  </p><p>参数的配置三种方式  </p><p>###(1).配置文件方式  </p><p><code>hive-site.xml</code></p><p>###(2).命令行参数方式  </p><p>启动Hive时，可以在命令行添加-hiveconf param&#x3D;value来设定参数  </p><p>比如：bin&#x2F;hive -hiveconf   </p><p><code>mapreduce.job.reduces=10;</code></p><p>注：仅对本次Hive启动有效  </p><p>###(3).参数声明方式  </p><p>可以在HQL中使用SET关键字设定参数  </p><p><code>set mapreduce.job.reduces=10;</code>  </p><p>上述三种设定方式的优先级依次递增,配置文件 &lt; 命令行参数 &lt; 参数声明  </p><p>##13.Hive常见属性配置  </p><p>Hive客户端显示当前库和表头<br>Set Hive-site.xml :<br>          <code>hive.cli.print.header = true</code><br>          <code>hive.cli.print.current.db</code>  </p><p>Hive运行日志路径配置<br>Set hive-log4j2.properties:<br>     <code>property.hive.log.dir=/opt/module/hive/logs</code>  </p><p>修改Hive的堆内存<br>Set hive-env.sh:<br>     <code>export HADOOP_HEAPSIZE=2048</code>  </p><p>关闭Hadoop虚拟内存检查<br>Set yarn-site.xml:<br>     <code>yarn.nodemanager.vmem-check-enabled =false</code>  </p><p>#二：Hive的DDL语法  </p><p>##1.创建数据库  </p><pre><code>CREATE DATABASE [IF NOT EXISTS] database_name  [COMMENT database_comment]  [LOCATION hdfs_path]  [WITH DBPROPERTIES (property_name=property_value, ...)];   </code></pre><p>创建一个数据库，指定路径<br>    hive (default)&gt; create database db_hive2 location ‘&#x2F;db_hive2’;  </p><p>##2.查看数据库信息  </p><pre><code>DESCRIBE DATABASE [EXTENDED] db_name;    </code></pre><p>###(1) 查看基本信息  </p><pre><code>desc database db_hive3;  </code></pre><p>###(2) 查看更多信息  </p><pre><code>desc database extended db_hive3;  </code></pre><p>##3.修改数据库   </p><p>需要注意的是：修改数据库location，不会改变当前已有表的路径信息，而只是改变后续创建的新表的默认的父目录  </p><p>###修改dbproperties<br>    ALTER DATABASE database_name SET DBPROPERTIES   (property_name&#x3D;property_value, …);  </p><p>###修改location<br>    ALTER DATABASE database_name SET LOCATION hdfs_path;  </p><p>###修改owner user<br>    ALTER DATABASE database_name SET OWNER USER user_name;  </p><p>##4.删除数据库  </p><pre><code>DROP DATABASE [IF EXISTS] database_name [RESTRICT|CASCADE];    </code></pre><p>RESTRICT：严格模式，若数据库不为空，则会删除失败，默认为该模式。  </p><p>CASCADE：级联模式，若数据库不为空，则会将库中的表一并删除。</p><p>##5.切换当前数据库  </p><p>USE database_name;  </p><p>##6.创建表  </p><pre><code>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name   [(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)][CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS][ROW FORMAT row_format] [STORED AS file_format][LOCATION hdfs_path][TBLPROPERTIES (property_name=property_value, ...)]</code></pre><p>关键字说明:<br>###(1)TEMPORARY<br>临时表，该表只在当前会话可见，会话结束，表会被删除。  </p><p>###(2)EXTERNAL（重点）  </p><p>外部表，与之相对应的是内部表（管理表）。内部表意味着Hive会完全接管该表，包括元数据和HDFS中的数据。而外部表则意味着Hive只接管元数据，而不完全接管HDFS中的数据</p><p>###(3)data_type（重点）</p><p>Hive中的字段类型可分为基本数据类型和复杂数据类型。  </p><p><img src="/2023/07/30/hive_learn/2.png" alt="&quot;hive数据类型&quot;">  </p><p>类型转换:  </p><p>Hive的基本数据类型可以做类型转换，转换的方式包括隐式转换以及显示转换。  </p><p>####方式一：隐式转换  </p><p>隐式地转换为一个范围更广的类型   </p><p>Hive官方隐式转换表<br><a href="https://cwiki.apache.org/confluence/display/hive/languagemanual+types#LanguageManualTypes-AllowedImplicitConversions">Allowed Implicit Conversions</a>  </p><p>####方式二：显示转换  </p><p>可以借助cast函数完成显示的类型转换(强制转换)  </p><pre><code>select &#39;1&#39; + 2, cast(&#39;1&#39; as int) + 2;</code></pre><p>###(4) PARTITIONED BY（重点）  </p><p>创建分区表  </p><p>###(5) CLUSTERED BY … SORTED BY…INTO … BUCKETS（重点）  </p><p>创建分桶表  </p><p>###(6) ROW FORMAT（重点）  </p><p>指定SERDE，SERDE是Serializer and Deserializer的简写。Hive使用SERDE序列化和反序列化每行数据  </p><p>Hive官方序列化反序列化器文档<br><a href="https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-HiveSerDe">Hive-Serde</a></p><p>####语法一：  </p><p>DELIMITED关键字表示对文件中的每个字段按照特定分割符进行分割，其会使用默认的SERDE对每行数据进行序列化和反序列化  </p><pre><code>ROW FORAMT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char]   </code></pre><p>注：<br>fields terminated by ：列分隔符<br>collection items terminated by ： map、struct和array中每个元素之间的分隔符<br>map keys terminated by ：map中的key与value的分隔符<br>lines terminated by ：行分隔符  </p><p>####语法二：  </p><p>SERDE关键字可用于指定其他内置的SERDE或者用户自定义的SERDE。例如JSON SERDE，可用于处理JSON字符串  </p><pre><code>ROW FORMAT SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,property_name=property_value, ...)] </code></pre><p>###(7) STORED AS（重点）  </p><p>指定文件格式，常用的文件格式有，textfile（默认值），sequence file，orc file、parquet file等等  </p><p>###(8) LOCATION  </p><p>指定表所对应的HDFS路径，若不指定路径，其默认值为<br>${hive.metastore.warehouse.dir}&#x2F;db_name.db&#x2F;table_name  </p><p>###(9) TBLPROPERTIES  </p><p>用于配置表的一些KV键值对参数  </p><p>##7.Create Table As Select（CTAS）建表  </p><pre><code>CREATE [TEMPORARY] TABLE [IF NOT EXISTS] table_name [COMMENT table_comment] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path][TBLPROPERTIES (property_name=property_value, ...)][AS select_statement]</code></pre><p>##8.Create Table Like语法</p><p>该语法允许用户复刻一张已经存在的表结构，与上述的CTAS语法不同，该语法创建出来的表中不包含数据  </p><pre><code>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name[LIKE exist_table_name][ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path][TBLPROPERTIES (property_name=property_value, ...)]</code></pre><p>##9.内部表与外部表  </p><p>Hive中默认创建的表都是的内部表，有时也被称为管理表。对于内部表，Hive会完全管理表的元数据和数据文件  </p><p>外部表通常可用于处理其他工具上传的数据文件，对于外部表，Hive只负责管理元数据，不负责管理HDFS中的数据文件  </p><p>##10.查看表  </p><pre><code>DESCRIBE [EXTENDED | FORMATTED] [db_name.]table_name  </code></pre><p>EXTENDED：展示详细信息  </p><p>FORMATTED：对详细信息进行格式化的展示</p><p>##11.修改列信息  </p><p>###增加列  </p><pre><code>ALTER TABLE table_name ADD COLUMNS (col_name data_type [COMMENT col_comment], ...)  </code></pre><p>新增列的位置位于末尾  </p><p>###更新列</p><pre><code>ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]</code></pre><p>该语句允许用户修改指定列的列名、数据类型、注释信息以及在表中的位置  </p><p>###替换列  </p><pre><code>ALTER TABLE table_name REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)  </code></pre><p>该语句允许用户用新的列集替换表中原有的全部列  </p><p>##12.清空表  </p><pre><code>TRUNCATE [TABLE] table_name  </code></pre><p>truncate只能清空管理表，不能删除外部表中数据  </p><p>#三：Hive的DML语法  </p><p>##1. Load  </p><p>Load语句可将文件导入到Hive表中    </p><pre><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)];</code></pre><p>关键字说明：  </p><p>（1）local：表示从本地加载数据到Hive表；否则从HDFS加载数据到Hive表。  </p><p>（2）overwrite：表示覆盖表中已有数据，否则表示追加。  </p><p>（3）partition：表示上传到指定分区，若目标是分区表，需指定分区    </p><p>###1.1加载本地文件到hive:  </p><pre><code>load data local inpath &#39;/opt/module/datas/student.txt&#39; into table student;  </code></pre><p>###1.2加载HDFS上数据:  </p><pre><code>hadoop fs -put /opt/module/datas/student.txt /user/atguigu;  load data inpath &#39;/user/atguigu/student.txt&#39; into table student;</code></pre><p>##2.Insert  </p><pre><code>INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement;</code></pre><p>关键字说明：  </p><p>（1）INTO：将结果追加到目标表  </p><p>（2）OVERWRITE：用结果覆盖原有数据  </p><p>###2.1 将给定Values插入表中</p><pre><code>INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...]  </code></pre><p>###2.2将查询结果写入目标路径  </p><pre><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory[ROW FORMAT row_format] [STORED AS file_format] select_statement;</code></pre><p>##3.Export&amp;Import  </p><p>Export导出语句可将表的数据和元数据信息一并到处的HDFS路径，Import可将Export导出的内容导入Hive，表的数据和元数据信息都会恢复。Export和Import可用于两个Hive实例之间的数据迁移</p><p>###导出<br>    EXPORT TABLE tablename TO ‘export_target_path’</p><p>###导入<br>    IMPORT [EXTERNAL] TABLE new_or_original_tablename FROM ‘source_path’ [LOCATION ‘import_target_path’]</p><p>#四:查询  </p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">Hive-select语法文档</a></p><p>##1.select 查询语法    </p><pre><code>SELECT [ALL | DISTINCT] select_expr, select_expr, FROM table_reference       -- 从什么表查[WHERE where_condition]   -- 过滤[GROUP BY col_list]        -- 分组查询[HAVING col_list]          -- 分组后过滤[ORDER BY col_list]        -- 排序[CLUSTER BY col_list| [DISTRIBUTE BY col_list] [SORT BY col_list]][LIMIT number]                -- 限制输出的行数</code></pre><p>注意：<br>（1）SQL 语言大小写不敏感。<br>（2）SQL 可以写在一行或者多行。<br>（3）关键字不能被缩写也不能分行。<br>（4）各子句一般要分行写。<br>（5）使用缩进提高语句的可读性    </p><p>##2.Limit语句  </p><p>典型的查询会返回多行数据。limit子句用于限制返回的行数  </p><pre><code>select * from emp limit 2,3; -- 表示从第2行开始，向下抓取3行</code></pre><p>##3.关系运算函数  </p><p>A&lt;&#x3D;&gt;B :  </p><p> 如果A和B都为null或者都不为null，则返回true，如果只有一边为null，返回false  </p><p>A [not] like B :  </p><p> B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回true；反之返回false。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母‘x’结尾，而‘%x%’表示A包含有字母‘x’,可以位于开头，结尾或者字符串中间。如果使用not关键字则可达到相反的效果。  </p><p>A rlike B, A regexp B:  </p><p>B是基于java的正则表达式，如果A与其匹配，则返回true；反之返回false。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。  </p><p>##4.聚合函数  </p><p>count(*)，表示统计所有行数，包含null值； </p><p>count(某列)，表示该列一共有多少行，不包含null值； </p><p>max()，求最大值，不包含null，除非所有值都是null； </p><p>min()，求最小值，不包含null，除非所有值都是null； </p><p>sum()，求和，不包含null。   </p><p>avg()，求平均值，不包含null。    </p><p>##5.分组<br>Group By语句：  </p><p>Group By语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</p><p>##6.Having语句  </p><p>having与where不同点  </p><p>（1）where后面不能写分组聚合函数，而having后面可以使用分组聚合函数。  </p><p>（2）having只用于group by分组统计语句    </p><p>##7.Join语句   </p><p>Hive支持通常的sql join语句，但是只支持等值连接，不支持非等值连接。  </p><p>inner join 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来  </p><p>left outer join 左外连接：join操作符左边表中符合where子句的所有记录将会被返回  </p><p>right outer join 右外连接：join操作符右边表中符合where子句的所有记录将会被返回  </p><p>full outer join : 满外连接：将会返回所有表中符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代    </p><p>多表连接: 连接n个表，至少需要n-1个连接条件    </p><p>大多数情况下，Hive会对每对join连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作</p><p>因为Hive总是按照从左到右的顺序执行的  </p><p>##8.笛卡尔积产生条件  </p><p>（1）省略连接条件  </p><p>（2）连接条件无效  </p><p>（3）所有表中的所有行互相连接  </p><p>##9.联合（union &amp; union all）  </p><p>union和union all都是上下拼接sql的结果，这点是和join有区别的，join是左右关联，union和union all是上下拼接。  </p><p>union去重，union all不去重  </p><p>union和union all在上下拼接sql结果时有两个要求：  </p><p>（1）两个sql的结果，列的个数必须相同  </p><p>（2）两个sql的结果，上下所对应列的类型必须一致    </p><p>##10.排序  </p><p>####全局排序 Order By  </p><p>全局排序，只有一个Reduce    </p><p>asc（ascend）：升序（默认）  </p><p>desc（descend）：降序  </p><p>####每个Reduce内部排序（Sort By）  </p><p>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用Sort by  </p><p>Sort by为每个reduce产生一个排序文件。每个Reduce内部进行排序，对全局结果集来说不是排序  </p><p>设置reduce个数  </p><pre><code>hive (default)&gt; set mapreduce.job.reduces=3;  </code></pre><p>查看设置reduce个数  </p><pre><code>hive (default)&gt; set mapreduce.job.reduces;  </code></pre><p>##11.分区（Distribute By）</p><p>Distribute By：在有些情况下，我们需要控制某个特定行应该到哪个Reducer，通常是为了进行后续的聚集操作。distribute by子句可以做这件事。distribute by类似MapReduce中partition（自定义分区），进行分区，结合sort by使用  </p><p>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行相除后，余数相同的分到一个区。  </p><p>Hive要求distribute by语句要写在sort by语句之前。  </p><p>演示完以后mapreduce.job.reduces的值要设置回-1，否则下面分区or分桶表load跑MapReduce的时候会报错  </p><p>##12.分区排序（Cluster By）  </p><p>当distribute by和sort by字段相同时，可以使用cluster by方式  </p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为asc或者desc      </p><p>以下两种写法等价<br>hive (default)&gt;<br>    select *<br>    from emp<br>    cluster by deptno;  </p><p>hive (default)&gt;<br>    select<br>    *<br>    from emp<br>    distribute by deptno sort by deptno asc;  </p><p>#五:函数  </p><p>Hive会将常用的逻辑封装成函数给用户进行使用，类似于Java中的函数    </p><p>Hive提供了大量的内置函数，按照其特点可大致分为如下几类：单行函数、聚合函数、炸裂函数、窗口函数  </p><p>查看系统内置函数</p><pre><code>show functions;  </code></pre><p>查看内置函数用法  </p><pre><code>desc function upper;  </code></pre><p>查看内置函数详细信息  </p><pre><code>desc function extended upper;  </code></pre><p>##1.单行函数  </p><p>单行函数的特点是一进一出，即输入一行，输出一行</p><p>单行函数按照功能可分为如下几类: 日期函数、字符串函数、集合函数、数学函数、流程控制函数等  </p><p>###数值函数  </p><pre><code>round：四舍五入    ceil：向上取整   floor：向下取整  </code></pre><p>###字符串函数  </p><pre><code>substring：截取字符串  substring(string A, int start)  substring(string A, int start, int len)   </code></pre><p>replace ：替换  </p><pre><code>replace(string A, string B, string C)   regexp_replace：正则替换  regexp_replace(string A, string B, string C)   说明：将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符  </code></pre><p>regexp：正则匹配 </p><pre><code>字符串 regexp 正则表达式     说明：若字符串符合正则表达式，则返回true，否则返回falseselect &#39;dfsaaaa&#39; regexp &#39;dfsa+&#39;  </code></pre><p>repeat：重复字符串  </p><pre><code>repeat(string A, int n)  说明：将字符串A重复n遍select repeat(&#39;123&#39;, 3);  </code></pre><p>split ：字符串切割  </p><pre><code>split(string str, string pat)   返回值：arrayhive&gt; select split(&#39;a-b-c-d&#39;,&#39;-&#39;);hive&gt; [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;]</code></pre><p>nvl ：替换null值  </p><pre><code>nvl(A,B) 若A的值不为null，则返回A，否则返回B  hive&gt; select nvl(null,1);   </code></pre><p>concat ：拼接字符串  </p><pre><code>concat(string A, string B, string C, ……)   将A,B,C……等字符拼接为一个字符串  hive&gt; select concat(&#39;beijing&#39;,&#39;-&#39;,&#39;shanghai&#39;,&#39;-&#39;,&#39;shenzhen&#39;);  hive&gt; beijing-shanghai-shenzhen  </code></pre><p>concat_ws：以指定分隔符拼接字符串或者字符串数组  </p><pre><code>concat_ws(string A, string…| array(string))   使用分隔符A拼接多个字符串，或者一个数组的所有元素。  hive&gt;select concat_ws(&#39;-&#39;,&#39;beijing&#39;,&#39;shanghai&#39;,&#39;shenzhen&#39;);  hive&gt; beijing-shanghai-shenzhen  hive&gt; select concat_ws(&#39;-&#39;,array(&#39;beijing&#39;,&#39;shenzhen&#39;,&#39;shanghai&#39;));  hive&gt; beijing-shanghai-shenzhen</code></pre><p>get_json_object：解析json字符串   </p><pre><code>get_json_object(string json_string, string path)解析json的字符串json_string，返回path指定的内容。如果输入的json字符串无效，那么返回NULL  hive&gt; select get_json_object(&#39;[&#123;&quot;name&quot;:&quot;大海海&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;25&quot;&#125;,&#123;&quot;name&quot;:&quot;小宋宋&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;47&quot;&#125;]&#39;,&#39;$.[0].name&#39;);  </code></pre><p>###日期函数  </p><p>unix_timestamp：返回当前或指定时间的时间戳</p><pre><code>unix_timestamp()   返回值：bigint    hive&gt; select unix_timestamp(&#39;2022/08/08 08-08-08&#39;,&#39;yyyy/MM/dd HH-mm-ss&#39;);1659946088  </code></pre><p>from_unixtime：转化UNIX时间戳（从 1970-01-01 00:00:00 UTC 到指定时间的秒数）到当前时区的时间格式</p><pre><code>from_unixtime(bigint unixtime[, string format])  返回值：string  hive&gt; select from_unixtime(1659946088);  2022-08-08 08:08:08  </code></pre><p>current_date：当前日期   </p><p>current_timestamp：当前的日期加时间，并且精确的毫秒   </p><p>month：获取日期中的月  </p><pre><code>语法：month (string date)   返回值：int   </code></pre><p>day：获取日期中的日  </p><pre><code>语法：day (string date)   返回值：int </code></pre><p>hour：获取日期中的小时  </p><pre><code>语法：hour (string date)   返回值：int   </code></pre><p>datediff：两个日期相差的天数（结束日期减去开始日期的天数）</p><pre><code>语法：datediff(string enddate, string startdate) 返回值：int   hive&gt; select datediff(&#39;2021-08-08&#39;,&#39;2022-10-09&#39;);        -427    </code></pre><p>date_add：日期加天数  </p><pre><code>date_add(string startdate, int days)   </code></pre><p>date_sub：日期减天数  </p><pre><code>date_sub (string startdate, int days)   </code></pre><p>date_format:将标准日期解析成指定格式字符串  </p><pre><code>hive&gt; select date_format(&#39;2022-08-08&#39;,&#39;yyyy年-MM月-dd日&#39;)  2022年-08月-08日 </code></pre><p>##流程控制函数  </p><p>case when：条件判断函数  </p><pre><code>语法一： case when a then b [when c then d]* [else e] end   语法二： case a when b then c [when d then e]* [else f] end判断同一个字段与多个值是否相等时才能这样写  </code></pre><p>if: 条件判断，类似于Java中三元运算符</p><pre><code>if（boolean testCondition, T valueTrue, T valueFalseOrNull）  当条件testCondition为true时，返回valueTrue；否则返回valueFalseOrNull hive&gt; select if(10 &gt; 5,&#39;正确&#39;,&#39;错误&#39;);    输出：正确  </code></pre><p>##集合函数  </p><p>size：集合中元素的个数  </p><pre><code>hive&gt; select size(friends) from test;  --2/2  每一行数据中的friends集合里的个数  </code></pre><p>map：创建map集合</p><pre><code>语法：map (key1, value1, key2, value2, …) 说明：根据输入的key和value对构建map类型  </code></pre><p>map_keys： 返回map中的key  </p><pre><code>hive&gt; select map_keys(map(&#39;xiaohai&#39;,1,&#39;dahai&#39;,2));  hive&gt;[&quot;xiaohai&quot;,&quot;dahai&quot;] </code></pre><p>map_values: 返回map中的value  </p><pre><code>hive&gt; select map_values(map(&#39;xiaohai&#39;,1,&#39;dahai&#39;,2));hive&gt;[1,2]  </code></pre><p>array 声明array集合  </p><pre><code>语法：array(val1, val2, …) 说明：根据输入的参数构建数组array类  hive&gt; select array(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;);  hive&gt;[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;]  </code></pre><p>array_contains: 判断array中是否包含某个元素  </p><pre><code>hive&gt; select array_contains(array(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;),&#39;a&#39;);   hive&gt; true </code></pre><p>sort_array：将array中的元素排序</p><pre><code>hive&gt; select sort_array(array(&#39;a&#39;,&#39;d&#39;,&#39;c&#39;));  hive&gt; [&quot;a&quot;,&quot;c&quot;,&quot;d&quot;]  </code></pre><p>struct声明struct中的各属性   </p><pre><code>语法：struct(val1, val2, val3, …)   说明：根据输入的参数构建结构体struct类  hive&gt; select struct(&#39;name&#39;,&#39;age&#39;,&#39;weight&#39;);  hive&gt; &#123;&quot;col1&quot;:&quot;name&quot;,&quot;col2&quot;:&quot;age&quot;,&quot;col3&quot;:&quot;weight&quot;&#125;  </code></pre><p>named_struct声明struct的属性和值  </p><pre><code>hive&gt; select named_struct(&#39;name&#39;,&#39;xiaosong&#39;,&#39;age&#39;,18,&#39;weight&#39;,80);  hive&gt; &#123;&quot;name&quot;:&quot;xiaosong&quot;,&quot;age&quot;:18,&quot;weight&quot;:80&#125;  </code></pre><p>##2.高级聚合函数</p><p>多进一出 （多行传入，一个行输出） </p><p>普通聚合  </p><p>collect_list 收集并形成list集合，结果不去重</p><pre><code>hive&gt;select sex,collect_list(job) from employee group by sex;  女[&quot;行政&quot;,&quot;研发&quot;,&quot;行政&quot;,&quot;前台&quot;]  男[&quot;销售&quot;,&quot;研发&quot;,&quot;销售&quot;,&quot;前台&quot;]  </code></pre><p>collect_set 收集并形成set集合，结果去重  </p><pre><code>hive&gt;select sex,collect_set(job) from employee group by sex;女[&quot;行政&quot;,&quot;研发&quot;,&quot;前台&quot;]男[&quot;销售&quot;,&quot;研发&quot;,&quot;前台&quot;]  </code></pre><p>##3.炸裂函数</p><p>UDTF,接收一行数据，输出一行或多行数据。</p><p>##4.窗口函数  </p><p>窗口函数，能为每行数据划分一个窗口，然后对窗口范围内的数据进行计算，最后将计算结果返回给该行的数据。</p><p>###窗口语法</p><p>####基于行<br><img src="/2023/07/30/hive_learn/3.png" alt="基于行">            </p><p>####基于值<br><img src="/2023/07/30/hive_learn/4.png" alt="基于值"></p><p>按照功能，常用窗口可划分为如下几类：聚合函数、跨行取值函数、排名函数  </p><p>聚合函数</p><pre><code>max：最大值min：最小值  sum：求和avg：平均值count：计数    </code></pre><p>跨行取值函数</p><pre><code>Lead：上移  Lag：下移注：lag和lead函数不支持自定义窗口first_valuelast_value  </code></pre><p>排名函数  </p><pre><code>rank dense_rankrow_number三者均不支持自定义窗口  </code></pre><p>#六：自定义函数  </p><p><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">自定义函数官方文档</a> </p><p>##编程步骤  </p><p>(1) 继承Hive提供的类  </p><p>org.apache.hadoop.hive.ql.udf.generic.GenericUDF  </p><p>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</p><p>(2) 实现类中的抽象方法  </p><p>(3) 在hive的命令行窗口创建函数  </p><p>添加jar  </p><pre><code>add jar linux_jar_path  </code></pre><p>创建function  </p><pre><code>create [temporary] function [dbname.]function_name AS class_name;</code></pre><p>(4) 在hive的命令行窗口删除函数  </p><pre><code>drop [temporary] function [if exists] [dbname.]function_name;</code></pre><p>#七：分区表和分桶表    </p><p>##1.分区表  </p><p>Hive中的分区就是把一张大表的数据按照业务需要分散的存储到多个目录，每个目录就称为该表的一个分区  </p><pre><code>hive (default)&gt; create table dept_partition(deptno int,    --部门编号dname  string, --部门名称loc    string  --部门位置)partitioned by (day string)row format delimited fields terminated by &#39;\t&#39;;</code></pre><p>装载语句  </p><pre><code>hive (default)&gt; load data local inpath &#39;/opt/module/hive/datas/dept_20220401.log&#39; into table dept_partition partition(day=&#39;20220401&#39;);  hive (default)&gt; insert overwrite table dept_partition partition (day = &#39;20220402&#39;)select deptno, dname, locfrom dept_partitionwhere day = &#39;2020-04-01&#39;;</code></pre><p>###分区表基本操作  </p><p>####1）查看所有分区信息  </p><pre><code>hive&gt; show partitions dept_partition;</code></pre><p>####2）增加分区<br>#####（1）创建单个分区  </p><pre><code>hive (default)&gt; alter table dept_partition add partition(day=&#39;20220403&#39;);  </code></pre><p>#####（2）同时创建多个分区（分区之间不能有逗号）  </p><pre><code>hive (default)&gt; alter table dept_partition add partition(day=&#39;20220404&#39;) partition(day=&#39;20220405&#39;);  </code></pre><p>####3）删除分区  </p><p>#####（1）删除单个分区  </p><pre><code>hive (default)&gt; alter table dept_partition drop partition (day=&#39;20220403&#39;);</code></pre><p>#####（2）同时删除多个分区（分区之间必须有逗号）  </p><pre><code>hive (default)&gt; alter table dept_partition drop partition (day=&#39;20220404&#39;), partition(day=&#39;20220405&#39;);</code></pre><p>####4）修复分区</p><p>add partition  </p><pre><code>若手动创建HDFS的分区路径，Hive无法识别，可通过add partition命令增加分区元数据信息，从而使元数据和分区路径保持一致</code></pre><p>drop partition  </p><pre><code>若手动删除HDFS的分区路径，Hive无法识别，可通过drop partition命令删除分区元数据信息，从而使元数据和分区路径保持一致</code></pre><p>msck </p><p>若分区元数据和HDFS的分区路径不一致，还可使用msck命令进行修复，以下是该命令的用法说明 </p><pre><code>hive (default)&gt; msck repair table table_name [add/drop/sync partitions];msck repair table table_name add partitions：该命令会增加HDFS路径存在但元数据缺失的分区信息 msck repair table table_name drop partitions：该命令会删除HDFS路径已经删除但元数据仍然存在的分区信息 msck repair table table_name sync partitions：该命令会同步HDFS路径和元数据分区信息，相当于同时执行上述的两个命令msck repair table table_name：等价于msck repair table table_name add partitions命令</code></pre><p><strong>所以msck修复hive元数据，首选msck repair table table_name sync partitions命令</strong>  </p><p>####二级分区表 </p><p>二级分区表建表语句  </p><pre><code>hive (default)&gt;create table dept_partition2(deptno int,    -- 部门编号dname string, -- 部门名称loc string     -- 部门位置)partitioned by (day string, hour string)row format delimited fields terminated by &#39;\t&#39;; </code></pre><p>数据装载语句  </p><pre><code>hive (default)&gt; load data local inpath &#39;/opt/module/hive/datas/dept_20220401.log&#39; into table dept_partition2 partition(day=&#39;20220401&#39;, hour=&#39;12&#39;);  </code></pre><p>查询分区数据  </p><pre><code>hive (default)&gt; select  * from dept_partition2 where day=&#39;20220401&#39; and hour=&#39;12&#39;;  </code></pre><p>####动态分区  </p><p>动态分区是指向分区表insert数据时，被写往的分区不由用户指定，而是由每行数据的最后一个字段的值来动态的决定，使用动态分区，可只用一个insert语句将数据写入多个分区  </p><p>#####1）动态分区相关参数  </p><p>(1) 动态分区功能总开关（默认true，开启）  </p><pre><code>set hive.exec.dynamic.partition=true  </code></pre><p>(2) 严格模式和非严格模式   </p><p>动态分区的模式，默认strict（严格模式），要求必须指定至少一个分区为静态分区，nonstrict（非严格模式）允许所有的分区字段都使用动态分区  </p><pre><code>set hive.exec.dynamic.partition.mode=nonstrict</code></pre><p>##2.分桶表  </p><p>并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分  </p><p><strong>分区针对的是数据的存储路径，分桶针对的是数据文件</strong>  </p><p>分桶表的基本原理是，首先为每行数据计算一个指定字段的数据的hash值，然后模以一个指定的分桶数，最后将取模运算结果相同的行，写入同一个文件中，这个文件就称为一个分桶（bucket）</p><p>建表语句</p><pre><code>hive (default)&gt; create table stu_buck(id int, name string)clustered by(id) sorted by(id)into 4 bucketsrow format delimited fields terminated by &#39;\t&#39;;load data local inpath &#39;/opt/module/hive/datas/student.txt&#39; into table stu_buck;</code></pre><p>#8.Hive压缩格式和文件格式  </p><p>##压缩格式</p><p>DEFLATE<br>gzip<br>bzip2<br>LZO<br>Snappy</p><p>##Hive文件格式<br>text file<br>orc<br>parquet<br>sequence file  </p><p>##行式存储和列式存储</p><p>###行式存储 - textfile，sequence file<br>文本文件是Hive默认使用的文件格式，文本文件中的一行内容，就对应Hive表中的一行记录</p><p>适用于会用到很多where语句的表</p><p>###列式存储 - orc，parquet</p><p>数仓中尽量选用列式存储方式</p><p><img src="/2023/07/30/hive_learn/5.png" alt="ORC文件基本格式">  </p><p><img src="/2023/07/30/hive_learn/6.png" alt="Parquet文件基本格式">   </p><p>##压缩</p><p>在Hive表中和计算过程中，保持数据的压缩，对磁盘空间的有效利用和提高查询性能都是十分有益的  </p><p>###Hive表数据进行压缩    </p><p>####1）TextFile    </p><p>无法直接在表结构中进行声明压缩    </p><p>直接将压缩后的文件导入到该表即可，Hive在查询表中数据时，可自动识别其压缩格式，进行解压  </p><p>TextFile压缩格式常为Gzip  </p><p>需要注意的是，在执行往表中导入数据的SQL语句时，用户需设置以下参数，来保证写入表中的数据是被压缩的。  </p><p>–SQL语句的最终输出结果是否压缩  </p><pre><code>set hive.exec.compress.output=true;  </code></pre><p>–输出结果的压缩格式（以下示例为snappy）  </p><pre><code>set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec;</code></pre><p>####2）ORC</p><p>可在建表时声明    </p><pre><code>create table orc_table(column_specs)stored as orctblproperties (&quot;orc.compress&quot;=&quot;snappy&quot;);  </code></pre><p>####3）Parquet  </p><p>可在建表时声明   </p><pre><code>create table orc_table(column_specs)stored as parquettblproperties (&quot;parquet.compression&quot;=&quot;snappy&quot;);</code></pre><p>###计算过程中使用压缩  </p><p>1）单个mr的中间结果进行压缩  </p><p>–开启MapReduce中间数据压缩功能  </p><pre><code>set mapreduce.map.output.compress=true;</code></pre><p>–设置MapReduce中间数据数据的压缩方式（以下示例为snappy） </p><pre><code>set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;  </code></pre><p>2）单条sql语句的中间结果进行压缩    </p><p>单条SQL语句的中间结果是指，两个MR（一条SQL语句可能需要通过MR进行计算）之间的临时数据，可通过以下参数进行配置：   </p><p>–是否对两个MR之间的临时数据进行压缩  </p><pre><code>set hive.exec.compress.intermediate=true;  </code></pre><p>–压缩格式（以下示例为snappy）  </p><pre><code>set hive.intermediate.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;</code></pre><p>#能看到这里，你是真滴牛批~</p><p>#猛男，帅哥儿，靓仔，点个赞再肘~</p><p><img src="/2023/07/30/hive_learn/7.png" alt="歪嘴猫"></p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E6%89%8B%E6%8F%A1%E6%97%A5%E6%9C%88%E6%91%98%E6%98%9F%E8%BE%B0%EF%BC%8C%E4%B8%96%E9%97%B4%E6%97%A0%E6%88%91%E8%BF%99%E8%88%AC%E4%BA%BA/">手握日月摘星辰，世间无我这般人</category>
      
      
      <comments>http://example.com/2023/07/30/hive_learn/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
