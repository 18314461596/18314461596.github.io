<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>第五门徒</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>初级以内我无敌，中级以上我一换一</description>
    <pubDate>Mon, 21 Aug 2023 16:06:53 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>java问题集</title>
      <link>http://example.com/2023/08/18/Java_questions/</link>
      <guid>http://example.com/2023/08/18/Java_questions/</guid>
      <pubDate>Fri, 18 Aug 2023 03:30:12 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;道虽迩，不行不至；事虽小，不为不成&quot;&gt;&lt;a href=&quot;#道虽迩，不行不至；事虽小，不为不成&quot; class=&quot;headerlink&quot; title=&quot;道虽迩，不行不至；事虽小，不为不成&quot;&gt;&lt;/a&gt;道虽迩，不行不至；事虽小，不为不成&lt;/h1&gt;&lt;h2 id=&quot;java中</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="道虽迩，不行不至；事虽小，不为不成"><a href="#道虽迩，不行不至；事虽小，不为不成" class="headerlink" title="道虽迩，不行不至；事虽小，不为不成"></a>道虽迩，不行不至；事虽小，不为不成</h1><h2 id="java中double类型数据如何指定保留位数？"><a href="#java中double类型数据如何指定保留位数？" class="headerlink" title="java中double类型数据如何指定保留位数？"></a>java中double类型数据如何指定保留位数？</h2><p><img src="/2023/08/18/Java_questions/1.png" alt="java中double类型数据如何指定保留位数"></p><h2 id="字符串的内容比较使用-为啥无效？"><a href="#字符串的内容比较使用-为啥无效？" class="headerlink" title="字符串的内容比较使用&#x3D;&#x3D;为啥无效？"></a>字符串的内容比较使用&#x3D;&#x3D;为啥无效？</h2><p>字符串的内容比较应该使用方法equals</p><pre><code>if(&quot;丁真&quot;.equals(name) &amp;&amp; &quot;666&quot;.equals(passwd))&#123;        //if(name == &quot;丁真&quot; &amp;&amp; passwd == &quot;666&quot;)&#123;  字符串判断相等时，不能使用 == ，会无效        System.out.println(&quot;恭喜你，登录成功~&quot;);        break;        &#125;else&#123;            chance--;            System.out.println(&quot;你还有&quot;+chance+&quot;次登录机会&quot;);        &#125;</code></pre><p>equals方法使用细节：  </p><pre><code>System.out.println(name.equals(&quot;林黛玉&quot;));System.out.println(&quot;林黛玉&quot;.equals(name));//T [推荐，可以避免空指针</code></pre><h2 id="Java中系统输入-char-字符-‘男’-应该如何做？"><a href="#Java中系统输入-char-字符-‘男’-应该如何做？" class="headerlink" title="Java中系统输入 char 字符 ‘男’ 应该如何做？"></a>Java中系统输入 char 字符 ‘男’ 应该如何做？</h2><pre><code>Scanner myScanner = new Scanner(System.in);char gender = myScanner.next().charAt(0); if (gender == &#39;男&#39;)&#123;            System.out.println(&quot;进入男子组&quot;);        &#125;else if(gender == &#39;女&#39;)&#123;            System.out.println(&quot;进入女子组&quot;);        &#125;else&#123;            System.out.println(&quot;你的性别有误，不能参加决赛 ~ &quot;);        &#125;</code></pre><h2 id="Java中数组的深拷贝和浅拷贝"><a href="#Java中数组的深拷贝和浅拷贝" class="headerlink" title="Java中数组的深拷贝和浅拷贝"></a>Java中数组的深拷贝和浅拷贝</h2><p>深拷贝： int[] arr2 &#x3D; new int[arr1.length]  ,arr2数组重新开辟了一个数组空间<br>浅拷贝： arr2 &#x3D; arr1 这是将arr1的数组地址赋值给了arr2，这样arr1和arr2其实都指向了同一个数组，此时arr1和arr2的元素都是同步变化的，并没有独立。</p><h2 id="Java-中对象在内存中的存在形式是怎样的"><a href="#Java-中对象在内存中的存在形式是怎样的" class="headerlink" title="Java 中对象在内存中的存在形式是怎样的?"></a>Java 中对象在内存中的存在形式是怎样的?</h2><p><img src="/2023/08/18/Java_questions/2.png" alt="Java 中对象在内存中的存在形式"></p><p>数值属性存储在堆内存中  </p><p>方法存储在方法区中  </p><p>方法区中的常量池负责存储数值属性之外的其他属性值  </p><h3 id="类和对象的内存分配机制"><a href="#类和对象的内存分配机制" class="headerlink" title="类和对象的内存分配机制"></a>类和对象的内存分配机制</h3><h4 id="Java-内存的结构分析"><a href="#Java-内存的结构分析" class="headerlink" title="Java 内存的结构分析"></a>Java 内存的结构分析</h4><ol><li><p>栈： 一般存放基本数据类型(局部变量)  </p></li><li><p>堆： 存放对象(Cat cat , 数组等)   </p></li><li><p>方法区：常量池(常量，比如字符串)， 类加载信息</p></li></ol><h2 id="类定义在public-class-代码块内外有何区别？-什么时候需要使用static-class-？"><a href="#类定义在public-class-代码块内外有何区别？-什么时候需要使用static-class-？" class="headerlink" title="类定义在public class 代码块内外有何区别？ 什么时候需要使用static class ？"></a>类定义在public class 代码块内外有何区别？ 什么时候需要使用static class ？</h2><p><img src="/2023/08/18/Java_questions/3.png" alt="类定义在public class 代码块内外有何区别？"></p><p><img src="/2023/08/18/Java_questions/4.png" alt="类定义在public class 代码块内外有何区别？"></p><p>在别的代码中定义了同名类，本次代码中需要重新定义时，类应当定义在public class代码块中，使用static描述，否则会报错.这个是叫静态内部类吗？  </p><p>类本质上就是一种数据类型。可以定义属性，定义方法。使用的时候和数据类型一样使用即可。  </p><h3 id="方法的调用机制原理"><a href="#方法的调用机制原理" class="headerlink" title="方法的调用机制原理"></a>方法的调用机制原理</h3><p><img src="/2023/08/18/Java_questions/5.png" alt="方法的调用机制原理"></p><h2 id="形参和实参"><a href="#形参和实参" class="headerlink" title="形参和实参"></a>形参和实参</h2><pre><code>getSumAndSub(int n1,int n2)  这就是形参  getSumAndSub(1,8) 这就是实参</code></pre><h2 id="基本数据类型的传参机制"><a href="#基本数据类型的传参机制" class="headerlink" title="基本数据类型的传参机制"></a>基本数据类型的传参机制</h2><p>基本数据类型，传递的是值(值拷贝),形参的任何改变不影响实参</p><p>main方法中的引用类型属性可以被方法修改    </p><p>main方法中的基本类型属性无法被方法修改  </p><p>引用类型传递的是地址（传递也是值，但是值是地址），可以通过形参影响实参！  </p><h2 id="Java中的可变参数"><a href="#Java中的可变参数" class="headerlink" title="Java中的可变参数"></a>Java中的可变参数</h2><p>int… 与 int[] 等价  </p><p>可变参数可以和普通类型的参数一起放在形参列表,但必须保证可变参数在最后    </p><p>一个形参列表中只能出现一个可变参数  </p><p>可变参数的实参可以是数组 </p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E9%81%93%E8%99%BD%E8%BF%A9%EF%BC%8C%E4%B8%8D%E8%A1%8C%E4%B8%8D%E8%87%B3%EF%BC%9B%E4%BA%8B%E8%99%BD%E5%B0%8F%EF%BC%8C%E4%B8%8D%E4%B8%BA%E4%B8%8D%E6%88%90/">道虽迩，不行不至；事虽小，不为不成</category>
      
      
      <comments>http://example.com/2023/08/18/Java_questions/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark报错经验记录</title>
      <link>http://example.com/2023/08/12/Spark-practice/</link>
      <guid>http://example.com/2023/08/12/Spark-practice/</guid>
      <pubDate>Sat, 12 Aug 2023 11:02:51 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;大鹏飞兮振八裔，中天摧兮力不济&quot;&gt;&lt;a href=&quot;#大鹏飞兮振八裔，中天摧兮力不济&quot; class=&quot;headerlink&quot; title=&quot;大鹏飞兮振八裔，中天摧兮力不济&quot;&gt;&lt;/a&gt;大鹏飞兮振八裔，中天摧兮力不济&lt;/h1&gt;&lt;h1 id=&quot;使用Spark的报错经验&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="大鹏飞兮振八裔，中天摧兮力不济"><a href="#大鹏飞兮振八裔，中天摧兮力不济" class="headerlink" title="大鹏飞兮振八裔，中天摧兮力不济"></a>大鹏飞兮振八裔，中天摧兮力不济</h1><h1 id="使用Spark的报错经验"><a href="#使用Spark的报错经验" class="headerlink" title="使用Spark的报错经验"></a>使用Spark的报错经验</h1><h2 id="IDEA工具中运行Spark程序报缺失scala-compiler-2-12-16-jar包"><a href="#IDEA工具中运行Spark程序报缺失scala-compiler-2-12-16-jar包" class="headerlink" title="IDEA工具中运行Spark程序报缺失scala-compiler-2.12.16.jar包"></a>IDEA工具中运行Spark程序报缺失scala-compiler-2.12.16.jar包</h2><p><img src="/2023/08/12/Spark-practice/1.png" alt="IDEA的Scala SDK设置位置">  </p><p>重新设置IDEA中Scala SDK即可  </p><h2 id="使用Spark-SQL往Mysql中写入数据，出现中文乱码"><a href="#使用Spark-SQL往Mysql中写入数据，出现中文乱码" class="headerlink" title="使用Spark SQL往Mysql中写入数据，出现中文乱码"></a>使用Spark SQL往Mysql中写入数据，出现中文乱码</h2><h3 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h3><p><a href="https://blog.csdn.net/MASILEJFOAISEGJIAE/article/details/89314591?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-89314591-blog-109695269.235%5Ev38%5Epc_relevant_sort_base2&spm=1001.2101.3001.4242.1&utm_relevant_index=4">解决方案</a></p><p>补充一下，如果在调整编码之前就已经创建了数据库和表的话，需要再单独修改数据库和表的编码格式  </p><p><img src="/2023/08/12/Spark-practice/2.png" alt="Spark SQL写入Mysql数据的案例代码">    </p><pre><code>package com.atguigu.bigdata.spark.core.sqlimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;SaveMode, SparkSession&#125;object Spark04_SparkSQL_JDBC &#123;  def main(args: Array[String]): Unit = &#123;    // TODO 创建SparkSQL的运行环境    val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)    val spark = SparkSession.builder().config(sparkConf).getOrCreate()    import spark.implicits._    // 读取MySQL数据    val df = spark.read      .format(&quot;jdbc&quot;)      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306/test&quot;)      .option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)      .option(&quot;user&quot;, &quot;root&quot;)      .option(&quot;password&quot;, &quot;Wo@6930886&quot;)      .option(&quot;dbtable&quot;, &quot;course&quot;)      .load()    df.show    // 保存数据    df.write      .format(&quot;jdbc&quot;)      .option(&quot;url&quot;,&quot;jdbc:mysql://hadoop101:3306/test?useUnicode=true&amp;characterEncoding=utf8&quot;)      .option(&quot;driver&quot;,&quot;com.mysql.jdbc.Driver&quot;)      .option(&quot;user&quot;,&quot;root&quot;)      .option(&quot;password&quot;,&quot;Wo@6930886&quot;)      .option(&quot;dbtable&quot;,&quot;course1&quot;)      .mode(SaveMode.Overwrite)      .save()    // TODO 关闭环境    spark.close()  &#125;&#125;</code></pre><h2 id="使用IDEA新建项目时，无法创建Scala代码"><a href="#使用IDEA新建项目时，无法创建Scala代码" class="headerlink" title="使用IDEA新建项目时，无法创建Scala代码"></a>使用IDEA新建项目时，无法创建Scala代码</h2><p>先随便创建一个文件，以scala结尾，之后代码编辑器右上角会弹出“设置Scala SDK”,点击设置之后，就可以正常创建Scala程序了  </p><p><img src="/2023/08/12/Spark-practice/3.png" alt="新增一个文件后缀写成scala">  </p><p><img src="/2023/08/12/Spark-practice/4.png" alt="可以创建Scala程序了">  </p><h2 id="IDEA中Alt-Enter自动导入import失效，且手动输入import语句，apache报错"><a href="#IDEA中Alt-Enter自动导入import失效，且手动输入import语句，apache报错" class="headerlink" title="IDEA中Alt + Enter自动导入import失效，且手动输入import语句，apache报错"></a>IDEA中Alt + Enter自动导入import失效，且手动输入import语句，apache报错</h2><p><img src="/2023/08/12/Spark-practice/5.png" alt="导包报错"> </p><p>原因是没有在pom文件中导入Spark相关包    </p><p>先在pom文件中配置参数导入Spark相关的包，再修改Maven仓库的版本为3.8.1以前的版本，重新使用maven加载包即可  </p><p><img src="/2023/08/12/Spark-practice/7.png" alt="导包报错">   </p><p>Pom文件配置示例  </p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;     xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;     xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;groupId&lt;/groupId&gt;&lt;artifactId&gt;SGG_scala_Spark3.0.0_practice&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;dependencies&gt;&lt;!--        spark--&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-yarn_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;        &lt;artifactId&gt;spark-streaming-kafka-0-10_2.12&lt;/artifactId&gt;        &lt;version&gt;2.4.0&lt;/version&gt;    &lt;/dependency&gt;&lt;!--mysql--&gt;    &lt;dependency&gt;        &lt;groupId&gt;mysql&lt;/groupId&gt;        &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;        &lt;version&gt;5.1.27&lt;/version&gt;    &lt;/dependency&gt;&lt;!--        jackson--&gt;    &lt;dependency&gt;        &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;        &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;        &lt;version&gt;2.9.9&lt;/version&gt;    &lt;/dependency&gt;&lt;!--        &lt;dependency&gt;--&gt;&lt;!--            &lt;groupId&gt; com.fasterxml.jackson.core &lt;/groupId&gt;--&gt;&lt;!--            &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;--&gt;&lt;!--            &lt;version&gt;2.9.9&lt;/version&gt;--&gt;&lt;!--        &lt;/dependency&gt;--&gt;        &lt;dependency&gt;        &lt;groupId&gt;com.thoughtworks.paranamer&lt;/groupId&gt;        &lt;artifactId&gt;paranamer&lt;/artifactId&gt;        &lt;version&gt;2.8&lt;/version&gt;    &lt;/dependency&gt;    &lt;!--common--&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;        &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;        &lt;version&gt;3.3&lt;/version&gt;    &lt;/dependency&gt;    &lt;!-- https://mvnrepository.com/artifact/com.alibaba/druid --&gt;    &lt;dependency&gt;        &lt;groupId&gt;com.alibaba&lt;/groupId&gt;        &lt;artifactId&gt;druid&lt;/artifactId&gt;        &lt;version&gt;1.1.10&lt;/version&gt;    &lt;/dependency&gt;&lt;!--hive--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;            &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;            &lt;version&gt;2.1.1&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;            &lt;artifactId&gt;hive-metastore&lt;/artifactId&gt;            &lt;version&gt;2.1.1&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><p>清除缓存重启IDEA    </p><p><img src="/2023/08/12/Spark-practice/8.png" alt="IDEA中修复import Spark包报错问题">   </p><p>OK鸟 ~  </p><h2 id="IDEA中下载源码和查看源码的方法"><a href="#IDEA中下载源码和查看源码的方法" class="headerlink" title="IDEA中下载源码和查看源码的方法"></a>IDEA中下载源码和查看源码的方法</h2><p><img src="/2023/08/12/Spark-practice/6.png" alt="IDEA中下载源码"> </p><p>ctrl + H 跳到源码内      </p><h2 id="IDEA中编写好Scala程序后无法运行程序"><a href="#IDEA中编写好Scala程序后无法运行程序" class="headerlink" title="IDEA中编写好Scala程序后无法运行程序"></a>IDEA中编写好Scala程序后无法运行程序</h2><p><img src="/2023/08/12/Spark-practice/9.png" alt="IDEA中无法运行Scala程序">  </p><p>手动构建应用程序  </p><p><img src="/2023/08/12/Spark-practice/10.png" alt="手动构建应用程序"></p><p>Scala编写Spark运行程序时，应当选用Object而非class类型文件  </p><p><img src="/2023/08/12/Spark-practice/11.png" alt="Scala编写Spark程序选用Object">  </p><h2 id="IDEA中本地编写Spark程序，提交到Yarn集群上运行出错"><a href="#IDEA中本地编写Spark程序，提交到Yarn集群上运行出错" class="headerlink" title="IDEA中本地编写Spark程序，提交到Yarn集群上运行出错"></a>IDEA中本地编写Spark程序，提交到Yarn集群上运行出错</h2><p>1.修改IDEA中本地代码  </p><p>本地开发测试时，val SparkConf &#x3D; new SparkConf().setMaster(“local[<em>]”).setAppName(“SparkSQL_mysql_join_mysql”)<br>打包上传到yarn集群运行前，需要删掉setMaster(“local[</em>]”)，即改成val SparkConf &#x3D; new SparkConf().setAppName(“SparkSQL_mysql_join_mysql”)  </p><p><img src="/2023/08/12/Spark-practice/12.png" alt="Spark程序打包前修改Sparkconf参数">  </p><p>2.先Maven clean，再构建项目，生成target文件夹，尤其注意，一定要获取到target&#x2F;classes文件夹   </p><p><img src="/2023/08/12/Spark-practice/13.png" alt="Spark程序打包"> </p><p>3.将打好的Spark程序jar包提交到服务器上  </p><p><img src="/2023/08/12/Spark-practice/14.png" alt="Spark程序jar包上传到服务器上">  </p><p>4.如果程序运行时需要调用到其他Jar包的类，集群运行时会报错，需要添加jars参数指定jar包位置  </p><pre><code>/opt/cloudera/parcels/CDH/lib/spark/bin/spark-submit --jars /opt/module/spark/mysql-connector-java.jar --class com.zyy.sparksql.SparkSQL_mysql_join_mysql --master yarn --deploy-mode cluster /opt/module/spark/SGG_scala_Spark3.0.0_practice-1.0-SNAPSHOT.jar 10  </code></pre><p>5.查看运行结果，运行成功   </p><p><img src="/2023/08/12/Spark-practice/15.png" alt="Spark程序运行成功">   </p><p><img src="/2023/08/12/Spark-practice/16.png" alt="Spark程序运行成功">    </p><p><img src="/2023/08/12/Spark-practice/17.png" alt="Spark History中运行记录"></p><p><img src="/2023/08/12/Spark-practice/18.png" alt="Spark History中查看工作流图">   </p><h2 id="IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表"><a href="#IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表" class="headerlink" title="IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表"></a>IDEA中编写Hive到Mysql的数据采集任务，需要先在Mysql中建好表</h2><p><img src="/2023/08/12/Spark-practice/19.png" alt="Hive_2_mysql">  </p><p>代码示例 </p><pre><code>package com.zyy.sparksqlimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;SaveMode, SparkSession&#125;object SparkSQL_Hive_2_Mysql &#123;  def main(args: Array[String]): Unit = &#123;    System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;)    // 1. TODO 创建SparkSQL运行环境    val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL_Hive&quot;)    val spark = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()    // 2. 编写Spark 操作Hive 的计算逻辑    //spark.sql(&quot;show databases&quot;).show()    //spark.sql(&quot;use hive_tiaoyou&quot;)    //spark.sql(&quot;show tables&quot;).show()    //spark.sql(&quot;select * from order_detail&quot;).show()    //spark.sql(&quot;select * from province_info as info1 left join province_info info2 on info1.id = info2.id&quot;).show()    spark.sql(&quot;use hive_tiaoyou&quot;)    val databases = spark.sql(&quot;show tables&quot;)    databases.show()    databases.write.format(&quot;jdbc&quot;)      .option(&quot;url&quot;,&quot;jdbc:mysql://hadoop101:3306/test?useUnicode=true&amp;characterEncoding=utf8&quot;)      .option(&quot;driver&quot;,&quot;com.mysql.jdbc.Driver&quot;)      .option(&quot;user&quot;,&quot;root&quot;)      .option(&quot;password&quot;,&quot;Wo@6930886&quot;)      .option(&quot;dbtable&quot;,&quot;test_databases&quot;)      .mode(SaveMode.Overwrite)      .save()    // 3. TODO 关闭环境    spark.close()      &#125;    &#125;  </code></pre><p>Spark要操作Hive之前，需要注意，要先把Hive-site.xml,hdfs-site.xml,core-site.xml,yarn-site.xml这几个文件从集群中下载下来，放到IDEA中resourses文件夹下  </p><p>之后通过创建环境时，设置enableHiveSupport，即可通过spark.sql(“Hive语句”)进行访问Hive的数据  </p><pre><code>val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL_Hive&quot;)  val spark = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()</code></pre>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%A4%A7%E9%B9%8F%E9%A3%9E%E5%85%AE%E6%8C%AF%E5%85%AB%E8%A3%94%EF%BC%8C%E4%B8%AD%E5%A4%A9%E6%91%A7%E5%85%AE%E5%8A%9B%E4%B8%8D%E6%B5%8E/">大鹏飞兮振八裔，中天摧兮力不济</category>
      
      
      <comments>http://example.com/2023/08/12/Spark-practice/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark-Streaming</title>
      <link>http://example.com/2023/08/11/Spark-Streaming/</link>
      <guid>http://example.com/2023/08/11/Spark-Streaming/</guid>
      <pubDate>Fri, 11 Aug 2023 14:03:30 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也&quot;&gt;&lt;a href=&quot;#英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也&quot; class=&quot;headerlink&quot; title=&quot;英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也&quot;&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也"><a href="#英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也" class="headerlink" title="英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也"></a>英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也</h1><h1 id="SparkStreaming-概述"><a href="#SparkStreaming-概述" class="headerlink" title="SparkStreaming 概述"></a>SparkStreaming 概述</h1><h2 id="Spark-Streaming-是什么"><a href="#Spark-Streaming-是什么" class="headerlink" title="Spark Streaming 是什么"></a>Spark Streaming 是什么</h2><p>Spark Streaming 用于流式数据的处理。Spark Streaming 支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如：map、reduce、join、window 等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。  </p><h2 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h2><p>和 Spark 基于 RDD 的概念很相似，Spark Streaming 使用离散化流(discretized stream)作为抽象表示，叫作 DStream。  </p><p>DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此得名“离散化”)。所以简单来将，DStream 就是对 RDD 在实时数据处理场景的一种封装。  </p><h2 id="Spark-Streaming-的特点"><a href="#Spark-Streaming-的特点" class="headerlink" title="Spark Streaming 的特点"></a>Spark Streaming 的特点</h2><p>易用    </p><p>容错  </p><p>易整合到 Spark 体系    </p><h2 id="Spark-Streaming-架构"><a href="#Spark-Streaming-架构" class="headerlink" title="Spark Streaming 架构"></a>Spark Streaming 架构</h2><p><img src="/2023/08/11/Spark-Streaming/1.png" alt="整体架构图">  </p><p><img src="/2023/08/11/Spark-Streaming/2.png" alt="架构图"> </p><h2 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h2><p>背压机制（即 Spark Streaming Backpressure）: 根据JobScheduler 反馈作业的执行信息来动态调整 Receiver 数据接收率。  </p><p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用 backpressure 机制，默认值false，即不启用。    </p><h2 id="Dstream-入门"><a href="#Dstream-入门" class="headerlink" title="Dstream 入门"></a>Dstream 入门</h2><h3 id="WordCount-案例实操"><a href="#WordCount-案例实操" class="headerlink" title="WordCount 案例实操"></a>WordCount 案例实操</h3><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><pre><code>&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;    </code></pre><h4 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h4><pre><code>object StreamWordCount &#123; def main(args: Array[String]): Unit = &#123;     //1.初始化 Spark 配置信息     val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;StreamWordCount&quot;)     //2.初始化 SparkStreamingContext     val ssc = new StreamingContext(sparkConf, Seconds(3))     //3.通过监控端口创建 DStream，读进来的数据为一行行     val lineStreams = ssc.socketTextStream(&quot;linux1&quot;, 9999)     //将每一行数据做切分，形成一个个单词     val wordStreams = lineStreams.flatMap(_.split(&quot; &quot;))     //将单词映射成元组（word,1）     val wordAndOneStreams = wordStreams.map((_, 1))     //将相同的单词次数做统计     val wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_)     //打印     wordAndCountStreams.print()     //启动 SparkStreamingContext     ssc.start()     ssc.awaitTermination()     &#125;    &#125;  </code></pre><h4 id="WordCount-解析"><a href="#WordCount-解析" class="headerlink" title="WordCount 解析"></a>WordCount 解析</h4><p>在内部实现上，DStream 是一系列连续的 RDD 来表示。每个 RDD 含有一段时间间隔内的数据。</p><p><img src="/2023/08/11/Spark-Streaming/3.png" alt="DStream">   </p><p>对数据的操作也是按照 RDD 为单位来进行的  </p><p><img src="/2023/08/11/Spark-Streaming/4.png" alt="DStream对数据的操作">     </p><p>计算过程由 Spark Engine 来完成  </p><p><img src="/2023/08/11/Spark-Streaming/5.png" alt="DStream的计算过程"> </p><h2 id="DStream创建"><a href="#DStream创建" class="headerlink" title="DStream创建"></a>DStream创建</h2><h3 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h3><h4 id="Kafka-0-10-Direct-模式"><a href="#Kafka-0-10-Direct-模式" class="headerlink" title="Kafka 0-10 Direct 模式"></a>Kafka 0-10 Direct 模式</h4><h5 id="需求：通过-SparkStreaming-从-Kafka-读取数据，并将读取过来的数据做简单计算，最终打印到控制台。"><a href="#需求：通过-SparkStreaming-从-Kafka-读取数据，并将读取过来的数据做简单计算，最终打印到控制台。" class="headerlink" title="需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。"></a>需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</h5><h5 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h5><pre><code>&lt;dependency&gt;     &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;     &lt;artifactId&gt;spark-streaming-kafka-0-10_2.12&lt;/artifactId&gt;     &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;     &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;     &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;     &lt;version&gt;2.10.1&lt;/version&gt;&lt;/dependency&gt;  </code></pre><h5 id="编写代码-1"><a href="#编写代码-1" class="headerlink" title="编写代码"></a>编写代码</h5><pre><code>import org.apache.kafka.clients.consumer.&#123;ConsumerConfig, ConsumerRecord&#125;  import org.apache.spark.SparkConf  import org.apache.spark.streaming.dstream.&#123;DStream, InputDStream&#125;  import org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;  import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;  object DirectAPI &#123;     def main(args: Array[String]): Unit = &#123;     //1.创建 SparkConf     val sparkConf: SparkConf = new      SparkConf().setAppName(&quot;ReceiverWordCount&quot;).setMaster(&quot;local[*]&quot;)     //2.创建 StreamingContext     val ssc = new StreamingContext(sparkConf, Seconds(3))     //3.定义 Kafka 参数     val kafkaPara: Map[String, Object] = Map[String, Object](     ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; &quot;linux1:9092,linux2:9092,linux3:9092&quot;,     ConsumerConfig.GROUP_ID_CONFIG -&gt; &quot;atguigu&quot;,     &quot;key.deserializer&quot; -&gt;     &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;,     &quot;value.deserializer&quot; -&gt;     &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;     )     //4.读取 Kafka 数据创建 DStream     val kafkaDStream: InputDStream[ConsumerRecord[String, String]] =     KafkaUtils.createDirectStream[String, String](ssc,     LocationStrategies.PreferConsistent,     ConsumerStrategies.Subscribe[String, String](Set(&quot;atguigu&quot;), kafkaPara))     //5.将每条消息的 KV 取出     val valueDStream: DStream[String] = kafkaDStream.map(record =&gt; record.value())     //6.计算 WordCount     valueDStream.flatMap(_.split(&quot; &quot;))     .map((_, 1))     .reduceByKey(_ + _)     .print()     //7.开启任务     ssc.start()     ssc.awaitTermination()     &#125;    &#125;  </code></pre><h5 id="查看-Kafka-消费进度"><a href="#查看-Kafka-消费进度" class="headerlink" title="查看 Kafka 消费进度"></a>查看 Kafka 消费进度</h5><pre><code>bin/kafka-consumer-groups.sh --describe --bootstrap-server linux1:9092 --group atguigu</code></pre><h2 id="DStream转换"><a href="#DStream转换" class="headerlink" title="DStream转换"></a>DStream转换</h2><p>DStream 上的操作与 RDD 的类似，分为 Transformations（转换）和 Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种 Window 相关的原语。  </p><h3 id="无状态转化操作"><a href="#无状态转化操作" class="headerlink" title="无状态转化操作"></a>无状态转化操作</h3><p>无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每<br>一个 RDD。<br>注意，针对键值对的 DStream 转化操作(比如reduceByKey())要添加 import StreamingContext._才能在 Scala 中使用。     </p><p><img src="/2023/08/11/Spark-Streaming/6.png" alt="无状态转化操作">   </p><p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部<br>是由许多 RDD（批次）组成，且无状态转化操作是分别应用到每个 RDD 上的。  </p><p>例如：reduceByKey()会归约每个时间区间中的数据，但不会归约不同区间之间的数据。  </p><h4 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h4><p>Transform 允许 DStream 上执行任意的 RDD-to-RDD 函数。即使这些函数并没有在 DStream<br>的 API 中暴露出来，通过该函数可以方便的扩展 Spark API。该函数每一批次调度一次。其实也<br>就是对 DStream 中的 RDD 应用转换。  </p><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>两个流之间的 join 需要两个流的批次大小一致，这样才能做到同时触发计算。计算过程就是对当前批次的两个流中各自的 RDD 进行 join，与两个 RDD 的 join 效果相同。  </p><pre><code>import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;object JoinTest &#123;     def main(args: Array[String]): Unit = &#123;     //1.创建 SparkConf     val sparkConf: SparkConf = new      SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;JoinTest&quot;)     //2.创建 StreamingContext     val ssc = new StreamingContext(sparkConf, Seconds(5))     //3.从端口获取数据创建流     val lineDStream1: ReceiverInputDStream[String] =      ssc.socketTextStream(&quot;linux1&quot;, 9999)     val lineDStream2: ReceiverInputDStream[String] =      ssc.socketTextStream(&quot;linux2&quot;, 8888)     //4.将两个流转换为 KV 类型     val wordToOneDStream: DStream[(String, Int)] = lineDStream1.flatMap(_.split(&quot; &quot;)).map((_, 1))     val wordToADStream: DStream[(String, String)] = lineDStream2.flatMap(_.split(&quot; &quot;)).map((_, &quot;a&quot;))     //5.流的 JOIN     val joinDStream: DStream[(String, (Int, String))] = wordToOneDStream.join(wordToADStream)     //6.打印     joinDStream.print()     //7.启动任务     ssc.start()     ssc.awaitTermination()     &#125;    &#125;</code></pre><h3 id="有状态转化操作"><a href="#有状态转化操作" class="headerlink" title="有状态转化操作"></a>有状态转化操作</h3><h4 id="UpdateStateByKey"><a href="#UpdateStateByKey" class="headerlink" title="UpdateStateByKey"></a>UpdateStateByKey</h4><p>UpdateStateByKey 原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加 wordcount)。  </p><p>updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。      </p><p>updateStateByKey 操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功<br>能，需要做下面两步：<br>    1.定义状态，状态可以是一个任意的数据类型。<br>    2.定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更<br>新。  </p><h4 id="WindowOperations"><a href="#WindowOperations" class="headerlink" title="WindowOperations"></a>WindowOperations</h4><p>Window Operations 可以设置窗口的大小和滑动窗口的间隔来动态的获取当前 Steaming 的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。  </p><p>窗口时长：计算内容的时间范围；  </p><p>滑动步长：隔多久触发一次计算。  </p><p>注意：这两者都必须为采集周期大小的整数倍。  </p><pre><code>object WorldCount &#123;     def main(args: Array[String]) &#123;         val conf = new          SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;NetworkWordCount&quot;)         val ssc = new StreamingContext(conf, Seconds(3))         ssc.checkpoint(&quot;./ck&quot;)         // Create a DStream that will connect to hostname:port, like localhost:9999         val lines = ssc.socketTextStream(&quot;linux1&quot;, 9999)         // Split each line into words         val words = lines.flatMap(_.split(&quot; &quot;))         // Count each word in each batch         val pairs = words.map(word =&gt; (word, 1))         val wordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b),Seconds(12), Seconds(6))         // Print the first ten elements of each RDD generated in this DStream to the console         wordCounts.print()         ssc.start() // Start the computation         ssc.awaitTermination() // Wait for the computation to terminate         &#125;        &#125;      </code></pre><p>关于 Window 的操作还有如下方法：  </p><pre><code>window(windowLength, slideInterval): 基于对源 DStream 窗化的批次进行计算返回一个新的 Dstream；  countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；  reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；  reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V)对的 DStream 上调用此函数，会返回一个新(K,V)对的 DStream，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值。  reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的 reduce 值都是通过用前一个窗的 reduce 值来递增计算。  </code></pre><p><img src="/2023/08/11/Spark-Streaming/7.png" alt="可逆的reduce函数">  </p><p>countByWindow()和 countByValueAndWindow()作为对数据进行计数操作的简写。countByWindow()返回一个表示每个窗口中元素个数的 DStream，而countByValueAndWindow()返回的 DStream 则包含窗口中每个值的个数。   </p><h2 id="DStream-输出"><a href="#DStream-输出" class="headerlink" title="DStream 输出"></a>DStream 输出</h2><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。<br>与 RDD 中的惰性求值类似，如果一个 DStream 及其派生出的 DStream 都没有被执行输出操作，那么这些 DStream 就都不会被求值。如果 StreamingContext 中没有设定输出操作，整个 context 就都不会启动。    </p><p>输出操作清单：</p><pre><code>print()：在运行流程序的驱动结点上打印 DStream 中每一批次数据的最开始 10 个元素 saveAsTextFiles(prefix, [suffix])：以 text 文件形式存储这个 DStream 的内容每一批次的存储文件名基于参数中的 prefix 和 suffix。”prefix-Time_IN_MS[.suffix]”  saveAsObjectFiles(prefix, [suffix])：以 Java 对象序列化的方式将 Stream 中的数据保存为SequenceFiles . 每一批次的存储文件名基于参数中的为&quot;prefix-TIME_IN_MS[.suffix]&quot;. Python中目前不可用  saveAsHadoopFiles(prefix, [suffix])：将 Stream 中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为&quot;prefix-TIME_IN_MS[.suffix]&quot;。Python API 中目前不可用  foreachRDD(func)：这是最通用的输出操作，即将函数 func 用于产生于 stream 的每一个RDD。其中参数传入的函数 func 应该实现将每一个 RDD 中数据推送到外部系统，如将RDD 存入文件或者通过网络将其写入数据库。  </code></pre><p>通用的输出操作 foreachRDD()，它用来对 DStream 中的 RDD 运行任意计算  </p><p>在 foreachRDD()中，可以重用我们在 Spark 中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如 MySQL 的外部数据库中。  </p><p>注意：<br>    1) 连接不能写在 driver 层面（序列化）<br>    2) 如果写在 foreach 则每个 RDD 中的每一条数据都创建，得不偿失；<br>    3) 增加 foreachPartition，在分区创建（获取）   </p><h2 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h2><p>流式任务需要 7*24 小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。使用外部文件系统来控制内部程序关闭。  </p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E8%8B%B1%E9%9B%84%E8%80%85%EF%BC%8C%E8%83%B8%E6%80%80%E5%A4%A7%E5%BF%97%EF%BC%8C%E8%85%B9%E6%9C%89%E8%89%AF%E8%B0%8B%EF%BC%8C%E6%9C%89%E5%8C%85%E8%97%8F%E5%AE%87%E5%AE%99%E4%B9%8B%E6%9C%BA%EF%BC%8C%E5%90%9E%E5%90%90%E5%A4%A9%E5%9C%B0%E4%B9%8B%E5%BF%97%E8%80%85%E4%B9%9F/">英雄者，胸怀大志，腹有良谋，有包藏宇宙之机，吞吐天地之志者也</category>
      
      
      <comments>http://example.com/2023/08/11/Spark-Streaming/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark-SQL</title>
      <link>http://example.com/2023/08/11/Spark-SQL/</link>
      <guid>http://example.com/2023/08/11/Spark-SQL/</guid>
      <pubDate>Fri, 11 Aug 2023 12:19:25 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功&quot;&gt;&lt;a href=&quot;#无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功&quot; class=&quot;headerlink&quot; title=&quot;无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功&quot;&gt;&lt;/a&gt;无冥冥之志者，无昭昭之</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功"><a href="#无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功" class="headerlink" title="无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功"></a>无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功</h1><h1 id="SparkSQL概述"><a href="#SparkSQL概述" class="headerlink" title="SparkSQL概述"></a>SparkSQL概述</h1><h2 id="SparkSQL是什么？"><a href="#SparkSQL是什么？" class="headerlink" title="SparkSQL是什么？"></a>SparkSQL是什么？</h2><p>Spark SQL是Spark用于结构化数据(structured data)处理的Spark模块。  </p><pre><code>➢ 数据兼容方面 SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据；➢ 性能优化方面 除了采取 In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等；➢ 组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展。  </code></pre><p>应用Spark的两个支线：SparkSQL 和 Hive on Spark  </p><p>SparkSQL 可以简化 RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。  </p><p>Spark SQL 为了简化 RDD 的开发，提高开发效率，提供了2个编程抽象，类似 Spark Core 中的RDD。</p><pre><code>➢ DataFrame➢ DataSet</code></pre><h2 id="SparkSQL-特点"><a href="#SparkSQL-特点" class="headerlink" title="SparkSQL 特点"></a>SparkSQL 特点</h2><h3 id="易整合"><a href="#易整合" class="headerlink" title="易整合"></a>易整合</h3><p>无缝的整合了 SQL 查询和 Spark 编程  </p><h3 id="统一的数据访问"><a href="#统一的数据访问" class="headerlink" title="统一的数据访问"></a>统一的数据访问</h3><p>使用相同的方式连接不同的数据源   </p><h3 id="兼容-Hive"><a href="#兼容-Hive" class="headerlink" title="兼容 Hive"></a>兼容 Hive</h3><p>在已有的仓库上直接运行 SQL 或者 HiveQL  </p><h3 id="标准数据连接"><a href="#标准数据连接" class="headerlink" title="标准数据连接"></a>标准数据连接</h3><p>通过 JDBC 或者 ODBC 来连接  </p><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。  </p><p>DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL得以洞察更多的结构信息，达到大幅提升运行时效率的目标。  </p><p>反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在 stage 层面进行简单、通用的流水线优化。  </p><p><img src="/2023/08/11/Spark-SQL/1.png" alt="DataFrame和RDD的区别">    </p><p>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待。     </p><p>DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计划通过 Spark catalyst optimiser 进行优化。  </p><p><img src="/2023/08/11/Spark-SQL/2.png" alt="逻辑查询计划优化">     </p><p>逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操<br>作的过程。     </p><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>DataSet 是分布式数据集合。  </p><p>它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）。  </p><pre><code>➢ DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象  ➢ 用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；  ➢ DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。  ➢ DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。     </code></pre><h2 id="SparkSQL-核心编程"><a href="#SparkSQL-核心编程" class="headerlink" title="SparkSQL 核心编程"></a>SparkSQL 核心编程</h2><p>SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和HiveContext的组合。  </p><h3 id="DataFrame-1"><a href="#DataFrame-1" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>Spark SQL 的 DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL表达式。    </p><h4 id="创建-DataFrame"><a href="#创建-DataFrame" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h4><p>创建 DataFrame有三种方式：<br>    1.通过 Spark 的数据源进行创建；<br>    2.从一个存在的 RDD 进行转换；<br>    3.从 Hive Table 进行查询返回。   </p><h5 id="从-Spark-数据源进行创建"><a href="#从-Spark-数据源进行创建" class="headerlink" title="从 Spark 数据源进行创建"></a>从 Spark 数据源进行创建</h5><pre><code>scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]</code></pre><h5 id="从一个存在的-RDD-进行转换"><a href="#从一个存在的-RDD-进行转换" class="headerlink" title="从一个存在的 RDD 进行转换"></a>从一个存在的 RDD 进行转换</h5><h5 id="从-Hive-Table-进行查询返回"><a href="#从-Hive-Table-进行查询返回" class="headerlink" title="从 Hive Table 进行查询返回"></a>从 Hive Table 进行查询返回</h5><h3 id="SQL-语法"><a href="#SQL-语法" class="headerlink" title="SQL 语法"></a>SQL 语法</h3><p>SQL 语法风格是指我们查询数据的时候使用 SQL 语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助。    </p><ol><li><p>读取 JSON 文件创建 DataFrame  </p><p> scala&gt; val df &#x3D; spark.read.json(“data&#x2F;user.json”)<br> df: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， username: string]</p></li><li><p>对 DataFrame 创建一个临时表 </p><p> scala&gt; df.createOrReplaceTempView(“people”)</p></li><li><p>通过 SQL 语句实现查询全表  </p><p> scala&gt; val sqlDF &#x3D; spark.sql(“SELECT * FROM people”)<br> sqlDF: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， name: string]</p></li><li><p>结果展示</p><p> scala&gt; sqlDF.show<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|</p></li></ol><p>注意：普通临时表是 Session 范围内的，如果想应用范围内有效，可以使用全局临时表。使<br>用全局临时表时需要全路径访问，如：global_temp.people  </p><ol start="5"><li><p>对于 DataFrame 创建一个全局表  </p><p> scala&gt; df.createGlobalTempView(“people”)</p></li><li><p>通过 SQL 语句实现查询全表  </p><p> scala&gt; spark.sql(“SELECT * FROM global_temp.people”).show()<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|<br> +—+——–+<br> scala&gt; spark.newSession().sql(“SELECT * FROM global_temp.people”).show()<br> +—+——–+<br> |age|username|<br> +—+——–+<br> | 20|zhangsan|<br> | 30| lisi|<br> | 40| wangwu|<br> +—+——–+</p></li></ol><h3 id="DSL-语法"><a href="#DSL-语法" class="headerlink" title="DSL 语法"></a>DSL 语法</h3><p>DataFrame 提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。  </p><p>可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了。    </p><ol><li><p>创建一个 DataFrame  </p><p> scala&gt; val df &#x3D; spark.read.json(“data&#x2F;user.json”)<br> df: org.apache.spark.sql.DataFrame &#x3D; [age: bigint， name: string]</p></li><li><p>查看 DataFrame 的 Schema 信息  </p><p> scala&gt; df.printSchema<br> root<br> |– age: Long (nullable &#x3D; true)<br> |– username: string (nullable &#x3D; true)  </p></li><li><p>只查看”username”列数据  </p><p> scala&gt; df.select(“username”).show()<br> +——–+<br> |username|<br> +——–+<br> |zhangsan|<br> | lisi|<br> | wangwu|<br> +——–+  </p></li><li><p>查看”username”列数据以及”age+1”数据  </p><p> 注意:涉及到运算的时候, 每列都必须使用$, 或者采用引号表达式：单引号+字段名<br> scala&gt; df.select($”username”,$”age” + 1).show<br> scala&gt; df.select(‘username, ‘age + 1).show()  </p><p> scala&gt; df.select(‘username, ‘age + 1 as “newage”).show()<br> +——–+———+<br> |username|(age + 1)|<br> +——–+———+<br> |zhangsan| 21|<br> | lisi| 31|<br> | wangwu| 41|<br> +——–+———+  </p></li><li><p>查看”age”大于”30”的数据  </p><p> scala&gt; df.filter($”age”&gt;30).show<br> +—+———+<br> |age| username|<br> +—+———+<br> | 40| wangwu|<br> +—+———+  </p></li><li><p>按照”age”分组，查看数据条数  </p><p> scala&gt; df.groupBy(“age”).count.show<br> +—+—–+<br> |age|count|<br> +—+—–+<br> | 20| 1|<br> | 30| 1|<br> | 40| 1|<br> +—+—–+</p></li></ol><h3 id="RDD-转换为-DataFrame"><a href="#RDD-转换为-DataFrame" class="headerlink" title="RDD 转换为 DataFrame"></a>RDD 转换为 DataFrame</h3><p>在 IDEA 中开发程序时，如果需要 RDD 与 DF 或者 DS 之间互相操作，那么需要引入import spark.implicits._   </p><p>这里的 spark 不是 Scala 中的包名，而是创建的 sparkSession 对象的变量名称，所以必<br>须先创建 SparkSession 对象再导入。这里的 spark 对象不能使用 var 声明，因为 <strong>Scala 只支持val 修饰的对象的引入</strong>。    </p><h4 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h4><pre><code>scala&gt; val idRDD = sc.textFile(&quot;data/id.txt&quot;)scala&gt; idRDD.toDF(&quot;id&quot;).show    +---+    | id|    +---+    | 1|    | 2|    | 3|    | 4|     +---+  </code></pre><h4 id="通过样例类-RDD-DataFrame"><a href="#通过样例类-RDD-DataFrame" class="headerlink" title="通过样例类 RDD -&gt; DataFrame"></a>通过样例类 RDD -&gt; DataFrame</h4><p>实际开发中，一般通过样例类将 RDD 转换为 DataFrame  </p><pre><code>scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,40))).map(t=&gt;User(t._1, t._2)).toDF.show    +--------+---+    | name|age|    +--------+---+    |zhangsan| 30|    | lisi| 40|    +--------+---+    </code></pre><h3 id="DataFrame-转换为-RDD"><a href="#DataFrame-转换为-RDD" class="headerlink" title="DataFrame 转换为 RDD"></a>DataFrame 转换为 RDD</h3><p>DataFrame 其实就是对 RDD 的封装，所以可以直接获取内部的 RDD  </p><h4 id="df-rdd"><a href="#df-rdd" class="headerlink" title="df.rdd"></a>df.rdd</h4><pre><code>scala&gt; val rdd = df.rddrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] at rdd at &lt;console&gt;:25  </code></pre><p>注意：此时得到的 RDD 存储类型为 Row   </p><h3 id="DataSet-1"><a href="#DataSet-1" class="headerlink" title="DataSet"></a>DataSet</h3><p>DataSet 是具有强类型的数据集合，需要提供对应的类型信息。  </p><h4 id="创建-DataSet"><a href="#创建-DataSet" class="headerlink" title="创建 DataSet"></a>创建 DataSet</h4><h5 id="使用样例类序列创建-DataSet"><a href="#使用样例类序列创建-DataSet" class="headerlink" title="使用样例类序列创建 DataSet"></a>使用样例类序列创建 DataSet</h5><pre><code>scala&gt; case class Person(name: String, age: Long)defined class Personscala&gt; val caseClassDS = Seq(Person(&quot;zhangsan&quot;,2)).toDS()caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]scala&gt; caseClassDS.show    +---------+---+    | name|age|    +---------+---+    | zhangsan| 2|    +---------+---+    </code></pre><h5 id="使用基本类型的序列创建-DataSet"><a href="#使用基本类型的序列创建-DataSet" class="headerlink" title="使用基本类型的序列创建 DataSet"></a>使用基本类型的序列创建 DataSet</h5><pre><code>scala&gt; val ds = Seq(1,2,3,4,5).toDSds: org.apache.spark.sql.Dataset[Int] = [value: int]      scala&gt; ds.show    +-----+    |value|    +-----+    | 1|    | 2|    | 3|    | 4|    | 5|    +-----+    </code></pre><p>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet  </p><h3 id="RDD-转换为-DataSet"><a href="#RDD-转换为-DataSet" class="headerlink" title="RDD 转换为 DataSet"></a>RDD 转换为 DataSet</h3><p>SparkSQL 能够自动将包含有 case 类的 RDD 转换成 DataSet，case 类定义了 table 的结构，case 类属性通过反射变成了表的列名。Case 类可以包含诸如 Seq 或者 Array 等复杂的结构。  </p><h4 id="toDS"><a href="#toDS" class="headerlink" title="toDS"></a>toDS</h4><pre><code>scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).map(t=&gt;User(t._1, t._2)).toDSres11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</code></pre><h3 id="DataSet-转换为-RDD"><a href="#DataSet-转换为-RDD" class="headerlink" title="DataSet 转换为 RDD"></a>DataSet 转换为 RDD</h3><p>DataSet 其实也是对 RDD 的封装，所以可以直接获取内部的 RDD    </p><h4 id="ds-rdd"><a href="#ds-rdd" class="headerlink" title="ds.rdd"></a>ds.rdd</h4><pre><code>scala&gt; val rdd = res11.rddrdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at &lt;console&gt;:25</code></pre><h3 id="DataFrame-和-DataSet-转换"><a href="#DataFrame-和-DataSet-转换" class="headerlink" title="DataFrame 和 DataSet 转换"></a>DataFrame 和 DataSet 转换</h3><p>DataFrame 其实是 DataSet 的特例，所以它们之间是可以互相转换的。  </p><h4 id="DataFrame-转换为-DataSet"><a href="#DataFrame-转换为-DataSet" class="headerlink" title="DataFrame 转换为 DataSet"></a>DataFrame 转换为 DataSet</h4><h5 id="df-as-样例类"><a href="#df-as-样例类" class="headerlink" title="df.as[样例类]"></a>df.as[样例类]</h5><pre><code>scala&gt; case class User(name:String, age:Int)defined class User  scala&gt; val df = sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).toDF(&quot;name&quot;,&quot;age&quot;)df: org.apache.spark.sql.DataFrame = [name: string, age: int]scala&gt; val ds = df.as[User]ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]  </code></pre><h4 id="DataSet-转换为-DataFrame"><a href="#DataSet-转换为-DataFrame" class="headerlink" title="DataSet 转换为 DataFrame"></a>DataSet 转换为 DataFrame</h4><h5 id="ds-toDF"><a href="#ds-toDF" class="headerlink" title="ds.toDF"></a>ds.toDF</h5><pre><code>scala&gt; val ds = df.as[User]ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]scala&gt; val df = ds.toDFdf: org.apache.spark.sql.DataFrame = [name: string, age: int]  </code></pre><h3 id="RDD、DataFrame、DataSet-三者的关系"><a href="#RDD、DataFrame、DataSet-三者的关系" class="headerlink" title="RDD、DataFrame、DataSet 三者的关系"></a>RDD、DataFrame、DataSet 三者的关系</h3><p>同样的数据都给到这三个数据结构,计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。    </p><h4 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h4><p>都是 spark 平台下的分布式弹性数据集。  </p><p>都有惰性机制。  </p><p>三者有许多共同的函数，如 filter，排序等。  </p><p>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）  </p><p>三者都会根据 Spark 的内存情况自动缓存运算。  </p><p>三者都有 partition 的概念  </p><p>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型   </p><h4 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h4><p>RDD 不支持 sparksql 操作。  </p><p>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直<br>接访问，只有通过解析才能获取各个字段的值。  </p><p>DataFrame 与 DataSet 一般不与 spark mllib 同时使用。  </p><p>DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能<br>注册临时表&#x2F;视窗，进行 sql 语句操作。  </p><p>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然。    </p><p>DataFrame 其实就是 DataSet 的一个特例 type DataFrame &#x3D; Dataset[Row]  </p><p>DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了case class 之后可以很自由的获得每一行的信息。      </p><p><img src="/2023/08/11/Spark-SQL/3.png" alt="三者的相互转换">  </p><h2 id="IDEA开发SparkSQL"><a href="#IDEA开发SparkSQL" class="headerlink" title="IDEA开发SparkSQL"></a>IDEA开发SparkSQL</h2><p>实际开发中，都是使用 IDEA 进行开发的。   </p><h3 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h3><pre><code>&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>object SparkSQL01_Demo &#123; def main(args: Array[String]): Unit = &#123;     //创建上下文环境配置对象     val conf: SparkConf = new         SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL01_Demo&quot;)     //创建 SparkSession 对象         val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()         //RDD=&gt;DataFrame=&gt;DataSet 转换需要引入隐式转换规则，否则无法转换         //spark 不是包名，是上下文环境对象名         import spark.implicits._         //读取 json 文件 创建 DataFrame &#123;&quot;username&quot;: &quot;lisi&quot;,&quot;age&quot;: 18&#125;         val df: DataFrame = spark.read.json(&quot;input/test.json&quot;)         //df.show()         //SQL 风格语法         df.createOrReplaceTempView(&quot;user&quot;)         //spark.sql(&quot;select avg(age) from user&quot;).show         //DSL 风格语法         //df.select(&quot;username&quot;,&quot;age&quot;).show()         //*****RDD=&gt;DataFrame=&gt;DataSet*****         //RDD         val rdd1: RDD[(Int, String, Int)] =          spark.sparkContext.makeRDD(List((1,&quot;zhangsan&quot;,30),(2,&quot;lisi&quot;,28),(3,&quot;wangwu&quot;,20)))         //DataFrame         val df1: DataFrame = rdd1.toDF(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;)         //df1.show()         //DateSet         val ds1: Dataset[User] = df1.as[User]         //ds1.show()         //*****DataSet=&gt;DataFrame=&gt;RDD*****         //DataFrame         val df2: DataFrame = ds1.toDF()         //RDD 返回的 RDD 类型为 Row，里面提供的 getXXX 方法可以获取字段值，类似 jdbc 处理结果集，        但是索引从 0 开始         val rdd2: RDD[Row] = df2.rdd         //rdd2.foreach(a=&gt;println(a.getString(1)))         //*****RDD=&gt;DataSet*****         rdd1.map&#123;          case (id,name,age)=&gt;User(id,name,age)         &#125;.toDS()         //*****DataSet=&gt;=&gt;RDD*****         ds1.rdd         //释放资源         spark.stop()     &#125;    &#125;case class User(id:Int,name:String,age:Int)  </code></pre><h3 id="toDF和toDS的用法区别："><a href="#toDF和toDS的用法区别：" class="headerlink" title="toDF和toDS的用法区别："></a>toDF和toDS的用法区别：</h3><p>使用toDF时：  </p><pre><code>rdd1.toDF(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;)  指定字段名即可，字段类型会自动解析rdd中的数据进行获取。 </code></pre><p>使用toDS时：  </p><pre><code>case class User(id:Int,name:String,age:Int)   rdd1.toDS  </code></pre><h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><h4 id="创建-DataFrame-1"><a href="#创建-DataFrame-1" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h4><pre><code>scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]</code></pre><h4 id="注册-UDF"><a href="#注册-UDF" class="headerlink" title="注册 UDF"></a>注册 UDF</h4><pre><code>scala&gt; spark.udf.register(&quot;addName&quot;,(x:String)=&gt; &quot;Name:&quot;+x)res9: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))</code></pre><h4 id="创建临时表"><a href="#创建临时表" class="headerlink" title="创建临时表"></a>创建临时表</h4><pre><code>scala&gt; df.createOrReplaceTempView(&quot;people&quot;)</code></pre><h4 id="应用-UDF"><a href="#应用-UDF" class="headerlink" title="应用 UDF"></a>应用 UDF</h4><pre><code>scala&gt; spark.sql(&quot;Select addName(name),age from people&quot;).show()   </code></pre><h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><p>用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从 Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以统一采用强类型聚合函数Aggregator。  </p><h2 id="数据的加载和保存"><a href="#数据的加载和保存" class="headerlink" title="数据的加载和保存"></a>数据的加载和保存</h2><p>SparkSQL 默认读取和保存的文件格式为 parquet。  </p><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>如果读取不同格式的数据，可以对不同的数据格式进行设定  </p><pre><code>scala&gt; spark.read.format(&quot;…&quot;)[.option(&quot;…&quot;)].load(&quot;…&quot;)  ➢ format(&quot;…&quot;)：指定加载的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。➢ load(&quot;…&quot;)：在&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入加载数据的路径。➢ option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable     </code></pre><p>也可以直接在文件上进行查询: 文件格式.<code>文件路径</code></p><pre><code>scala&gt;spark.sql(&quot;select * from json.`/opt/module/data/user.json`&quot;).show  </code></pre><h3 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h3><p>df.write.save 是保存数据的通用方法  </p><pre><code>scala&gt;df.write.format(&quot;…&quot;)[.option(&quot;…&quot;)].save(&quot;…&quot;)  ➢ format(&quot;…&quot;)：指定保存的数据类型，包括&quot;csv&quot;、&quot;jdbc&quot;、&quot;json&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;。➢ save (&quot;…&quot;)：在&quot;csv&quot;、&quot;orc&quot;、&quot;parquet&quot;和&quot;textFile&quot;格式下需要传入保存数据的路径。➢ option(&quot;…&quot;)：在&quot;jdbc&quot;格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable  </code></pre><p>保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。  </p><pre><code>df.write.mode(&quot;append&quot;).json(&quot;/opt/module/data/output&quot;)   </code></pre><p><img src="/2023/08/11/Spark-SQL/4.png" alt="SaveMode枚举值">    </p><h3 id="修改默认数据源格式"><a href="#修改默认数据源格式" class="headerlink" title="修改默认数据源格式"></a>修改默认数据源格式</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式<br>存储格式。</p><p>修改配置项 spark.sql.sources.default，可修改默认数据源格式。   </p><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以<br>通过 SparkSession.read.json()去加载 JSON 文件。  </p><p>注意：Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串 </p><pre><code>&#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;， &quot;age&quot;:30&#125;[&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;,&#123;&quot;name&quot;:&quot;Justin&quot;， &quot;age&quot;:19&#125;]  </code></pre><h3 id="Spark读取本地Json文件的案例"><a href="#Spark读取本地Json文件的案例" class="headerlink" title="Spark读取本地Json文件的案例"></a>Spark读取本地Json文件的案例</h3><h4 id="导入隐式转换"><a href="#导入隐式转换" class="headerlink" title="导入隐式转换"></a>导入隐式转换</h4><pre><code>import spark.implicits._</code></pre><h4 id="加载-JSON-文件"><a href="#加载-JSON-文件" class="headerlink" title="加载 JSON 文件"></a>加载 JSON 文件</h4><pre><code>val path = &quot;/opt/module/spark-local/people.json&quot;val peopleDF = spark.read.json(path)  </code></pre><h4 id="创建临时表-1"><a href="#创建临时表-1" class="headerlink" title="创建临时表"></a>创建临时表</h4><pre><code>peopleDF.createOrReplaceTempView(&quot;people&quot;)</code></pre><h4 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h4><pre><code>val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)  teenagerNamesDF.show()    +------+    | name|    +------+    |Justin|    +------+  </code></pre><h3 id="Spark读取本地CSV文件的案例"><a href="#Spark读取本地CSV文件的案例" class="headerlink" title="Spark读取本地CSV文件的案例"></a>Spark读取本地CSV文件的案例</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为<br>数据列。  </p><pre><code>spark.read.format(&quot;csv&quot;).option(&quot;sep&quot;, &quot;;&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).load(&quot;data/user.csv&quot;)  </code></pre><h3 id="Spark通过JDBC连接Mysql的案例"><a href="#Spark通过JDBC连接Mysql的案例" class="headerlink" title="Spark通过JDBC连接Mysql的案例"></a>Spark通过JDBC连接Mysql的案例</h3><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对<br>DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用 spark-shell 操作，可在启动 shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下。  </p><pre><code>bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar  </code></pre><p>在 Idea 中通过 JDBC 对 Mysql 进行操作的案例代码如下  </p><h4 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h4><pre><code>&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.27&lt;/version&gt;&lt;/dependency&gt;  </code></pre><h4 id="读取数据-（选用方式一）"><a href="#读取数据-（选用方式一）" class="headerlink" title="读取数据 （选用方式一）"></a>读取数据 （选用方式一）</h4><pre><code>val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)//创建 SparkSession 对象val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()import spark.implicits._      //方式 1：通用的 load 方法读取spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;).option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123123&quot;).option(&quot;dbtable&quot;, &quot;user&quot;).load().show  //方式 2:通用的 load 方法读取 参数另一种形式spark.read.format(&quot;jdbc&quot;)    .options(Map(&quot;url&quot;-&gt;&quot;jdbc:mysql://linux1:3306/spark-sql?user=root&amp;password=123123&quot;,    &quot;dbtable&quot;-&gt;&quot;user&quot;,&quot;driver&quot;-&gt;&quot;com.mysql.jdbc.Driver&quot;)).load().show//方式 3:使用 jdbc 方法读取val props: Properties = new Properties()props.setProperty(&quot;user&quot;, &quot;root&quot;)props.setProperty(&quot;password&quot;, &quot;123123&quot;)val df: DataFrame = spark.read.jdbc(&quot;jdbc:mysql://linux1:3306/spark-sql&quot;, &quot;user&quot;, props)df.show  //释放资源spark.stop()    </code></pre><h4 id="写入数据-选用方式一"><a href="#写入数据-选用方式一" class="headerlink" title="写入数据  (选用方式一)"></a>写入数据  (选用方式一)</h4><pre><code>case class User2(name: String, age: Long)。。。val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)//创建 SparkSession 对象val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()  import spark.implicits._    val rdd: RDD[User2] = spark.sparkContext.makeRDD(List(User2(&quot;lisi&quot;, 20), User2(&quot;zs&quot;, 30)))val ds: Dataset[User2] = rdd.toDS    //方式 1：通用的方式 format 指定写出类型ds.write.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123123&quot;).option(&quot;dbtable&quot;, &quot;user&quot;).mode(SaveMode.Append).save()//方式 2：通过 jdbc 方法val props: Properties = new Properties()props.setProperty(&quot;user&quot;, &quot;root&quot;)props.setProperty(&quot;password&quot;, &quot;123123&quot;)ds.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql://linux1:3306/spark-sql&quot;, &quot;user&quot;, props)    //释放资源  spark.stop() </code></pre><h4 id="使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作"><a href="#使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作" class="headerlink" title="使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作"></a>使用Spark-SQL实现mysql数据库中表数据的逻辑处理操作</h4><pre><code>val conf: SparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;SparkSQL&quot;)//创建 SparkSession 对象val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()import spark.implicits._      //方式 1：通用的 load 方法读取res1 = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;).option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123123&quot;).option(&quot;dbtable&quot;, &quot;user&quot;).load()     res2 = spark.sql(&quot;select * from user1 where age &gt; 10&quot;)res2.write.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://linux1:3306/spark-sql&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;123123&quot;).option(&quot;dbtable&quot;, &quot;user&quot;).mode(SaveMode.Append).save() //释放资源  spark.stop()   </code></pre><h3 id="Spark操作Hive"><a href="#Spark操作Hive" class="headerlink" title="Spark操作Hive"></a>Spark操作Hive</h3><p>若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到<br>Spark 的配置文件目录中($SPARK_HOME&#x2F;conf)。   </p><p>需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 &#x2F;user&#x2F;hive&#x2F;warehouse 目录中(如果你的 classpath 中有配好的hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。   </p><p>spark-shell 默认是 Hive 支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。    </p><p>在实际使用中, 几乎没有任何人会使用内置的 Hive    </p><h4 id="Spark访问外部Hive的前置条件"><a href="#Spark访问外部Hive的前置条件" class="headerlink" title="Spark访问外部Hive的前置条件"></a>Spark访问外部Hive的前置条件</h4><p>如果想连接外部已经部署好的 Hive，需要通过以下几个步骤：  </p><pre><code>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下➢ 把 Mysql 的驱动 copy 到 jars/目录下➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下➢ 重启 spark-shell   </code></pre><h4 id="Spark-shell中访问Hive"><a href="#Spark-shell中访问Hive" class="headerlink" title="Spark-shell中访问Hive"></a>Spark-shell中访问Hive</h4><pre><code>scala&gt; spark.sql(&quot;show tables&quot;).show   </code></pre><h4 id="运行-Spark-SQL-CLI"><a href="#运行-Spark-SQL-CLI" class="headerlink" title="运行 Spark SQL CLI"></a>运行 Spark SQL CLI</h4><p>Spark SQL CLI 可以很方便的在本地运行 Hive 元数据服务以及从命令行执行查询任务。在Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一 Hive 窗口    </p><pre><code>bin/spark-sql    </code></pre><h4 id="运行-Spark-beeline"><a href="#运行-Spark-beeline" class="headerlink" title="运行 Spark beeline"></a>运行 Spark beeline</h4><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。Spark Thrift Server 的目的也只是取代HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。<br>如果想连接 Thrift Server，需要通过以下几个步骤：  </p><pre><code>➢ Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下➢ 把 Mysql 的驱动 copy 到 jars/目录下➢ 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下➢ 启动 Thrift Server    sbin/start-thriftserver.sh      </code></pre><h5 id="使用-beeline-连接-Thrift-Server"><a href="#使用-beeline-连接-Thrift-Server" class="headerlink" title="使用 beeline 连接 Thrift Server"></a>使用 beeline 连接 Thrift Server</h5><pre><code>bin/beeline -u jdbc:hive2://linux1:10000 -n root  </code></pre><h4 id="Spark操作Hive的代码示例"><a href="#Spark操作Hive的代码示例" class="headerlink" title="Spark操作Hive的代码示例"></a>Spark操作Hive的代码示例</h4><h5 id="导入依赖-1"><a href="#导入依赖-1" class="headerlink" title="导入依赖"></a>导入依赖</h5><pre><code>&lt;dependency&gt;     &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;     &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;     &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;     &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;     &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;     &lt;version&gt;1.2.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;     &lt;groupId&gt;mysql&lt;/groupId&gt;     &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;     &lt;version&gt;5.1.27&lt;/version&gt;&lt;/dependency&gt;  </code></pre><h5 id="拷贝Hive-Site-xml"><a href="#拷贝Hive-Site-xml" class="headerlink" title="拷贝Hive-Site.xml"></a>拷贝Hive-Site.xml</h5><p>将 hive-site.xml 文件拷贝到项目的 resources 目录中</p><h5 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h5><pre><code>//创建 SparkSessionval spark: SparkSession = SparkSession.builder().enableHiveSupport().master(&quot;local[*]&quot;).appName(&quot;sql&quot;).getOrCreate()  </code></pre><p>在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址:<br>config(“spark.sql.warehouse.dir”, “hdfs:&#x2F;&#x2F;linux1:8020&#x2F;user&#x2F;hive&#x2F;warehouse”)</p><p>代码最前面增加如下代码解决权限不足的问题：  </p><p>System.setProperty(“HADOOP_USER_NAME”, “root”)</p><p>此处的 root 改为你们自己的 hadoop 用户名称      </p><h5 id="整理后代码实现"><a href="#整理后代码实现" class="headerlink" title="整理后代码实现"></a>整理后代码实现</h5><pre><code>//创建 SparkSessionSystem.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)val spark: SparkSession = SparkSession.builder().config(&quot;spark.sql.warehouse.dir&quot;, &quot;hdfs://linux1:8020/user/hive/warehouse&quot;).enableHiveSupport().master(&quot;local[*]&quot;).appName(&quot;show databases&quot;).getOrCreate() </code></pre>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E6%97%A0%E5%86%A5%E5%86%A5%E4%B9%8B%E5%BF%97%E8%80%85%EF%BC%8C%E6%97%A0%E6%98%AD%E6%98%AD%E4%B9%8B%E6%98%8E%EF%BC%9B%E6%97%A0%E6%83%9B%E6%83%9B%E4%B9%8B%E4%BA%8B%E8%80%85%EF%BC%8C%E6%97%A0%E8%B5%AB%E8%B5%AB%E4%B9%8B%E5%8A%9F/">无冥冥之志者，无昭昭之明；无惛惛之事者，无赫赫之功</category>
      
      
      <comments>http://example.com/2023/08/11/Spark-SQL/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Java数据结构和算法</title>
      <link>http://example.com/2023/08/07/Java_datastrcut/</link>
      <guid>http://example.com/2023/08/07/Java_datastrcut/</guid>
      <pubDate>Mon, 07 Aug 2023 03:10:06 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;君子藏器于身，待时而动&quot;&gt;&lt;a href=&quot;#君子藏器于身，待时而动&quot; class=&quot;headerlink&quot; title=&quot;君子藏器于身，待时而动&quot;&gt;&lt;/a&gt;君子藏器于身，待时而动&lt;/h1&gt;&lt;h2 id=&quot;线性结构和非线性结构&quot;&gt;&lt;a href=&quot;#线性结构和非线</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="君子藏器于身，待时而动"><a href="#君子藏器于身，待时而动" class="headerlink" title="君子藏器于身，待时而动"></a>君子藏器于身，待时而动</h1><h2 id="线性结构和非线性结构"><a href="#线性结构和非线性结构" class="headerlink" title="线性结构和非线性结构"></a>线性结构和非线性结构</h2><h3 id="线性结构"><a href="#线性结构" class="headerlink" title="线性结构"></a>线性结构</h3><p>线性结构作为最常用的数据结构，其特点是数据元素之间存在一对一的线性关系  </p><p>线性结构有两种不同的存储结构，即顺序存储结构和链式存储结构。顺序存储的线性表称为顺序表，顺序表中的存储元素是连续的  </p><p>链式存储的线性表称为链表，链表中的存储元素不一定是连续的，元素节点中存放数据元素以及相邻元素的地址信息  </p><p>线性结构常见的有：数组、队列、链表和栈，后面我们会详细讲解   </p><p>非线性结构包括：二维数组，多维数组，广义表，树结构，图结构  </p><h1 id="稀疏数组"><a href="#稀疏数组" class="headerlink" title="稀疏数组"></a>稀疏数组</h1><p>当一个数组中大部分元素为０，或者为同一个值的数组时，可以使用稀疏数组来保存该数组。  </p><p>稀疏数组的处理方法是:  </p><p>记录数组一共有几行几列，有多少个不同的值  </p><p>把具有不同值的元素的行列及值记录在一个小规模的数组中，从而缩小程序的规模  </p><p><img src="/2023/08/07/Java_datastrcut/1.png" alt="稀疏数组">   </p><p><img src="/2023/08/07/Java_datastrcut/2.png" alt="稀疏数组转换思路">    </p><p>二维表转稀疏数组代码实现：    </p><pre><code>package com.zyy;public class SparseArray &#123;    public static void main(String[] args)&#123;        //创建一个原始的二维数组 11*11        // 0: 表示没有棋子，1表示黑子 2表示蓝子        int chessArr1[][] = new int[11][11];        chessArr1[1][2] = 1; //第二行第三列 有一颗黑子        chessArr1[2][3] = 2; //第三行第四列 有一颗蓝子        chessArr1[4][5] = 2; //第五行第六列 有一颗蓝子        //输出原始的二维数组        System.out.println(&quot;原始的二维数组~~&quot;);        //从二维数组中拿出每一行数据，返回为一维数组int[] row        for(int[] row:chessArr1) &#123;            //从拿到的每一行数据中拿到每一个值            for (int data:row)&#123;                System.out.printf(&quot;%d\t&quot;,data);            &#125;            System.out.println();        &#125;        // 将二维数组转稀疏数组的思想        //1.先遍历二维数组，得到非0数据的个数        int sum = 0;        for (int i = 0; i &lt; 11; i++)&#123;            for (int j = 0;j &lt; 11; j++)&#123;                if (chessArr1[i][j] != 0 )&#123;                    sum++;                &#125;            &#125;        &#125;        //2.创建对应的稀疏数组        //由统计出来的非0数个数+1，构成稀疏数组的行数        //稀疏数组的列数固定为3，记录行坐标，列坐标，值        int sparseArr[][] = new int[sum+1][3];        //给稀疏数组赋值        sparseArr[0][0] = 11;        sparseArr[0][1] = 11;        sparseArr[0][2] = sum;        //遍历二维数组，将非0的值存放到sparseArr中        int count = 0;  //count用于记录是第几个非0数据        for(int i = 0;i &lt; 11; i++)&#123;            for (int j = 0 ;j &lt; 11; j++)&#123;                if(chessArr1[i][j] != 0 )&#123;                    count++;                    //记录第count个非0数据的行i                    sparseArr[count][0] = i;                    //记录第count个非0数据的列j                    sparseArr[count][1] = j;                    //记录第count个非0数据的值                    sparseArr[count][2] = chessArr1[i][j];                &#125;            &#125;        &#125;        //输出稀疏数组的形式        System.out.println();        System.out.println(&quot;得到稀疏数组为~~~&quot;);        //二维数组结构为[[数组1],[数组2],[数组3]]        //所以sparseArr.length实际上是在统计外层一维数组的长度        for(int i = 0; i &lt; sparseArr.length;i++)&#123;            System.out.printf(&quot;%d\t%d\t%d\t\n&quot;,sparseArr[i][0],sparseArr[i][1],sparseArr[i][2]);        &#125;        System.out.println();        //将稀疏数组 -》 恢复成 原始的二维数组        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组，比如上面的chessArr2 = int[11][11]        //2.在读取稀疏数组后几行的数据，并赋值给原始的二维数组，即可        //1.先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组        int chessArr2[][] = new int[sparseArr[0][0]][sparseArr[0][1]];        //2.在读取稀疏数组后几行的数据（从第二行开始），并赋值给原始的二维数组即可        for (int i = 1; i &lt; sparseArr.length; i++)&#123;            chessArr2[sparseArr[i][0]][sparseArr[i][1]] = sparseArr[i][2];        &#125;        //输出恢复后的二维数组        System.out.println();        System.out.println(&quot;恢复后的二维数组&quot;);        for (int[] row:chessArr2)&#123;            for(int data:row)&#123;                System.out.printf(&quot;%d\t&quot;,data);            &#125;            //每行数据打印完之后，执行换行            System.out.println();        &#125;    &#125;&#125;原始的二维数组~~0000000000000100000000000200000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000得到稀疏数组为~~~11113121232452恢复后的二维数组0000000000000100000000000200000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000</code></pre><h1 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h1><p>队列是一个有序列表，可以用数组或者链表来实现。遵循先进先出原则（FIFO）</p><p><img src="/2023/08/07/Java_datastrcut/3.png" alt="数组模拟队列">  </p><h2 id="数组模拟队列"><a href="#数组模拟队列" class="headerlink" title="数组模拟队列"></a>数组模拟队列</h2><pre><code>package com.zyy;import java.util.Scanner;public class ArrayQueueDemo &#123;public static void main(String[] args) &#123;    // 测试一把    // 创建一个队列    ArrayQueue queue = new ArrayQueue(5);    char key ;//接收用户输入    Scanner scanner = new Scanner(System.in);    boolean loop = true;    //输出一个菜单    while(loop)&#123;        System.out.println(&quot;s(show):显示队列&quot;);        System.out.println(&quot;e(exit):退出程序&quot;);        System.out.println(&quot;a(add):添加数据到队列&quot;);        System.out.println(&quot;g(get):从队列取出数据&quot;);        System.out.println(&quot;h(head):查看队列头的数据&quot;);        key = scanner.next().charAt(0);//接收一个字符        switch(key)&#123;            case &#39;s&#39;:                queue.showQueue();                break;            case &#39;a&#39;:                System.out.println(&quot;输入一个数&quot;);                int value = scanner.nextInt();                queue.addQueue(value);                break;            case &#39;g&#39;://取出数据                try&#123;                    int res = queue.getQueue();                    System.out.printf(&quot;取出的数据是%d\n&quot;,res);                &#125;catch (Exception e)&#123;                    //TODO:handle exception                    System.out.println(e.getMessage());                &#125;                break;            case &#39;h&#39;://查看队列头的数据                try&#123;                    int res = queue.headQueue();                    System.out.printf(&quot;队列头的数据是%d\n&quot;,res);                &#125;catch(Exception e)&#123;                    //TODO:handle exception                    System.out.println(e.getMessage());                &#125;                break;            case &#39;e&#39;://退出                scanner.close();                loop = false;                break;            default:                break;        &#125;    &#125;    System.out.println(&quot;程序退出~~&quot;);&#125;    //使用数组模拟队列-编写一个ArrayQueue类static class ArrayQueue&#123;        private int maxSize;// 表示数组的最大容量        private int front;//队列头        private int rear;//队列尾        private int[] arr;//该数据用于存放数据，模拟队列        //创建队列的构造器        public ArrayQueue(int arrMaxSize)&#123;            maxSize = arrMaxSize;            arr = new int[maxSize];            front = -1;//指向队列头部，分析出front是指向队列头的前一个位置            rear = -1;//指向队列尾，指向队列尾的数据（即就是队列最后一个数据）        &#125;        //判断队列是否满        public boolean isFull()&#123;            return rear == maxSize - 1;        &#125;        //判断队列是否为空        public boolean isEmpty()&#123;            return rear == front;        &#125;        //添加数据到队列        public void addQueue(int n)&#123;            //判断队列是否满            if(isFull())&#123;                System.out.println(&quot;队列满，不能加入数据~&quot;);                return;            &#125;            rear++;//让rear后移            arr[rear] = n;        &#125;        //获取队列的数据，出队列        public int getQueue()&#123;            //判断队列是否为空            if(isEmpty())&#123;                //通过抛出异常                throw new RuntimeException(&quot;队列空，不能取数据&quot;);            &#125;            front++;//front后移            return arr[front];        &#125;        //显示队列的所有数据        public void showQueue()&#123;            //遍历            if(isEmpty())&#123;                System.out.println(&quot;队列空的，没有数据~~&quot;);                return;            &#125;            for(int i=0;i&lt;arr.length;i++)&#123;                System.out.printf(&quot;arr[%d]=%d\n&quot;,i,arr[i]);            &#125;        &#125;        //显示队列的头数据，注意不是取出数据        public int headQueue()&#123;            //判断            if(isEmpty())&#123;                throw new RuntimeException(&quot;队列空的，没有数据~~&quot;);            &#125;            return arr[front + 1];        &#125;    &#125;&#125;</code></pre><h3 id="使用数组模拟队列存在的问题："><a href="#使用数组模拟队列存在的问题：" class="headerlink" title="使用数组模拟队列存在的问题："></a>使用数组模拟队列存在的问题：</h3><p>数组只能使用一次，因为front和rear指针无法再回头指向已经走过的数组位置  </p><h3 id="优化方案："><a href="#优化方案：" class="headerlink" title="优化方案："></a>优化方案：</h3><p>通过取模运算，让front和rear指针能循环指向已经走过的数组位置，让数组复用  </p><p>分析说明:  </p><p>1):尾索引的下一个为头索引时表示队列满，即将队列容量空出一个作为约定，这个在做判断队列满的时候需要注意（rear+1）%maxSize &#x3D;&#x3D; front 满</p><p>2):rear &#x3D;&#x3D; front(空)</p><p>3):与数组模拟队列不同，数组模拟环形队列时，front指向队列的第一个元素，front的初始值为0 ，rear指向队列的最后一个元素的最后一个位置，因为希望空出一个空间作为约定，rear的初始值为0</p><p>4):队列中的有效数据个数计算方法:  </p><p>(rear+maxSize-front)%maxSize</p><p><img src="/2023/08/07/Java_datastrcut/4.png" alt="循环队列相关判断条件"></p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%90%9B%E5%AD%90%E8%97%8F%E5%99%A8%E4%BA%8E%E8%BA%AB%EF%BC%8C%E5%BE%85%E6%97%B6%E8%80%8C%E5%8A%A8/">君子藏器于身，待时而动</category>
      
      
      <comments>http://example.com/2023/08/07/Java_datastrcut/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hadoop学习笔记</title>
      <link>http://example.com/2023/08/06/hadoop/</link>
      <guid>http://example.com/2023/08/06/hadoop/</guid>
      <pubDate>Sun, 06 Aug 2023 02:45:20 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;博观而约取，厚积而薄发&quot;&gt;&lt;a href=&quot;#博观而约取，厚积而薄发&quot; class=&quot;headerlink&quot; title=&quot;博观而约取，厚积而薄发&quot;&gt;&lt;/a&gt;博观而约取，厚积而薄发&lt;/h1&gt;&lt;p&gt;相关学习文档&lt;br&gt;链接：&lt;a href=&quot;https://pan.</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="博观而约取，厚积而薄发"><a href="#博观而约取，厚积而薄发" class="headerlink" title="博观而约取，厚积而薄发"></a>博观而约取，厚积而薄发</h1><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA">https://pan.baidu.com/s/1WLK6GP99XAgJcX3FtrqjhA</a><br>提取码：mvcs   </p><h1 id="免密登录原理"><a href="#免密登录原理" class="headerlink" title="免密登录原理"></a>免密登录原理</h1><p><img src="/2023/08/06/hadoop/4.png" alt="免密登录原理">    </p><h1 id="HDFS架构概述"><a href="#HDFS架构概述" class="headerlink" title="HDFS架构概述"></a>HDFS架构概述</h1><p><img src="/2023/08/06/hadoop/1.png" alt="HDFS架构概述">  </p><p>HDFS适合一次写入，多次读出的场景，且不支持文件的修改  </p><p>HDFS的缺点：仅支持数据append，不支持文件的随机修改  </p><p><img src="/2023/08/06/hadoop/5.png" alt="HDFS组成架构">  </p><p><img src="/2023/08/06/hadoop/6.png" alt="HDFS组成架构">   </p><p>HDFS文件块大小：在Hadoop2.x版本中是128M  </p><p>寻址时间为传输时间的1%时，是最佳状态  </p><p>HDFS块的大小设置主要取决于磁盘传输速率  </p><h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><p><img src="/2023/08/06/hadoop/7.png" alt="HDFS写数据流程">    </p><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据    </p><h3 id="HDFS副本节点选择"><a href="#HDFS副本节点选择" class="headerlink" title="HDFS副本节点选择"></a>HDFS副本节点选择</h3><p><img src="/2023/08/06/hadoop/8.png" alt="HDFS副本节点选择">       </p><h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><p><img src="/2023/08/06/hadoop/9.png" alt="HDFS读数据流程">    </p><h1 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h1><p>NameNode中的元数据是存储在内存中，在内存上维护一个Edits文件，磁盘上维护一个FsImage文件，每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据  </p><p>需要定期进行FsImage和Edits的合并，由SecondaryNamenode完成，专门负责FsImage和Edits的合并  </p><h2 id="NameNode工作机制"><a href="#NameNode工作机制" class="headerlink" title="NameNode工作机制"></a>NameNode工作机制</h2><p><img src="/2023/08/06/hadoop/10.png" alt="NameNode工作机制">    </p><h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录    </p><p>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中  </p><p><img src="/2023/08/06/hadoop/11.png" alt="集群安全模式">    </p><p>（1）bin&#x2F;hdfs dfsadmin -safemode get（功能描述：查看安全模式状态）  </p><p>（2）bin&#x2F;hdfs dfsadmin -safemode enter  （功能描述：进入安全模式状态）  </p><p>（3）bin&#x2F;hdfs dfsadmin -safemode leave（功能描述：离开安全模式状态）  </p><p>（4）bin&#x2F;hdfs dfsadmin -safemode wait（功能描述：等待安全模式状态）  </p><h2 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h2><p><img src="/2023/08/06/hadoop/12.png" alt="DataNode工作机制">   </p><h3 id="DataNode如何保证数据完整性？"><a href="#DataNode如何保证数据完整性？" class="headerlink" title="DataNode如何保证数据完整性？"></a>DataNode如何保证数据完整性？</h3><p>1）当DataNode读取Block的时候，它会计算CheckSum。  </p><p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。  </p><p>3）Client读取其他DataNode上的Block。  </p><p>4）DataNode在其文件创建后周期验证CheckSum    </p><p><img src="/2023/08/06/hadoop/13.png" alt="DataNode数据完整性">    </p><p>HDFS中默认DataNode掉线的超时时长为10分钟+30秒    </p><h3 id="DataNode配置多目录"><a href="#DataNode配置多目录" class="headerlink" title="DataNode配置多目录"></a>DataNode配置多目录</h3><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本   </p><p>hdfs-site.xml  </p><pre><code>&lt;property&gt;    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;    &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt;</code></pre><h2 id="HDFS-HA故障转移机制"><a href="#HDFS-HA故障转移机制" class="headerlink" title="HDFS-HA故障转移机制"></a>HDFS-HA故障转移机制</h2><p><img src="/2023/08/06/hadoop/14.png" alt="HDFS-HA故障转移机制">   </p><h1 id="YARN架构概述"><a href="#YARN架构概述" class="headerlink" title="YARN架构概述"></a>YARN架构概述</h1><p><img src="/2023/08/06/hadoop/2.png" alt="YARN架构概述">    </p><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序  </p><p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成</p><h2 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h2><p><img src="/2023/08/06/hadoop/25.png" alt="Yarn工作机制">    </p><p>工作机制详解  </p><pre><code>（1）MR程序提交到客户端所在的节点。  （2）YarnRunner向ResourceManager申请一个Application。  （3）RM将该应用程序的资源路径返回给YarnRunner。（4）该程序将运行所需资源提交到HDFS上。（5）程序资源提交完毕后，申请运行mrAppMaster。（6）RM将用户的请求初始化成一个Task。（7）其中一个NodeManager领取到Task任务。（8）该NodeManager创建容器Container，并产生MRAppmaster。（9）Container从HDFS上拷贝资源到本地。（10）MRAppmaster向RM 申请运行MapTask资源。（11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。（12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。  （14）ReduceTask向MapTask获取相应分区的数据。（15）程序运行完毕后，MR会向RM申请注销自己</code></pre><p>总结：程序提交之后，找RM申请Application,告知运行程序需要的资源。RM生成一个资源分配Task任务放进yarn队列。这个Task会随机分配给Nodemanager，NodeManager领取任务后会根据资源要求创建Container容器，开始运行程序，运行完毕后向RM报告，注销资源占用和Task任务。  </p><h3 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h3><p>1.先进先出调度器（FIFO）  </p><p>2.容量调度器（Capacity Scheduler）  </p><p>3．公平调度器（Fair Scheduler）  </p><h1 id="MapReduce架构概述"><a href="#MapReduce架构概述" class="headerlink" title="MapReduce架构概述"></a>MapReduce架构概述</h1><p><img src="/2023/08/06/hadoop/3.png" alt="MapReduce架构概述">     </p><h2 id="MapReduce核心思想"><a href="#MapReduce核心思想" class="headerlink" title="MapReduce核心思想"></a>MapReduce核心思想</h2><p><img src="/2023/08/06/hadoop/15.png" alt="MapReduce核心思想">   </p><p>MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行  </p><h3 id="MapTask并行度决定机制"><a href="#MapTask并行度决定机制" class="headerlink" title="MapTask并行度决定机制"></a>MapTask并行度决定机制</h3><p>数据块：Block是HDFS物理上把数据分成一块一块。  </p><p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储  </p><p><img src="/2023/08/06/hadoop/16.png" alt="MapTask并行度决定机制">  </p><h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><p><img src="/2023/08/06/hadoop/17.png" alt="MapReduce工作流程">  </p><p><img src="/2023/08/06/hadoop/18.png" alt="MapReduce工作流程">  </p><p>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快,默认100M   </p><h2 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h2><p><img src="/2023/08/06/hadoop/19.png" alt="Shuffle机制">    </p><h2 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h2><p><img src="/2023/08/06/hadoop/20.png" alt="MapTask工作机制">    </p><h2 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a>ReduceTask工作机制</h2><p><img src="/2023/08/06/hadoop/21.png" alt="ReduceTask工作机制">  </p><p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置  </p><h1 id="Hadoop文件压缩"><a href="#Hadoop文件压缩" class="headerlink" title="Hadoop文件压缩"></a>Hadoop文件压缩</h1><p><img src="/2023/08/06/hadoop/22.png" alt="Hadoop文件压缩">  </p><h3 id="Hadoop文件压缩性能比较"><a href="#Hadoop文件压缩性能比较" class="headerlink" title="Hadoop文件压缩性能比较"></a>Hadoop文件压缩性能比较</h3><p><img src="/2023/08/06/hadoop/23.png" alt="Hadoop文件压缩性能比较">  </p><p>Gzip压缩：每个文件压缩后都在130M以内的（一个块大小内），都可以考虑Gzip压缩格式  ，不支持Split  </p><p>Bzip压缩：支持Split，压缩率很高，但压缩&#x2F;解压缩 速度慢  </p><p>LZO压缩：支持Split，合理的压缩率，是Hadoop中最流行的压缩格式    </p><p>Snappy压缩：不支持Split，压缩率比Gzip低，但Hadoop本身不支持，需要安装  </p><h3 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h3><p><img src="/2023/08/06/hadoop/24.png" alt="压缩位置选择">   </p><p>压缩可以在MapReduce作用的任意阶段启用    </p><p>速度是最优先考虑的因素，而不是压缩率  </p><h1 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h1><p>是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。   </p><p>ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功 能稳定的系统提供给用户    </p><h2 id="Zookeeper工作机制"><a href="#Zookeeper工作机制" class="headerlink" title="Zookeeper工作机制"></a>Zookeeper工作机制</h2><p><img src="/2023/08/06/hadoop/26.png" alt="Zookeeper工作机制">  </p><h3 id="Zookeeper特点"><a href="#Zookeeper特点" class="headerlink" title="Zookeeper特点"></a>Zookeeper特点</h3><p><img src="/2023/08/06/hadoop/27.png" alt="Zookeeper特点">    </p><h3 id="Zookeeper的数据结构"><a href="#Zookeeper的数据结构" class="headerlink" title="Zookeeper的数据结构"></a>Zookeeper的数据结构</h3><p><img src="/2023/08/06/hadoop/28.png" alt="Zookeeper的数据结构">    </p><h3 id="软负载均衡"><a href="#软负载均衡" class="headerlink" title="软负载均衡"></a>软负载均衡</h3><p>在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求  </p><h2 id="选举机制（面试重点）"><a href="#选举机制（面试重点）" class="headerlink" title="选举机制（面试重点）"></a>选举机制（面试重点）</h2><p><img src="/2023/08/06/hadoop/29.png" alt="Zookeeper选举机制-第一次启动">    </p><p><img src="/2023/08/06/hadoop/30.png" alt="Zookeeper选举机制-非第一次启动">   </p><h3 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h3><p>1）启动客户端    </p><pre><code>[atguigu@hadoop102 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop102:2181</code></pre><p>2）显示所有操作命令  </p><pre><code>[zk: hadoop102:2181(CONNECTED) 1] help  </code></pre><h4 id="znode-节点数据信息"><a href="#znode-节点数据信息" class="headerlink" title="znode 节点数据信息"></a>znode 节点数据信息</h4><p>1）查看当前znode中所包含的内容   </p><pre><code>[zk: hadoop102:2181(CONNECTED) 0] ls /  </code></pre><p>2）查看当前节点详细数据  </p><pre><code>[zk: hadoop102:2181(CONNECTED) 5] ls -s /[zookeeper]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x0cversion = -1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1    </code></pre><p>（1）czxid：创建节点的事务 zxid  </p><p>每次修改 ZooKeeper 状态都会产生一个 ZooKeeper 事务 ID。事务ID是 ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1在zxid2之前发生。  </p><p>（2）ctime：znode 被创建的毫秒数（从 1970 年开始）  </p><p>（3）mzxid：znode 最后更新的事务 zxid  </p><p>（4）mtime：znode 最后修改的毫秒数（从 1970 年开始）  </p><p>（5）pZxid：znode 最后更新的子节点 zxid    </p><p>（6）cversion：znode 子节点变化号，znode子节点修改次数  </p><p>（7）dataversion：znode 数据变化号  </p><p>（8）aclVersion：znode 访问控制列表的变化号  </p><p>（9）ephemeralOwner：如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。  </p><p>（10）dataLength：znode 的数据长度  </p><p>（11）numChildren：znode 子节点数量  </p><h4 id="节点类型（持久-短暂-有序号-无序号）"><a href="#节点类型（持久-短暂-有序号-无序号）" class="headerlink" title="节点类型（持久&#x2F;短暂&#x2F;有序号&#x2F;无序号）"></a>节点类型（持久&#x2F;短暂&#x2F;有序号&#x2F;无序号）</h4><p><img src="/2023/08/06/hadoop/31.png" alt="Zookeeper节点类型">   </p><p>1）分别创建2个普通节点（永久节点 + 不带序号）   </p><pre><code>[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;diaochan&quot; 注意：创建节点时，要赋值  </code></pre><p>2）获得节点的值  </p><pre><code>[zk: localhost:2181(CONNECTED) 5] get -s /sanguo  </code></pre><p>3）创建带序号的节点（永久节点 + 带序号）  </p><p>（1）先创建一个普通的根节点&#x2F;sanguo&#x2F;weiguo    </p><pre><code>[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot;</code></pre><p>（2）创建带序号的节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/zhangliao &quot;zhangliao&quot;  </code></pre><p>如果原来没有序号节点，序号从 0 开始依次递增。如果原节点下已有 2 个节点，则再排序时从 2 开始，以此类推。    </p><p>4）创建短暂节点（短暂节点 + 不带序号 or 带序号）   </p><p>（1）创建短暂的不带序号的节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot;</code></pre><p>（2）创建短暂的带序号的节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 2] create -e -s /sanguo/wuguo &quot;zhouyu&quot;    </code></pre><p>（3）在当前客户端是能查看到的  </p><pre><code>[zk: localhost:2181(CONNECTED) 3] ls /sanguo   </code></pre><p>（4）修改节点数据值   </p><pre><code>[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo &quot;simayi&quot;  </code></pre><h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><p>监听机制保证 ZooKeeper 保存的任何的数据的任何改变都能快速的响应到监听了该节点的应用程序。  </p><p><img src="/2023/08/06/hadoop/32.png" alt="监听器原理">      </p><h4 id="节点删除与查看"><a href="#节点删除与查看" class="headerlink" title="节点删除与查看"></a>节点删除与查看</h4><p>1）删除节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin</code></pre><p>2）递归删除节点  </p><pre><code>[zk: localhost:2181(CONNECTED) 15] deleteall /sanguo/shuguo</code></pre><p>3）查看节点状态  </p><pre><code>[zk: localhost:2181(CONNECTED) 17] stat /sanguo</code></pre><h4 id="客户端向服务端写数据流程"><a href="#客户端向服务端写数据流程" class="headerlink" title="客户端向服务端写数据流程"></a>客户端向服务端写数据流程</h4><p><img src="/2023/08/06/hadoop/33.png" alt="客户端向服务端写数据流程">  </p><h2 id="ZooKeeper-分布式锁"><a href="#ZooKeeper-分布式锁" class="headerlink" title="ZooKeeper 分布式锁"></a>ZooKeeper 分布式锁</h2><p>“进程1” 在使用该资源的时候，会先去获得锁，保持独占，这样其他进程就无法访问该资源,用完该资源以后就将锁释放掉,保证了分布式系统中多个进程能够有序的访问该临界资源。  </p><p><img src="/2023/08/06/hadoop/34.png" alt="Zookeeper分布式锁">      </p><h2 id="Zookeeper企业面试真题（面试重点）总结"><a href="#Zookeeper企业面试真题（面试重点）总结" class="headerlink" title="Zookeeper企业面试真题（面试重点）总结"></a>Zookeeper企业面试真题（面试重点）总结</h2><h3 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h3><p>半数机制，超过半数的投票通过，即通过。  </p><p>（1）第一次启动选举规则：  </p><pre><code>投票过半数时，服务器 id 大的胜出  </code></pre><p>（2）第二次启动选举规则： </p><pre><code>①EPOCH 大的直接胜出  ②EPOCH 相同，事务 id 大的胜出  ③事务 id 相同，服务器 id 大的胜出  </code></pre><p>生产集群安装多少 zk 合适？  </p><pre><code>安装奇数台  </code></pre><p>生产经验：  </p><pre><code>10 台服务器：3 台 zk  20 台服务器：5 台 zk  100 台服务器：11 台 zk  200 台服务器：11 台 zk    </code></pre><p>服务器台数多：好处，提高可靠性；坏处：提高通信延时  </p><p>常用命令  </p><pre><code>ls、get、create、delete</code></pre>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%8D%9A%E8%A7%82%E8%80%8C%E7%BA%A6%E5%8F%96%EF%BC%8C%E5%8E%9A%E7%A7%AF%E8%80%8C%E8%96%84%E5%8F%91/">博观而约取，厚积而薄发</category>
      
      
      <comments>http://example.com/2023/08/06/hadoop/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>JavaSE</title>
      <link>http://example.com/2023/08/04/JavaSE/</link>
      <guid>http://example.com/2023/08/04/JavaSE/</guid>
      <pubDate>Fri, 04 Aug 2023 09:50:53 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;天行健，君子以自强不息~  &lt;/p&gt;
&lt;h2 id=&quot;Java-核心机制-Java-虚拟机-JVM-java-virtual-machine&quot;&gt;&lt;a href=&quot;#Java-核心机制-Java-虚拟机-JVM-java-virtual-machine&quot; class=&quot;hea</description>
        
      
      
      
      <content:encoded><![CDATA[<p>天行健，君子以自强不息~  </p><h2 id="Java-核心机制-Java-虚拟机-JVM-java-virtual-machine"><a href="#Java-核心机制-Java-虚拟机-JVM-java-virtual-machine" class="headerlink" title="Java 核心机制-Java 虚拟机 [JVM java virtual machine]"></a>Java 核心机制-Java 虚拟机 [JVM java virtual machine]</h2><p>JVM 是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器，包含在JDK中  </p><p>对于不同的平台，有不同的虚拟机    </p><h2 id="什么是-JDK，JRE"><a href="#什么是-JDK，JRE" class="headerlink" title="什么是 JDK，JRE"></a>什么是 JDK，JRE</h2><h3 id="JDK-JRE-java-的开发工具-java-javac-javadoc-javap-等"><a href="#JDK-JRE-java-的开发工具-java-javac-javadoc-javap-等" class="headerlink" title="JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等]"></a>JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等]</h3><p>JDK 是提供给 Java 开发人员使用的，其中包含了 java 的开发工具，也包括了 JRE。所以安装了 JDK，就不用在单独安装JRE了  </p><h3 id="JRE-JVM-Java-的核心类库-类"><a href="#JRE-JVM-Java-的核心类库-类" class="headerlink" title="JRE &#x3D; JVM + Java 的核心类库[类]"></a>JRE &#x3D; JVM + Java 的核心类库[类]</h3><p>包括 Java 虚拟机(JVM Java Virtual Machine)和 Java 程序所需的核心类库等，如果想要运行一个开发好的 Java 程序，计算机中只需要安装 JRE 即可    </p><h4 id="Java-转义字符"><a href="#Java-转义字符" class="headerlink" title="Java 转义字符"></a>Java 转义字符</h4><p>\t ：一个制表位，实现对齐的功能  </p><p>\n ：换行符  </p><p>\ ：一个\  </p><p>&quot; :一个”  </p><p>&#39; ：一个’ \r :一个回车 System.out.println(“韩顺平教育\r 北京”);  </p><h4 id="Java-中的注释类型"><a href="#Java-中的注释类型" class="headerlink" title="Java 中的注释类型"></a>Java 中的注释类型</h4><ol><li><p>单行注释 &#x2F;&#x2F;  </p></li><li><p>多行注释 &#x2F;* *&#x2F;    </p><p> 多行注释里面不允许有多行注释嵌套</p></li><li><p>文档注释 &#x2F;** *&#x2F;</p></li></ol><p><strong>文档注释：</strong>  </p><p>&#x2F;**  </p><ul><li>@author 韩顺平  </li><li>@version 1.0<br>*&#x2F;</li></ul><h4 id="Java-代码规范"><a href="#Java-代码规范" class="headerlink" title="Java 代码规范"></a>Java 代码规范</h4><p><img src="/2023/08/04/JavaSE/1.png" alt="Java代码规范">    </p><h3 id="DOS-介绍"><a href="#DOS-介绍" class="headerlink" title="DOS 介绍"></a>DOS 介绍</h3><p>Dos： Disk Operating System 磁盘操作系统  </p><h4 id="常用的-dos-命令"><a href="#常用的-dos-命令" class="headerlink" title="常用的 dos 命令"></a>常用的 dos 命令</h4><ol><li><p>查看当前目录是有什么内容 dir  </p><p> dir dir d:\abc2\test200</p></li><li><p>切换到其他盘下：盘符号 cd : change directory</p></li></ol><p>案例演示：切换到 c 盘 cd &#x2F;D c:  </p><ol start="3"><li>切换到当前盘的其他目录下 (使用相对路径和绝对路径演示), ..\表示上一级目录</li></ol><p>案例演示： cd d:\abc2\test200 cd ....\abc2\test200  </p><ol start="4"><li>切换到上一级：</li></ol><p>案例演示： cd .. 5) 切换到根目录：cd \  </p><p>案例演示：cd \  </p><ol start="6"><li><p>查看指定的目录下所有的子级目录 tree  </p></li><li><p>清屏 cls [苍老师]  </p></li><li><p>退出 DOS exit</p></li></ol><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p><strong>变量三要素：变量名+值+数据类型</strong>  </p><p>变量相当于内存中一个数据存储空间的表示，你可以把变量看做是一个房间的门牌号，通过门牌号我们可以找到房间，而通过变量名可以访问到变量(值)    </p><ol><li><p>声明变量  </p><p> int a;  </p></li><li><p>赋值  </p><p> a &#x3D; 60; &#x2F;&#x2F;应该这么说: 把 60 赋给 a</p></li></ol><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>每一种数据都定义了明确的数据类型，在内存中分配了不同大小的内存空间(字节)。  </p><p><img src="/2023/08/04/JavaSE/2.png" alt="数据类型">    </p><h4 id="整型的类型"><a href="#整型的类型" class="headerlink" title="整型的类型"></a>整型的类型</h4><p><img src="/2023/08/04/JavaSE/3.png" alt="数据类型">     </p><p>&#x2F;&#x2F;Java 的整型常量（具体值）默认为 int 型，声明 long 型常量须后加‘l’或‘L’  </p><pre><code>int n1 = 1;//4 个字节long n3 = 1L;//长整型  </code></pre><h4 id="浮点类型"><a href="#浮点类型" class="headerlink" title="浮点类型"></a>浮点类型</h4><p><img src="/2023/08/04/JavaSE/4.png" alt="数据类型">    </p><p>&#x2F;&#x2F;Java 的浮点型常量(具体值)默认为 double 型，声明 float 型常量，须后加‘f’或‘F’  </p><pre><code>float num2 = 1.1F;    double num3 = 1.1; double num4 = 1.1f; </code></pre><p>十进制数形式：如：5.12 512.0f .512 (必须有小数点）   </p><p>Java类的组织形式  </p><p><img src="/2023/08/04/JavaSE/5.png" alt="Java类的组织形式">   </p><h4 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h4><p><img src="/2023/08/04/JavaSE/6.png" alt="字符编码">  </p><h4 id="基本数据类型转换"><a href="#基本数据类型转换" class="headerlink" title="基本数据类型转换"></a>基本数据类型转换</h4><h5 id="自动类型转换"><a href="#自动类型转换" class="headerlink" title="自动类型转换"></a>自动类型转换</h5><p><img src="/2023/08/04/JavaSE/7.png" alt="自动类型转换">    </p><h5 id="强制类型转换"><a href="#强制类型转换" class="headerlink" title="强制类型转换"></a>强制类型转换</h5><p>自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符 ( )，但可能造成精度降低或溢出,格外要注意  </p><p><img src="/2023/08/04/JavaSE/8.png" alt="强制类型转换">  </p><h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><h3 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h3><p>说明逻辑运算规则：  </p><ol><li><p>a&amp;b : &amp; 叫逻辑与：规则：当 a 和 b 同时为 true ,则结果为 true, 否则为 false  </p></li><li><p>a&amp;&amp;b : &amp;&amp; 叫短路与：规则：当 a 和 b 同时为 true ,则结果为 true,否则为 false  </p></li><li><p>a|b : | 叫逻辑或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p></li><li><p>a||b : || 叫短路或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false  </p></li><li><p>!a : 叫取反，或者非运算。当 a 为 true, 则结果为 false, 当 a 为 false 是，结果为 true  </p></li><li><p>a^b: 叫逻辑异或，当 a 和 b 不同时，则结果为 true, 否则为 false</p></li></ol><p><strong>&amp;&amp; 和 &amp; 使用区别</strong>  </p><ol><li><p>&amp;&amp;短路与：如果第一个条件为 false，则第二个条件不会判断，最终结果为 false，效率高  </p></li><li><p>&amp; 逻辑与：不管第一个条件是否为 false，第二个条件都要判断，效率低</p></li></ol><p><strong>|| 和 | 使用区别</strong>  </p><ol><li><p>||短路或：如果第一个条件为 true，则第二个条件不会判断，最终结果为 true，效率高  </p></li><li><p>| 逻辑或：不管第一个条件是否为 true，第二个条件都要判断，效率低</p></li></ol><h3 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h3><p>条件表达式 ? 表达式 1: 表达式 2;  </p><p>运算规则：  </p><p>1.如果条件表达式为 true，运算后的结果是表达式 1；  </p><p>2.如果条件表达式为 false，运算后的结果是表达式 2；  </p><p>口诀: [一灯大师：一真大师]  </p><h5 id="接收控制台输入Scanner"><a href="#接收控制台输入Scanner" class="headerlink" title="接收控制台输入Scanner"></a>接收控制台输入Scanner</h5><pre><code>import java.util.Scanner;    Scanner myScanner = new Scanner(System.in);  </code></pre><h5 id="原码、反码、补码-重点-难点"><a href="#原码、反码、补码-重点-难点" class="headerlink" title="原码、反码、补码(重点 难点)"></a>原码、反码、补码(重点 难点)</h5><p><img src="/2023/08/04/JavaSE/9.png">    </p><h2 id="程序控制结构"><a href="#程序控制结构" class="headerlink" title="程序控制结构"></a>程序控制结构</h2><p>主要有三大流程控制语句。  </p><ol><li><p>顺序控制  </p></li><li><p>分支控制    </p><ol><li>单分支 if  </li><li>双分支 if-else  </li><li>多分支 if-else if -….-else  </li><li>switch分支<br> switch(表达式){<br>         case xxx<br>                 }</li></ol></li><li><p>循环控制</p></li></ol><p><strong>for 循环控制</strong><br>    for(循环变量初始化;循环条件;循环变量迭代){<br>        循环操作(可以多条语句)<br>            }  </p><pre><code>eg:  for(int i = 1;i&lt;=10;i++)&#123;    System.out.println(&quot;Hello World ~ ！&quot;)&#125;</code></pre><p><strong>while 循环控制</strong>  </p><pre><code>循环变量初始化;  while（循环条件）&#123;    循环体（语句）；    循环变量迭代；&#125;  eg:int i = 1;while (i &lt;= 10)&#123;    System.out.println(&quot;Hello World ~ !&quot;)      i ++ &#125;  </code></pre><p><strong>do..while 循环控制</strong>  </p><pre><code>循环变量初始化;do&#123;    循环体(语句);    循环变量迭代;&#125;while(循环条件);  </code></pre><p>先执行，再判断，也就是说，一定会至少执行一次    </p><h2 id="跳转控制语句-break"><a href="#跳转控制语句-break" class="headerlink" title="跳转控制语句-break"></a>跳转控制语句-break</h2><p>break 语句用于终止某个语句块的执行，一般使用在 switch 或者循环[for , while , do-while]中  </p><h2 id="跳转控制语句-continue"><a href="#跳转控制语句-continue" class="headerlink" title="跳转控制语句-continue"></a>跳转控制语句-continue</h2><p>continue 语句用于结束本次循环，继续执行下一次循环  </p><h2 id="跳转控制语句-return"><a href="#跳转控制语句-return" class="headerlink" title="跳转控制语句-return"></a>跳转控制语句-return</h2><p>return 使用在方法，表示跳出所在的方法  </p><h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>数组可以存放多个同一类型的数据。数组也是一种数据类型，是<strong>引用类型</strong>  </p><p><img src="/2023/08/04/JavaSE/10.png" alt="数组的使用">  </p><pre><code>方式一：int a[] = new Int[5]   方式二：       int[] a;       a = new Int[10];   方式三：      int a[] = &#123;2,3,4,5,6&#125;  </code></pre><h3 id="数组赋值机制"><a href="#数组赋值机制" class="headerlink" title="数组赋值机制"></a>数组赋值机制</h3><p><img src="/2023/08/04/JavaSE/11.png" alt="数组的使用">      </p><h1 id="面向对象编程-基础部分"><a href="#面向对象编程-基础部分" class="headerlink" title="面向对象编程(基础部分)"></a>面向对象编程(基础部分)</h1><p>类和对象的区别和联系</p><ol><li><p>类是抽象的，概念的，代表一类事物,比如人类,猫类.., 即它是数据类型  </p></li><li><p>对象是具体的，实际的，代表一个具体事物, 即 是实例   </p></li><li><p>类是对象的模板，对象是类的一个个体，对应一个实例</p></li></ol><h2 id="对象在内存中存在形式-重要的-必须搞清楚"><a href="#对象在内存中存在形式-重要的-必须搞清楚" class="headerlink" title="对象在内存中存在形式(重要的)必须搞清楚"></a>对象在内存中存在形式(重要的)必须搞清楚</h2><p><img src="/2023/08/04/JavaSE/12.png" alt="对象在内存中存在形式">     </p><h3 id="类和对象的内存分配机制"><a href="#类和对象的内存分配机制" class="headerlink" title="类和对象的内存分配机制"></a>类和对象的内存分配机制</h3><p>Java 内存的结构分析  </p><ol><li><p>栈： 一般存放基本数据类型(局部变量)  </p></li><li><p>堆： 存放对象(Cat cat , 数组等)  </p></li><li><p>方法区：常量池(常量，比如字符串)， 类加载信息</p></li></ol><h2 id="属性-成员变量-字段"><a href="#属性-成员变量-字段" class="headerlink" title="属性&#x2F;成员变量&#x2F;字段"></a>属性&#x2F;成员变量&#x2F;字段</h2><p>成员变量 &#x3D; 属性 &#x3D; field(字段)  </p><p>属性是类的一个组成部分，一般是基本数据类型,也可是引用类型(对象，数组)    </p><h2 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h2><p>Cat cat1 &#x3D; new Cat();  </p><h3 id="成员方法"><a href="#成员方法" class="headerlink" title="成员方法"></a>成员方法</h3><pre><code>class Person &#123;    String name;    int age;      //方法(成员方法)      public void speak() &#123;        System.out.println(&quot;我是一个好人&quot;);        &#125;    &#125;  </code></pre><h3 id="方法的调用机制原理"><a href="#方法的调用机制原理" class="headerlink" title="方法的调用机制原理"></a>方法的调用机制原理</h3><p><img src="/2023/08/04/JavaSE/13.png">  </p><h3 id="成员方法的好处"><a href="#成员方法的好处" class="headerlink" title="成员方法的好处"></a>成员方法的好处</h3><ol><li><p>提高代码的复用性  </p></li><li><p>可以将实现的细节封装起来，然后供其他用户来调用即可</p></li></ol><h3 id="成员方法的定义"><a href="#成员方法的定义" class="headerlink" title="成员方法的定义"></a>成员方法的定义</h3><pre><code>访问修饰符 返回数据类型 方法名（形参列表..） &#123;    //方法体语句；    return 返回值;&#125;</code></pre><h3 id="传参"><a href="#传参" class="headerlink" title="传参"></a>传参</h3><p>引用类型传递的是地址（传递也是值，但是值是地址），可以通过形参影响实参！  </p><h3 id="方法递归调用-非常非常重要，比较难"><a href="#方法递归调用-非常非常重要，比较难" class="headerlink" title="方法递归调用(非常非常重要，比较难)"></a>方法递归调用(非常非常重要，比较难)</h3><p>递归就是方法自己调用自己  </p><p>递归重要规则  </p><p><img src="/2023/08/04/JavaSE/14.png"></p><h2 id="方法重载-OverLoad"><a href="#方法重载-OverLoad" class="headerlink" title="方法重载(OverLoad)"></a>方法重载(OverLoad)</h2><p>java 中允许同一个类中，多个同名方法的存在，但要求 形参列表不一致！  </p><p>案例：类：MyCalculator 方法：calculate  </p><ol><li>calculate(int n1, int n2) &#x2F;&#x2F;两个整数的和  </li><li>calculate(int n1, double n2) &#x2F;&#x2F;一个整数，一个 double 的和  </li><li>calculate(double n2, int n1)&#x2F;&#x2F;一个 double ,一个 Int 和  </li><li>calculate(int n1, int n2,int n3)&#x2F;&#x2F;三个 int 的和</li></ol><p><img src="/2023/08/04/JavaSE/15.png" alt="方法重载">  </p><h3 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h3><p>java 允许将同一个类中多个同名同功能但参数个数不同的方法，封装成一个方法。<br>就可以通过可变参数实现</p><p>eg:方法 sum 【可以计算 2 个数的和，3 个数的和 ， 4. 5， 。。】  </p><pre><code>//1. int... 表示接受的是可变参数，类型是 int ,即可以接收多个 int(0-多)//2. 使用可变参数时，可以当做数组来使用 即 nums 可以当做数组//3. 遍历 nums 求和即可public int sum(int... nums) &#123;//System.out.println(&quot;接收的参数个数=&quot; + nums.length);int res = 0;for(int i = 0; i &lt; nums.length; i++) &#123;res += nums[i];&#125;return res;&#125;&#125;  </code></pre><h3 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h3><p>变量：  </p><p>1.全局变量（属性）</p><p>2.局部变量（局部变量一般是指在成员方法中定义的变量）</p><p>全局变量和局部变量可以重名</p><p><img src="/2023/08/04/JavaSE/16.png" alt="变量作用域">  </p><p>全局变量和局部变量的区别  </p><p><img src="/2023/08/04/JavaSE/17.png" alt="全局变量和局部变量的区别">    </p><h3 id="构造方法-构造器"><a href="#构造方法-构造器" class="headerlink" title="构造方法&#x2F;构造器"></a>构造方法&#x2F;构造器</h3><p>在创建人类的对象时，就直接指定这个对象的年龄和姓名，该怎么做? 这时就可以使用构造器  </p><p>[修饰符] 方法名(形参列表){<br>方法体;<br>}   </p><ol><li><p>构造器的修饰符可以默认， 也可以是 public protected private  </p></li><li><p>构造器没有返回值   </p></li><li><p>方法名 和类名字必须一样  </p></li><li><p>参数列表 和 成员方法一样的规则  </p></li><li><p>构造器的调用, 由系统完成</p></li></ol><p>构造方法又叫构造器(constructor)，是类的一种特殊的方法，它的主要作用是完成对新对象的初始化  </p><p><img src="/2023/08/04/JavaSE/18.png" alt="构造器使用注意事项"></p><p><img src="/2023/08/04/JavaSE/19.png" alt="构造器使用注意事项"></p><h3 id="this-关键字"><a href="#this-关键字" class="headerlink" title="this 关键字"></a>this 关键字</h3><p><img src="/2023/08/04/JavaSE/20.png" alt="This关键字"></p><ol><li><p>this 关键字可以用来访问本类的属性、方法、构造器  </p></li><li><p>this 用于区分当前类的属性和局部变量  </p></li><li><p>访问成员方法的语法：this.方法名(参数列表);    </p></li><li><p>访问构造器语法：this(参数列表); 注意只能在构造器中使用(即只能在构造器中访问另外一个构造器, 必须放在第一条语句)  </p></li><li><p>this 不能在类定义的外部使用，只能在类定义的方法中使用。</p></li></ol><h1 id="面向对象编程-中级部分"><a href="#面向对象编程-中级部分" class="headerlink" title="面向对象编程(中级部分)"></a>面向对象编程(中级部分)</h1><p>IDEA 常用快捷键  </p><ol><li><p>删除当前行, 默认是 ctrl + Y 自己配置 ctrl + d  </p></li><li><p>复制当前行, 自己配置 ctrl + alt + 向下光标  </p></li><li><p>补全代码 alt + &#x2F;  </p></li><li><p>添加注释和取消注释 ctrl + &#x2F; 【第一次是添加注释，第二次是取消注释】  </p></li><li><p>导入该行需要的类 先配置 auto import , 然后使用 alt+enter 即可  </p></li><li><p>快速格式化代码 ctrl + alt + L  </p></li><li><p>快速运行程序 自己定义 alt + R  </p></li><li><p>生成构造器等 alt + insert [提高开发效率]  </p></li><li><p>查看一个类的层级关系 ctrl + H [学习继承后，非常有用]  </p></li><li><p>将光标放在一个方法上，输入 ctrl + B , 可以定位到方法 [学继承后，非常有用]  </p></li><li><p>自动的分配变量名 , 通过 在后面假 .var [老师最喜欢的]  </p></li><li><p>还有很多其它的快捷键</p></li></ol><h2 id="包"><a href="#包" class="headerlink" title="包"></a>包</h2><p><img src="/2023/08/04/JavaSE/21.png" alt="包">  </p><p>包的本质 </p><p><img src="/2023/08/04/JavaSE/22.png">  </p><p>包的命名：  </p><p>com.公司名.项目名.业务模块名  </p><h3 id="Java常用的包"><a href="#Java常用的包" class="headerlink" title="Java常用的包"></a>Java常用的包</h3><p>一个包下,包含很多的类,java 中常用的包有:  </p><ol><li><p>java.lang.* &#x2F;&#x2F;lang 包是基本包，默认引入，不需要再引入.  </p></li><li><p>java.util.* &#x2F;&#x2F;util 包，系统提供的工具包, 工具类，使用 Scanner  </p></li><li><p>java.net.* &#x2F;&#x2F;网络包，网络开发  </p></li><li><p>java.awt.* &#x2F;&#x2F;是做 java 的界面开发，GUI</p></li></ol><p>引入包的语法：  </p><p>import 包;  </p><p>eg:  import java.util.*  &#x2F;&#x2F;表示将java.util包所有都引入    </p><p>我们需要使用到哪个类，就导入哪个类即可，不建议使用*导入   </p><h2 id="访问修饰符"><a href="#访问修饰符" class="headerlink" title="访问修饰符"></a>访问修饰符</h2><p>java 提供四种访问控制修饰符号，用于控制方法和属性(成员变量)的访问权限（范围）   </p><ol><li><p>公开级别:用 public 修饰,对外公开  </p></li><li><p>受保护级别:用 protected 修饰,对子类和同一个包中的类公开  </p></li><li><p>默认级别:没有修饰符号,向同一个包的类公开.   </p></li><li><p>私有级别:用 private 修饰,只有类本身可以访问,不对外公开</p></li></ol><p><img src="/2023/08/04/JavaSE/23.png" alt="访问控制符">  </p><p><img src="/2023/08/04/JavaSE/24.png" alt="访问控制符使用说明">   </p><h2 id="面向对象编程三大特征"><a href="#面向对象编程三大特征" class="headerlink" title="面向对象编程三大特征"></a>面向对象编程三大特征</h2><p>封装、继承和多态    </p><h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><p><img src="/2023/08/04/JavaSE/25.png" alt="封装"></p><p>封装的实现步骤    </p><p><img src="/2023/08/04/JavaSE/26.png" alt="封装的实现步骤">    </p><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>继承可以解决代码复用,让我们的编程更加靠近人类思维.当多个类存在相同的属性(变量)和方法时,可以从这些类中抽象出父类,在父类中定义这些相同的属性和方法，所有的子类不需要重新定义这些属性和方法，只需要通过 extends 来声明继承父类即可  </p><p><img src="/2023/08/04/JavaSE/27.png" alt="继承">  </p><p>继承的基本语法：  </p><p>class 子类 extends 父类 {</p><p>}<br>子类就会自动拥有父类定义的属性和方法<br>子类又叫超类，基类<br>子类又叫派生类  </p><p>继承给编程带来的便利  </p><ol><li><p>代码的复用性提高了  </p></li><li><p>代码的扩展性和维护性提高了</p></li></ol><p>**继承的细节问题： ** </p><ol><li><p>子类继承了所有的属性和方法，非私有的属性和方法可以在子类直接访问, 但是私有属性和方法不能在子类直接访问，要通过父类提供公共的方法去访问  </p></li><li><p>子类必须调用父类的构造器,完成父类的初始化  </p></li><li><p>当创建子类对象时，不管使用子类的哪个构造器，默认情况下总会去调用父类的无参构造器，如果父类没有提供无参构造器，则必须在子类的构造器中用 super 去指定使用父类的哪个构造器完成对父类的初始化工作，否则，编译不会通过(怎么理解。)   </p></li><li><p>如果希望指定去调用父类的某个构造器，则显式的调用一下 : super(参数列表)  </p></li><li><p>super 在使用时，必须放在构造器第一行(super 只能在构造器中使用)  </p></li><li><p>super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器  </p></li><li><p>java 所有类都是 Object 类的子类, Object 是所有类的基类.  </p></li><li><p>父类构造器的调用不限于直接父类！将一直往上追溯直到 Object 类(顶级父类)    </p></li><li><p>子类最多只能继承一个父类(指直接继承)，即 java 中是单继承机制。<br>思考：如何让 A 类继承 B 类和 C 类？ 【A 继承 B， B 继承 C】  </p></li><li><p>不能滥用继承，子类和父类之间必须满足 is-a 的逻辑关系</p></li></ol><p><strong>输入 ctrl + H 可以看到类的继承关系</strong></p><pre><code>public class Sub extends Base &#123; //子类    public Sub(String name, int age) &#123;    //1. 调用父类的无参构造器, 如下或者什么都不写,默认就是调用 super()    //super();//父类的无参构造器    //2. 调用父类的 Base(String name) 构造器    //super(&quot;hsp&quot;);    //调用父类的 Base(String name, int age) 构造器    super(&quot;king&quot;, 20);    //细节： super 在使用时，必须放在构造器第一行    //细节: super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器    //this() 不能再使用了    System.out.println(&quot;子类 Sub(String name, int age)构造器被调用....&quot;);    &#125;</code></pre><p>子类创建的内存布局  </p><p><img src="/2023/08/04/JavaSE/28.png" alt="子类创建的内存布局">    </p><h3 id="super-关键字"><a href="#super-关键字" class="headerlink" title="super 关键字"></a>super 关键字</h3><p>super 代表父类的引用，用于访问父类的属性、方法、构造器  </p><p><img src="/2023/08/04/JavaSE/29.png" alt="super关键字">  </p><p>&#x2F;&#x2F; (1)先找本类，如果有，则调用  </p><p>&#x2F;&#x2F; (2)如果没有，则找父类(如果有，并可以调用，则调用)  </p><p>&#x2F;&#x2F; (3)如果父类没有，则继续找父类的父类,整个规则，就是一样的,直到 Object 类  </p><p>&#x2F;&#x2F; 提示：如果查找方法的过程中，找到了，但是不能访问， 则报错, cannot access  </p><p>&#x2F;&#x2F; 如果查找方法的过程中，没有找到，则提示方法不存在    </p><p><img src="/2023/08/04/JavaSE/30.png" alt="Super关键字的用法细节">      </p><p><strong>super 和 this 的比较</strong>    </p><p><img src="/2023/08/04/JavaSE/31.png" alt="super和this的比较">    </p><h2 id="方法重写-覆盖-override"><a href="#方法重写-覆盖-override" class="headerlink" title="方法重写&#x2F;覆盖(override)"></a>方法重写&#x2F;覆盖(override)</h2><p><img src="/2023/08/04/JavaSE/32.png" alt="方法重写">  </p><p><img src="/2023/08/04/JavaSE/33.png" alt="方法重写的注意事项">    </p><p><strong>方法重载和方法重写的区别</strong>  </p><p><img src="/2023/08/04/JavaSE/34.png" alt="方法重载和方法重写的区别">  </p><h2 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h2><p>多态是建立在封装和继承基础之上的<br><img src="/2023/08/04/JavaSE/35.png" alt="多态">     </p><p>多态的具体体现  </p><ol><li>方法的多态</li></ol><p>重写和重载就体现多态    </p><ol start="2"><li>对象的多态 (核心，困难，重点)</li></ol><p><img src="/2023/08/04/JavaSE/36.png" alt="多态案例">     </p><p>多态的前提是：两个对象(类)存在继承关系  </p><p>多态的向上转型   </p><p><img src="/2023/08/04/JavaSE/37.png" alt="多态的向上转型">   </p><p>多态向下转型  </p><p><img src="/2023/08/04/JavaSE/38.png" alt="多态的向下转型">    </p><p>属性没有重写之说！属性的值看编译类型   </p><p>instanceOf 比较操作符，用于判断对象的运行类型是否为 XX 类型或 XX 类型的子类型  </p><p>java 的动态绑定机制(非常非常重要.)  page 365</p><p>JDBC page 1119  </p><p>正则表达式 page 1210</p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E5%A4%A9%E8%A1%8C%E5%81%A5%EF%BC%8C%E5%90%9B%E5%AD%90%E4%BB%A5%E8%87%AA%E5%BC%BA%E4%B8%8D%E6%81%AF/">天行健，君子以自强不息</category>
      
      
      <comments>http://example.com/2023/08/04/JavaSE/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数仓建模</title>
      <link>http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/</link>
      <guid>http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/</guid>
      <pubDate>Thu, 03 Aug 2023 10:25:47 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;人生在勤，不索何获&quot;&gt;&lt;a href=&quot;#人生在勤，不索何获&quot; class=&quot;headerlink&quot; title=&quot;人生在勤，不索何获&quot;&gt;&lt;/a&gt;人生在勤，不索何获&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://help.aliyun.com/zh/datawo</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="人生在勤，不索何获"><a href="#人生在勤，不索何获" class="headerlink" title="人生在勤，不索何获"></a>人生在勤，不索何获</h1><p><a href="https://help.aliyun.com/zh/dataworks/user-guide/dataworks-data-modeling/?spm=a2c4g.11186623.0.0.6cbf2c36NLQ1IR">阿里数仓建模理论</a></p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E4%BA%BA%E7%94%9F%E5%9C%A8%E5%8B%A4%EF%BC%8C%E4%B8%8D%E7%B4%A2%E4%BD%95%E8%8E%B7/">人生在勤，不索何获</category>
      
      
      <comments>http://example.com/2023/08/03/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>kafka3.0.0学习记录</title>
      <link>http://example.com/2023/08/03/kafka/</link>
      <guid>http://example.com/2023/08/03/kafka/</guid>
      <pubDate>Thu, 03 Aug 2023 02:18:49 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;黄沙百战穿金甲，不破楼兰终不还！   &lt;/p&gt;
&lt;p&gt;相关学习文档&lt;br&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw&quot;&gt;https://pan.baidu.com/s/1GAiGG8E6vI94YO</description>
        
      
      
      
      <content:encoded><![CDATA[<p>黄沙百战穿金甲，不破楼兰终不还！   </p><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw">https://pan.baidu.com/s/1GAiGG8E6vI94YOI7_JZCAw</a><br>提取码：85vn   </p><h1 id="1-Kafka概述"><a href="#1-Kafka概述" class="headerlink" title="1.Kafka概述"></a>1.Kafka概述</h1><h2 id="1-1定义："><a href="#1-1定义：" class="headerlink" title="1.1定义："></a>1.1定义：</h2><p>Kafka传 统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（Message<br>Queue），主要应用于大数据实时处理领域。    </p><p>发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息<br>分为不同的类别，订阅者只接收感兴趣的消息。  </p><h2 id="1-2消息队列"><a href="#1-2消息队列" class="headerlink" title="1.2消息队列"></a>1.2消息队列</h2><p>大数据场景主要采用 Kafka 作为消息队列。  </p><h3 id="1-2-1传统消息队列的应用场景"><a href="#1-2-1传统消息队列的应用场景" class="headerlink" title="1.2.1传统消息队列的应用场景"></a>1.2.1传统消息队列的应用场景</h3><p>传统的消息队列的主要应用场景包括：缓存&#x2F;消峰、解耦和异步通信。</p><p>缓冲&#x2F;消峰：解决生产消息和消费消息的处理速度不一致的情况。 </p><p>解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p><p><img src="/2023/08/03/kafka/kafka1.png" alt="解耦"> </p><p>异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。  </p><h3 id="1-2-2消息队列的两种模式"><a href="#1-2-2消息队列的两种模式" class="headerlink" title="1.2.2消息队列的两种模式"></a>1.2.2消息队列的两种模式</h3><p><img src="/2023/08/03/kafka/2.png" alt="消息队列的两种模式">  </p><h2 id="1-3kafka基础架构"><a href="#1-3kafka基础架构" class="headerlink" title="1.3kafka基础架构"></a>1.3kafka基础架构</h2><p><img src="/2023/08/03/kafka/3.png" alt="kafka基础架构">   </p><p>（1）Producer：消息生产者，就是向 Kafka broker 发消息的客户端。  </p><p>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。  </p><p>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。消费者组内每个消<br>费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不<br>影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。  </p><p>（4）Broker：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个broker 可以容纳多个 topic。  </p><p>（5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个 topic。  </p><p>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。  </p><p>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个Follower。  </p><p>（8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数<br>据的对象都是 Leader。  </p><p>（9）Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和<br>Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Leader。    </p><h2 id="2kafka快速入门"><a href="#2kafka快速入门" class="headerlink" title="2kafka快速入门"></a>2kafka快速入门</h2><p>kafka命令行操作    </p><p>1）查看操作主题命令参数    </p><pre><code> bin/kafka-topics.sh    </code></pre><p>2）查看当前服务器中的所有 topic  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list</code></pre><p>3）创建 first topic  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 1 --replication-factor 3 --topic first  </code></pre><p>选项说明：    </p><pre><code>--topic 定义 topic 名  --replication-factor 定义副本数  --partitions 定义分区数  </code></pre><p>4）查看 first 主题的详情  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first</code></pre><p>5）修改分区数（注意：分区数只能增加，不能减少）  </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3 </code></pre><p>6）删除 topic    </p><pre><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first</code></pre><h2 id="生产者命令行操作"><a href="#生产者命令行操作" class="headerlink" title="生产者命令行操作"></a>生产者命令行操作</h2><p>发送消息</p><pre><code>bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</code></pre><h2 id="消费者命令行操作"><a href="#消费者命令行操作" class="headerlink" title="消费者命令行操作"></a>消费者命令行操作</h2><h3 id="消费-first-主题中的数据"><a href="#消费-first-主题中的数据" class="headerlink" title="消费 first 主题中的数据"></a>消费 first 主题中的数据</h3><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first</code></pre><h3 id="把主题中所有的数据都读取出来（包括历史数据）"><a href="#把主题中所有的数据都读取出来（包括历史数据）" class="headerlink" title="把主题中所有的数据都读取出来（包括历史数据）"></a>把主题中所有的数据都读取出来（包括历史数据）</h3><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first  </code></pre><h2 id="生产者重要参数"><a href="#生产者重要参数" class="headerlink" title="生产者重要参数"></a>生产者重要参数</h2><p>acks </p><pre><code>1）0：生产者发送过来的数据，不需要等数据落盘应答。  2）1：生产者发送过来的数据，Leader 收到数据后应答。  3）-1（all）：生产者发送过来的数据，Leader+和 isr 队列里面的所有节点收齐数据后应答。默认值是-1，-1 和all 是等价的。  </code></pre><p>enable.idempotence  </p><p>是否开启幂等性，默认 true，开启幂等性。  </p><p>compression.type  </p><p>生产者发送的所有数据的压缩方式。默认是 none，也就是不压缩。  </p><p>支持压缩类型：none、gzip、snappy、lz4 和 zstd。    </p><h2 id="生产者分区"><a href="#生产者分区" class="headerlink" title="生产者分区"></a>生产者分区</h2><h3 id="1-分区好处"><a href="#1-分区好处" class="headerlink" title="1.分区好处"></a>1.分区好处</h3><p>（1）便于合理使用存储资源  </p><p>每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一<br>块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果   </p><p>（2）提高并行度  </p><p>生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。  </p><h2 id="生产经验——生产者如何提高吞吐量"><a href="#生产经验——生产者如何提高吞吐量" class="headerlink" title="生产经验——生产者如何提高吞吐量"></a>生产经验——生产者如何提高吞吐量</h2><p>batch.size：批次大小，默认16k   </p><p>linger.ms：等待时间，修改为5-100ms  </p><p>compression.type：压缩snappy   </p><p>RecordAccumulator：缓冲区大小，修改为64m  </p><h2 id="ack-应答原理"><a href="#ack-应答原理" class="headerlink" title="ack 应答原理"></a>ack 应答原理</h2><p><img src="/2023/08/03/kafka/4.png" alt="ack应答原理">  </p><p><img src="/2023/08/03/kafka/5.png" alt="ack应答原理"> </p><p>数据完全可靠条件 &#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2  </p><h3 id="可靠性总结："><a href="#可靠性总结：" class="headerlink" title="可靠性总结："></a>可靠性总结：</h3><p>acks&#x3D;0，生产者发送过来数据就不管了，可靠性差，效率高；  </p><p>acks&#x3D;1，生产者发送过来数据Leader应答，可靠性中等，效率中等；  </p><p>acks&#x3D;-1，生产者发送过来数据Leader和ISR队列里面所有Follwer应答，可靠性高，效率低；  </p><p>在生产环境中，acks&#x3D;0很少使用；acks&#x3D;1，一般用于传输普通日志，允许丢个别数据；acks&#x3D;-1，一般用于传输和钱相关的数据，对可靠性要求比较高的场景。    </p><h2 id="生产经验——数据去重"><a href="#生产经验——数据去重" class="headerlink" title="生产经验——数据去重"></a>生产经验——数据去重</h2><p>至少一次（At Least Once）&#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2  </p><p>最多一次（At Most Once）&#x3D; ACK级别设置为0  </p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>At Least Once可以保证数据不丢失，但是不能保证数据不重复；  </p><p>At Most Once可以保证数据不重复，但是不能保证数据不丢失。  </p><p>精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。  </p><p>Kafka 0.11版本以后，引入了一项重大特性：幂等性和事务。  </p><h3 id="幂等性原理"><a href="#幂等性原理" class="headerlink" title="幂等性原理"></a>幂等性原理</h3><p>幂等性就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。  </p><p>精确一次（Exactly Once） &#x3D; 幂等性 + 至少一次（ ack&#x3D;-1 + 分区副本数&gt;&#x3D;2 + ISR最小副本数量&gt;&#x3D;2）  </p><p>幂等性只能保证的是在单分区单会话内不重复。  </p><h4 id="如何使用幂等性"><a href="#如何使用幂等性" class="headerlink" title="如何使用幂等性"></a>如何使用幂等性</h4><p>开启参数 enable.idempotence 默认为 true，false 关闭。    </p><p>开启事务，必须开启幂等性。  </p><h2 id="生产经验——数据乱序"><a href="#生产经验——数据乱序" class="headerlink" title="生产经验——数据乱序"></a>生产经验——数据乱序</h2><p>）kafka在1.x版本之前保证数据单分区有序，条件如下：  </p><p>max.in.flight.requests.per.connection&#x3D;1（不需要考虑是否开启幂等性）。  </p><p>2）kafka在1.x及以后版本保证数据单分区有序，条件如下：    </p><p>（1）未开启幂等性  </p><p>max.in.flight.requests.per.connection需要设置为1。  </p><p>（2）开启幂等性   </p><p>max.in.flight.requests.per.connection需要设置小于等于5。    </p><h2 id="Kafka-副本"><a href="#Kafka-副本" class="headerlink" title="Kafka 副本"></a>Kafka 副本</h2><h3 id="副本基本信息"><a href="#副本基本信息" class="headerlink" title="副本基本信息"></a>副本基本信息</h3><p>（1）Kafka 副本作用：提高数据可靠性。  </p><p>（2）Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；  </p><p>（3）Kafka 中副本分为：Leader 和 Follower。    </p><p>（4）Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。AR &#x3D; ISR + OSR  </p><p>ISR，表示和 Leader 保持同步的 Follower 集合。如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值由 replica.lag.time.max.ms参数设定，默认 30s。Leader 发生故障之后，就会从 ISR 中选举新的 Leader    </p><p>OSR，表示 Follower 与 Leader 副本同步时，延迟过多的副本。    </p><h3 id="Leader-选举流程"><a href="#Leader-选举流程" class="headerlink" title="Leader 选举流程"></a>Leader 选举流程</h3><p>Kafka 集群中有一个 broker 的 Controller 会被选举为 Controller Leader，负责管理集群broker 的上下线，所有 topic 的分区副本分配和 Leader 选举等工作。  </p><p><img src="/2023/08/03/kafka/6.png" alt="Leader选举流程">  </p><h3 id="Leader-和-Follower-故障处理细节"><a href="#Leader-和-Follower-故障处理细节" class="headerlink" title="Leader 和 Follower 故障处理细节"></a>Leader 和 Follower 故障处理细节</h3><p><img src="/2023/08/03/kafka/7.png" alt="Follower故障处理细节">   </p><p><img src="/2023/08/03/kafka/8.png" alt="Leader故障处理细节"></p><h3 id="生产经验——Leader-Partition-负载平衡"><a href="#生产经验——Leader-Partition-负载平衡" class="headerlink" title="生产经验——Leader Partition 负载平衡"></a>生产经验——Leader Partition 负载平衡</h3><p>正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p><p>auto.leader.rebalance.enable 默认是 true。 自动 Leader Partition 平衡。生产环境中，leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭。   </p><p>leader.imbalance.per.broker.percentage 默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器<br>会触发 leader 的平衡。  </p><p>leader.imbalance.check.interval.seconds 默认值 300 秒。检查 leader 负载是否平衡的间隔时间。  </p><h3 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h3><p>1）Topic 数据的存储机制</p><p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。  </p><p><img src="/2023/08/03/kafka/9.png" alt="Kafka文件存储机制">   </p><p>Topic数据存储位置：每个broker节点的 kafka&#x2F;datas&#x2F;目录下</p><p>通过工具查看 index 和 log 信息<br>[atguigu@hadoop104 first-1]$ kafka-run-class.sh kafka.tools.DumpLogSegments –files .&#x2F;00000000000000000000.index   </p><pre><code>Dumping ./00000000000000000000.index  offset: 3 position: 152 </code></pre><h4 id="Log文件和Index文件详解"><a href="#Log文件和Index文件详解" class="headerlink" title="Log文件和Index文件详解"></a>Log文件和Index文件详解</h4><p><img src="/2023/08/03/kafka/10.png" alt="Log文件和Index文件详解">    </p><p>log.segment.bytes Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，默认值 1G。  </p><p>log.index.interval.bytes 默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。 稀疏索引。    </p><h3 id="文件清理策略"><a href="#文件清理策略" class="headerlink" title="文件清理策略"></a>文件清理策略</h3><p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。    </p><p>Kafka 中提供的日志清理策略有 delete 和 compact 两种。  </p><p>1）delete 日志删除：将过期数据删除<br> log.cleanup.policy &#x3D; delete 所有数据启用删除策略   </p><p>（1）基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳。  </p><p>（2）基于大小：默认关闭。超过设置的所有日志总大小，删除最早的 segment。log.retention.bytes，默认等于-1，表示无穷大。  </p><p>2）compact 日志压缩  </p><p>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。    </p><p>log.cleanup.policy &#x3D; compact 所有数据启用压缩策略</p><p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。  </p><p>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。  </p><p><img src="/2023/08/03/kafka/11.png" alt="compact日志压缩">  </p><h2 id="高效读写数据"><a href="#高效读写数据" class="headerlink" title="高效读写数据"></a>高效读写数据</h2><p>1）Kafka 本身是分布式集群，可以采用分区技术，并行度高  </p><p>2）读数据采用稀疏索引，可以快速定位要消费的数据  </p><p>3）顺序写磁盘  </p><p>   顺序写之所以快，是因为其省去了大量磁头寻址的时间。  </p><p>4）页缓存 + 零拷贝技术     </p><h2 id="Kafka-消费者"><a href="#Kafka-消费者" class="headerlink" title="Kafka 消费者"></a>Kafka 消费者</h2><h3 id="Kafka-消费方式"><a href="#Kafka-消费方式" class="headerlink" title="Kafka 消费方式"></a>Kafka 消费方式</h3><p>pull（拉）模 式：</p><p>consumer采用从broker中主动拉取数据。Kafka采用这种方式。</p><p>push（推）模式：  </p><p>Kafka没有采用这种方式，因为由broker决定消息发送速率，很难适应所有消费者的消费速率。    </p><p>pull模式不足之处是，如 果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据。  </p><p>Kafka 消费者总体工作流程  </p><p><img src="/2023/08/03/kafka/12.png" alt="kafka消费总体工作流程">    </p><p>消费者组原理    </p><p>Consumer Group（CG）：消费者组，由多个consumer组成。形成一个消费者组的条件，是所有消费者的groupid相同。  </p><p>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费。  </p><p>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。    </p><p><img src="/2023/08/03/kafka/13.png" alt="消费者组原理">  </p><h2 id="生产经验——分区的分配以及再平衡"><a href="#生产经验——分区的分配以及再平衡" class="headerlink" title="生产经验——分区的分配以及再平衡"></a>生产经验——分区的分配以及再平衡</h2><p>Kafka有四种主流的分区分配策略： Range、RoundRobin、Sticky、CooperativeSticky。可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略  </p><h3 id="Range-以及再平衡"><a href="#Range-以及再平衡" class="headerlink" title="Range 以及再平衡"></a>Range 以及再平衡</h3><p>Range 是对每个 topic 而言的。首先对同一个 topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。  </p><p>容易产生数据倾斜！    </p><p>Kafka 默认的分区分配策略就是 Range + CooperativeSticky   </p><h3 id="RoundRobin-以及再平衡"><a href="#RoundRobin-以及再平衡" class="headerlink" title="RoundRobin 以及再平衡"></a>RoundRobin 以及再平衡</h3><p>RoundRobin 轮询分区策略，是把所有的 partition 和所有的consumer 都列出来，然后按照 hashcode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。  </p><h3 id="Sticky-以及再平衡"><a href="#Sticky-以及再平衡" class="headerlink" title="Sticky 以及再平衡"></a>Sticky 以及再平衡</h3><p>粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，<br>考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。<br>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。   </p><h2 id="offset-位移"><a href="#offset-位移" class="headerlink" title="offset 位移"></a>offset 位移</h2><p>从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets</p><p>__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是group.id+topic+分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact，也就是每个 group.id+topic+分区号就保留最新数据。  </p><h3 id="自动提交-offset"><a href="#自动提交-offset" class="headerlink" title="自动提交 offset"></a>自动提交 offset</h3><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。  </p><p>自动提交offset的相关参数：  </p><p>enable.auto.commit：是否开启自动提交offset功能，默认是true  </p><p>auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s  </p><h3 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h3><p>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次提交的一批数据最高的偏移量提交；不同点是，同步提交阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故有可能提交失败。  </p><p>commitSync（同步提交）：必须等待offset提交完毕，再去消费下一批数据。  </p><p>commitAsync（异步提交） ：发送完提交offset请求后，就开始消费下一批数据了。  </p><h3 id="指定-Offset-消费"><a href="#指定-Offset-消费" class="headerlink" title="指定 Offset 消费"></a>指定 Offset 消费</h3><pre><code>auto.offset.reset = earliest | latest | none 默认是 latest</code></pre><p>（1）earliest：自动将偏移量重置为最早的偏移量，–from-beginning。  </p><p>（2）latest（默认值）：自动将偏移量重置为最新偏移量。  </p><p>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。  </p><p>（4）任意指定 offset 位移开始消费    </p><h2 id="漏消费和重复消费"><a href="#漏消费和重复消费" class="headerlink" title="漏消费和重复消费"></a>漏消费和重复消费</h2><p>重复消费：已经消费了数据，但是 offset 没提交。 </p><p>漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。  </p><h3 id="生产经验——消费者事务"><a href="#生产经验——消费者事务" class="headerlink" title="生产经验——消费者事务"></a>生产经验——消费者事务</h3><p>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交<br>offset过程做原子绑定。  </p><h3 id="生产经验——数据积压（消费者如何提高吞吐量）"><a href="#生产经验——数据积压（消费者如何提高吞吐量）" class="headerlink" title="生产经验——数据积压（消费者如何提高吞吐量）"></a>生产经验——数据积压（消费者如何提高吞吐量）</h3><p><img src="/2023/08/03/kafka/14.png" alt="数据积压">    </p><h2 id="Kafka-Kraft-模式"><a href="#Kafka-Kraft-模式" class="headerlink" title="Kafka-Kraft 模式"></a>Kafka-Kraft 模式</h2><p><img src="/2023/08/03/kafka/15.png" alt="kafka-kraft架构">  </p><p>右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。  </p><p>这样做的好处有以下几个：  </p><p>Kafka 不再依赖外部框架，而是能够独立运行；    </p><p>controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升；  </p><p>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制；  </p><p>controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强  </p><p>controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。  </p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E9%BB%84%E6%B2%99%E7%99%BE%E6%88%98%E7%A9%BF%E9%87%91%E7%94%B2%EF%BC%8C%E4%B8%8D%E7%A0%B4%E6%A5%BC%E5%85%B0%E7%BB%88%E4%B8%8D%E8%BF%98/">黄沙百战穿金甲，不破楼兰终不还</category>
      
      
      <comments>http://example.com/2023/08/03/kafka/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark学习笔记</title>
      <link>http://example.com/2023/08/02/Spark-Core/</link>
      <guid>http://example.com/2023/08/02/Spark-Core/</guid>
      <pubDate>Wed, 02 Aug 2023 02:51:06 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;纵浪大化中，不喜亦不惧&quot;&gt;&lt;a href=&quot;#纵浪大化中，不喜亦不惧&quot; class=&quot;headerlink&quot; title=&quot;纵浪大化中，不喜亦不惧 ~&quot;&gt;&lt;/a&gt;纵浪大化中，不喜亦不惧 ~&lt;/h1&gt;&lt;p&gt;相关学习文档&lt;br&gt;链接：&lt;a href=&quot;https://</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="纵浪大化中，不喜亦不惧"><a href="#纵浪大化中，不喜亦不惧" class="headerlink" title="纵浪大化中，不喜亦不惧 ~"></a>纵浪大化中，不喜亦不惧 ~</h1><p>相关学习文档<br>链接：<a href="https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g">https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g</a><br>提取码：mg5w   </p><h1 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h1><h2 id="Spark-是什么？"><a href="#Spark-是什么？" class="headerlink" title="Spark 是什么？"></a>Spark 是什么？</h2><p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎     </p><p>Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎  </p><p>Spark Core 中提供了 Spark 最基础与最核心的功能  </p><p>Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据   </p><p>Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API  </p><p>Spark 一直被认为是 Hadoop 框架的升级版。  </p><h2 id="Mapreduce是什么？"><a href="#Mapreduce是什么？" class="headerlink" title="Mapreduce是什么？"></a>Mapreduce是什么？</h2><p>MapReduce 是一种编程模型，作为 Hadoop 的分布式计算模型，是 Hadoop 的核心    </p><h2 id="HBase是什么？"><a href="#HBase是什么？" class="headerlink" title="HBase是什么？"></a>HBase是什么？</h2><p>HBase 是一个基于 HDFS 的分布式数据库，擅长实时地随机读&#x2F;写超大规模数据集  </p><h2 id="Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）"><a href="#Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）" class="headerlink" title="Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）"></a>Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）</h2><p>Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘  </p><p>Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互  </p><p>Spark 的缓存机制比 HDFS 的缓存机制高效  </p><h2 id="什么时候选用Spark什么时候选用Mapreduce？"><a href="#什么时候选用Spark什么时候选用Mapreduce？" class="headerlink" title="什么时候选用Spark什么时候选用Mapreduce？"></a>什么时候选用Spark什么时候选用Mapreduce？</h2><p>Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark并不能完全替代 MR。  </p><h2 id="提交Spark应用的代码示例"><a href="#提交Spark应用的代码示例" class="headerlink" title="提交Spark应用的代码示例"></a>提交Spark应用的代码示例</h2><h3 id="local模式"><a href="#local模式" class="headerlink" title="local模式"></a>local模式</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ./examples/jars/spark-examples_2.12-3.0.0.jar 101) --class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序  2) --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟 CPU 核数量  3) spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包  4) 数字 10 表示程序的入口参数，用于设定当前应用的任务数量  </code></pre><h3 id="Yarn模式-（生产：Cluster模式）"><a href="#Yarn模式-（生产：Cluster模式）" class="headerlink" title="Yarn模式  （生产：Cluster模式）"></a>Yarn模式  （生产：Cluster模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2.12-3.0.0.jar 10  </code></pre><h3 id="Yarn模式-（测试：Client模式）"><a href="#Yarn模式-（测试：Client模式）" class="headerlink" title="Yarn模式  （测试：Client模式）"></a>Yarn模式  （测试：Client模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.12-3.0.0.jar 10  </code></pre><h4 id="Spark的端口号"><a href="#Spark的端口号" class="headerlink" title="Spark的端口号"></a>Spark的端口号</h4><pre><code>➢ Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）  ➢ Spark Master 内部通信服务端口号：7077  ➢ Standalone 模式下，Spark Master Web 端口号：8080（资源）  ➢ Spark 历史服务器端口号：18080  ➢ Hadoop YARN 任务运行情况查看端口号：8088     </code></pre><h2 id="Spark运行架构"><a href="#Spark运行架构" class="headerlink" title="Spark运行架构"></a>Spark运行架构</h2><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构  </p><p><img src="/2023/08/02/Spark-Core/1.png" alt="Spark运行架构">  </p><p>Driver表示Master，负责管理整个集群中的作业调度  </p><p>Executor表示slave，负责实际执行任务  </p><h3 id="Spark核心组件"><a href="#Spark核心组件" class="headerlink" title="Spark核心组件"></a>Spark核心组件</h3><p>Driver和Executor &amp; Master 和 Worker </p><h4 id="Driver的作用"><a href="#Driver的作用" class="headerlink" title="Driver的作用"></a>Driver的作用</h4><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。  </p><p>Driver 在 Spark 作业执行时主要负责：  </p><pre><code>➢ 将用户程序转化为作业（job）  ➢ 在 Executor 之间调度任务(task)  ➢ 跟踪 Executor 的执行情况  ➢ 通过 UI 展示查询运行情况      </code></pre><p>所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。  </p><h4 id="Executor的作用"><a href="#Executor的作用" class="headerlink" title="Executor的作用"></a>Executor的作用</h4><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立  </p><p>Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。  </p><h5 id="Executor-有两个核心功能："><a href="#Executor-有两个核心功能：" class="headerlink" title="Executor 有两个核心功能："></a>Executor 有两个核心功能：</h5><pre><code>➢ 负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程➢ 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。  </code></pre><h3 id="Master-和-Worker-Local模式时"><a href="#Master-和-Worker-Local模式时" class="headerlink" title="Master 和 Worker  (Local模式时)"></a>Master 和 Worker  (Local模式时)</h3><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker    </p><p>在Yarn模式时，Master 就是 RM ,Worker 就是 NM</p><h4 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h4><p>这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM  </p><h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>Worker也是进程，一个 Worker运行在集群中的一台服务器上，由 Master分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NM。    </p><h3 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h3><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。<br>说的简单点就是，ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是ApplicationMaster。    </p><h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="Executor-与-Core"><a href="#Executor-与-Core" class="headerlink" title="Executor 与 Core"></a>Executor 与 Core</h2><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。  </p><p>应用程序相关启动参数如下：<br>–num-executors 配置 Executor 的数量<br>–executor-memory 配置每个 Executor 的内存大小<br>–executor-cores 配置每个 Executor 的虚拟 CPU core 数量    </p><h2 id="并行度（Parallelism）"><a href="#并行度（Parallelism）" class="headerlink" title="并行度（Parallelism）"></a>并行度（Parallelism）</h2><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，是并行，而不是并发。  </p><p>将整个集群并行执行任务的数量称之为并行度。</p><p>一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。   </p><h2 id="有向无环图（DAG）"><a href="#有向无环图（DAG）" class="headerlink" title="有向无环图（DAG）"></a>有向无环图（DAG）</h2><p><img src="/2023/08/02/Spark-Core/2.png" alt="有向无环图">  </p><p>这里所谓的有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。    </p><p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。    </p><h2 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h2><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。  </p><p><img src="/2023/08/02/Spark-Core/3.png" alt="基于Yarn的Spark任务提交流程">   </p><p>Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client和 Cluster。两种模式主要区别在于：<strong>Driver 程序的运行节点位置</strong>。  </p><h3 id="Yarn-Client-模式"><a href="#Yarn-Client-模式" class="headerlink" title="Yarn Client 模式"></a>Yarn Client 模式</h3><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。  </p><pre><code>➢ Driver 在任务提交的本地机器上运行➢ Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster➢ ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存➢ ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程  ➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。  </code></pre><h3 id="Yarn-Cluster-模式"><a href="#Yarn-Cluster-模式" class="headerlink" title="Yarn Cluster 模式"></a>Yarn Cluster 模式</h3><p>Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。  </p><pre><code>➢ 在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster➢ 随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。➢ Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程  ➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行    </code></pre><h1 id="Spark-核心编程"><a href="#Spark-核心编程" class="headerlink" title="Spark 核心编程"></a>Spark 核心编程</h1><p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。<strong>三大数据结构</strong>分别是：  </p><pre><code>➢ RDD : 弹性分布式数据集➢ 累加器：分布式共享只写变量➢ 广播变量：分布式共享只读变量</code></pre><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合  </p><pre><code>➢ 弹性    存储的弹性：内存与磁盘的自动切换；    容错的弹性：数据丢失可以自动恢复；    计算的弹性：计算出错重试机制；    分片的弹性：可根据需要重新分片。➢ 分布式：数据存储在大数据集群不同节点上➢ 数据集：RDD 封装了计算逻辑，并不保存数据➢ 数据抽象：RDD 是一个抽象类，需要子类具体实现➢ 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑➢ 可分区、并行计算  </code></pre><h2 id="执行原理"><a href="#执行原理" class="headerlink" title="执行原理"></a>执行原理</h2><p>从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。  </p><p>执行时，需要将计算资源和计算模型进行协调和整合。  </p><p>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。  </p><p>在 Yarn 环境中，RDD的工作原理:  </p><p>1）启动Yarn集群环境  </p><p><img src="/2023/08/02/Spark-Core/5.png" alt="启动Yarn集群环境">   </p><p>2）Spark通过申请资源创建调度节点和计算节点   </p><p><img src="/2023/08/02/Spark-Core/6.png" alt="创建调度节点和计算节点">   </p><p>3）Spark框架根据需求将计算逻辑根据分区划分成不同的任务  </p><p><img src="/2023/08/02/Spark-Core/7.png" alt="根据分区划分成不同的任务">    </p><p>4）调度节点将任务根据计算节点状态发送到对应的计算节点进行计算  </p><p><img src="/2023/08/02/Spark-Core/8.png" alt="将任务分发给对应的计算节点进行计算"> </p><p>RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算  </p><h1 id="RDD算子总结"><a href="#RDD算子总结" class="headerlink" title="RDD算子总结"></a>RDD算子总结</h1><h2 id="Value类型总结"><a href="#Value类型总结" class="headerlink" title="Value类型总结"></a>Value类型总结</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><pre><code>｜ def map[U: ClassTag](f: T =&gt; U): RDD[U]｜ 将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。｜ val dataRDD1: RDD[Int] = dataRDD.map(｜  num =&gt;｜        &#123;   num * 2 &#125;｜ )｜ val dataRDD2: RDD[String] = dataRDD1.map(｜  num =&gt; &#123;｜  &quot;&quot; + num｜  &#125;｜ )｜ ​val mapRDD: RDD[Int] = rdd.map(_*2)对传入的数据，一个一个的进行转换，再返回给结果集</code></pre><h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><pre><code>｜ def mapPartitions[U: ClassTag](｜  f: Iterator[T] =&gt; Iterator[U],｜  preservesPartitioning: Boolean = false): RDD[U]｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。｜ val dataRDD1: RDD[Int] = dataRDD.mapPartitions(｜  datas =&gt; &#123;｜  datas.filter(_==2)｜  &#125;｜ )｜ ｜ val mpRDD: RDD[Int] = rdd.mapPartitions(    iter =&gt; &#123;println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;)        iter.map(_ * 2) &#125;)一个分区一个分区的数据进行转换，再返回给结果集</code></pre><h3 id="map-和-mapPartitions-的区别："><a href="#map-和-mapPartitions-的区别：" class="headerlink" title="map 和 mapPartitions 的区别："></a>map 和 mapPartitions 的区别：</h3><pre><code>数据处理角度：    Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作功能的角度：    Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。    MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据性能的角度：    Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高    但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作</code></pre><h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h3><pre><code>｜ def mapPartitionsWithIndex[U: ClassTag](｜  f: (Int, Iterator[T]) =&gt; Iterator[U],｜  preservesPartitioning: Boolean = false): RDD[U]｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引｜ val dataRDD1 = dataRDD.mapPartitionsWithIndex(｜  (index, datas) =&gt; &#123;｜  datas.map(index, _)｜  &#125;｜ )｜ ｜ mapPartitionsWithIndex在mapPartitions基础上加上了分区indexval mpiRDD = rdd.mapPartitionsWithIndex(  (index,iter) =&gt; &#123;// 1 ,     2 ,     3 ,    4    // (0,1)   (2,2)   (4,3)  (6,4)    iter.map(      num =&gt; &#123; (index,num) &#125;  ）&#125; )</code></pre><h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>｜ def flatMap[U: ClassTag](f: T &#x3D;&gt; TraversableOnce[U]): RDD[U]<br>｜ 将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射<br>｜ val dataRDD1 &#x3D; dataRDD.flatMap( list &#x3D;&gt; list)<br>｜<br>    val flatRDD:RDD[String] &#x3D; rdd.flatMap( s &#x3D;&gt; { s.split(“ “)  })<br>        Hello<br>        Scala<br>        Hello<br>        Spark</p><pre><code>如果使用rdd.map( s =&gt; &#123; s.split(&quot; &quot;)  &#125;)，会发现打印的结果是    [Ljava.lang.String;@f1a45f8    [Ljava.lang.String;@5edf2821所以切割等扁平映射操作，选用flatMap</code></pre><h3 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h3><p>｜ def glom(): RDD[Array[T]]<br>｜ 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变<br>｜ val dataRDD1:RDD[Array[Int]] &#x3D; dataRDD.glom()<br>    将同一个分区的数据直接转换为相同类型的内存数组进行处理<br>    List[Int] &#x3D;&gt; Array[Int]</p><h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><pre><code>｜ def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]｜ 将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。极限情况下，数据可能被分在同一个分区中｜ ​val groupRDD:RDD[(Int,Iterable[Int])] = rdd.groupBy(num % 2)    (0,CompactBuffer(2, 4))    (1,CompactBuffer(1, 3))    会输出一个RDD[(K, Iterable[T])]</code></pre><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><pre><code>｜ 将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。val dataRDD1 = dataRDD.filter(_%2 == 0)</code></pre><h3 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h3><p>｜ def sample(<br>｜ withReplacement: Boolean,<br>｜  fraction: Double,<br>｜  seed: Long &#x3D; Utils.random.nextLong): RDD[T]<br>｜ 根据指定的规则从数据集中抽取数据<br>｜<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4<br>    ｜ ),1)<br>｜ &#x2F;&#x2F; 抽取数据不放回（伯努利算法）<br>｜ &#x2F;&#x2F; 伯努利算法：又叫 0、1 分布。例如扔硬币，要么正面，要么反面。<br>｜ &#x2F;&#x2F; 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不<br>｜ 要<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD1 &#x3D; dataRDD.sample(false, 0.5)<br>｜ &#x2F;&#x2F; 抽取数据放回（泊松算法）<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，true：放回；false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：重复数据的几率，范围大于等于 0.表示每一个元素被期望抽取到的次数<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD2 &#x3D; dataRDD.sample(true, 2)<br>｜ ​<br>｜<br>    相同的seed种子，多次运行依旧是相同的抽样结果,修改withReplacement也不会发生变化，&#x2F;&#x2F; 修改fraction后结果会发生改变<br>    rdd.sample(true,0.5,101)</p><h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>｜ def distinct()(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>｜ def distinct(numPartitions: Int)(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>    ｜ 将数据集中重复的数据去重<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4,1,2<br>    ｜ ),1)<br>    ｜ val dataRDD1 &#x3D; dataRDD.distinct()<br>    ｜ val dataRDD2 &#x3D; dataRDD.distinct(2)<br>｜ ​<br>｜ </p><h3 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h3><p>｜ def coalesce(numPartitions: Int, shuffle: Boolean &#x3D; false,<br>｜  partitionCoalescer: Option[PartitionCoalescer] &#x3D; Option.empty)<br>｜  (implicit ord: Ordering[T] &#x3D; null)<br>｜  : RDD[T]<br>｜ 根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率<br>｜ 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少<br>｜ 分区的个数，减小任务调度成本<br>｜ ​<br>｜<br>    &#x2F;&#x2F; coalesce 方法默认情况下不会将分区的数据打乱重新组合<br>    &#x2F;&#x2F; 这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜<br>    &#x2F;&#x2F; 如果想要让数据倾斜，可以进行shuffle处理<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2,shuffle &#x3D; false)<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2) 默认不进行shuffle<br>    &#x2F;&#x2F; 进行shuffle处理后会出现数据倾斜val newRDD &#x3D; rdd.coalesce(2, true)</p><h3 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h3><pre><code>｜ def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]repartition 操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true</code></pre><h3 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h3><p>｜ def sortBy[K](<br>｜  f: (T) &#x3D;&gt; K,<br>｜ ascending: Boolean &#x3D; true,<br>｜  numPartitions: Int &#x3D; this.partitions.length)<br>｜  (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]<br>｜ 该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，之后按照 f 函数处理<br>｜ 的结果进行排序，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一<br>｜ 致。中间存在 shuffle 的过程<br>｜ ​<br>｜ ​<br>    默认升序排序<br>    val dataRDD1 &#x3D; dataRDD.sortBy(num&#x3D;&gt;num, false, 4)<br>    val newRDD &#x3D; rdd.sortBy(t &#x3D;&gt; t._1.toInt, true)</p><h2 id="双-Value-类型总结"><a href="#双-Value-类型总结" class="headerlink" title="双 Value 类型总结"></a>双 Value 类型总结</h2><h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><pre><code>def intersection(other: RDD[T]): RDD[T]val dataRDD = dataRDD1.intersection(dataRDD2)求交集</code></pre><h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><pre><code>def union(other: RDD[T]): RDD[T]val dataRDD = dataRDD1.union(dataRDD2)求并集</code></pre><h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h3><pre><code>def subtract(other: RDD[T]): RDD[T]val dataRDD = dataRDD1.subtract(dataRDD2)求差集</code></pre><h3 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h3><pre><code>def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]val dataRDD = dataRDD1.zip(dataRDD2)将两个 RDD 中的元素，以键值对的形式进行合并。其中，键值对中的 Key 为第 1 个 RDD中的元素，Value 为第 2 个 RDD 中的相同位置的元素</code></pre><h2 id="Key-Value类型总结"><a href="#Key-Value类型总结" class="headerlink" title="Key-Value类型总结"></a>Key-Value类型总结</h2><h3 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h3><pre><code>def partitionBy(partitioner: Partitioner): RDD[(K, V)]将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitionerval rdd2: RDD[(Int, String)] = rdd.partitionBy(new HashPartitioner(2))</code></pre><h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3><pre><code>def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]可以将数据按照相同的 Key 对 Value 进行聚合val dataRDD2 = dataRDD1.reduceByKey(_+_)</code></pre><h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><pre><code>def groupByKey(): RDD[(K, Iterable[V])]def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]将数据源的数据根据 key 对 value 进行分组val dataRDD2 = dataRDD1.groupByKey()val dataRDD3 = dataRDD1.groupByKey(2)val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2))</code></pre><h3 id="reduceByKey-和-groupByKey-的区别："><a href="#reduceByKey-和-groupByKey-的区别：" class="headerlink" title="reduceByKey 和 groupByKey 的区别："></a>reduceByKey 和 groupByKey 的区别：</h3><pre><code>从 shuffle 的角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。从功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey</code></pre><h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h3><pre><code>def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): RDD[(K, U)]将数据根据不同的规则进行分区内计算和分区间计算dataRDD1.aggregateByKey(0)(_+_,_+_)｜ // 1. 第一个参数列表中的参数表示初始值｜ // 2. 第二个参数列表中含有两个参数｜ // 2.1 第一个参数表示分区内的计算规则｜ // 2.2 第二个参数表示分区间的计算规则｜ </code></pre><h3 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h3><pre><code>def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey</code></pre><h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h3><pre><code>def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]最通用的对 key-value 型 rdd 进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致val combineRdd: RDD[(String, (Int, Int))] = input.combineByKey( (_, 1), (acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1), (acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2))</code></pre><h3 id="reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别："><a href="#reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别：" class="headerlink" title="reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别："></a>reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别：</h3><pre><code>｜ reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同｜ FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同｜ AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同｜ CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同｜ </code></pre><h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3><pre><code>def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length): RDD[(K, V)]val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(true)val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(false)</code></pre><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><pre><code>def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的(K,(V,W))的 RDDrdd.join(rdd1).collect().foreach(println)</code></pre><h3 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h3><pre><code>def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]类似于 SQL 语句的左外连接val rdd: RDD[(String, (Int, Option[Int]))] = dataRDD1.leftOuterJoin(dataRDD2)</code></pre><h3 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h3><pre><code>类似于 SQL 语句的右外连接</code></pre><h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><pre><code>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))类型的 RDDval value: RDD[(String, (Iterable[Int], Iterable[Int]))] = dataRDD1.cogroup(dataRDD2)(a,(CompactBuffer(1),CompactBuffer(4)))(b,(CompactBuffer(2),CompactBuffer(5)))(c,(CompactBuffer(3),CompactBuffer(6, 7)))</code></pre><h2 id="Spark行动算子"><a href="#Spark行动算子" class="headerlink" title="Spark行动算子"></a>Spark行动算子</h2><h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>在驱动程序中，以数组 Array 的形式返回数据集的所有元素  </p><pre><code>// 收集数据到 Driverrdd.collect().foreach(println)  </code></pre><h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>返回 RDD 中元素的个数  </p><pre><code>// 返回 RDD 中元素的个数val countResult: Long = rdd.count()  </code></pre><h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>返回 RDD 中的第一个元素  </p><pre><code>// 返回 RDD 中元素的第1个元素val firstResult: Int = rdd.first()  </code></pre><h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>返回一个由 RDD 的前 n 个元素组成的数组  </p><pre><code>// 返回 RDD 中元素的个数val takeResult: Array[Int] = rdd.take(2)println(takeResult.mkString(&quot;,&quot;))  </code></pre><h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>返回该 RDD 排序后的前 n 个元素组成的数组  </p><pre><code>// 返回 RDD 中元素的个数val result: Array[Int] = rdd.takeOrdered(2)  </code></pre><h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合  </p><pre><code>val result: Int = rdd.aggregate(10)(_ + _, _ + _)  </code></pre><h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p>折叠操作，aggregate 的简化版操作  </p><pre><code>val foldResult: Int = rdd.fold(0)(_+_)  </code></pre><h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>统计每种 key 的个数  </p><pre><code>// 统计每种 key 的个数val result: collection.Map[Int, Long] = rdd.countByKey()  </code></pre><h3 id="save-相关算子"><a href="#save-相关算子" class="headerlink" title="save 相关算子"></a>save 相关算子</h3><p>将数据保存到不同格式的文件中  </p><pre><code>// 保存成 Text 文件rdd.saveAsTextFile(&quot;output&quot;)  </code></pre><h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><pre><code>// 收集后打印rdd.map(num=&gt;num).collect().foreach(println)  </code></pre><h2 id="RDD-序列化"><a href="#RDD-序列化" class="headerlink" title="RDD 序列化"></a>RDD 序列化</h2><h3 id="闭包检查"><a href="#闭包检查" class="headerlink" title="闭包检查"></a>闭包检查</h3><p>从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。</p><h1 id="RDD-依赖关系"><a href="#RDD-依赖关系" class="headerlink" title="RDD 依赖关系"></a>RDD 依赖关系</h1><h2 id="1）RDD血缘关系"><a href="#1）RDD血缘关系" class="headerlink" title="1）RDD血缘关系"></a>1）RDD血缘关系</h2><p>RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage（血统）记录下来，以便恢复丢失的分区。    </p><p>RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。   </p><pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)println(fileRDD.toDebugString)</code></pre><h2 id="2）RDD依赖关系"><a href="#2）RDD依赖关系" class="headerlink" title="2）RDD依赖关系"></a>2）RDD依赖关系</h2><p>所谓的依赖关系，其实就是两个相邻 RDD 之间的关系。  </p><pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)println(fileRDD.dependencies)   </code></pre><h2 id="3）RDD-窄依赖（没有Shuffle）"><a href="#3）RDD-窄依赖（没有Shuffle）" class="headerlink" title="3）RDD 窄依赖（没有Shuffle）"></a>3）RDD 窄依赖（没有Shuffle）</h2><p>窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，窄依赖我们形象的比喻为独生子女。</p><h2 id="4）RDD宽依赖-（有Shuffle）"><a href="#4）RDD宽依赖-（有Shuffle）" class="headerlink" title="4）RDD宽依赖 （有Shuffle）"></a>4）RDD宽依赖 （有Shuffle）</h2><p>宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。    </p><h2 id="RDD-任务划分"><a href="#RDD-任务划分" class="headerlink" title="RDD 任务划分"></a>RDD 任务划分</h2><p>RDD 任务切分中间分为：Application、Job、Stage 和 Task  </p><pre><code>Application：初始化一个 SparkContext 即生成一个 Application；Job：一个 Action 算子就会生成一个 Job；Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数。  </code></pre><p>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系</p><p><img src="/2023/08/02/Spark-Core/9.png" alt="Spark任务划分流程">     </p><h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><h3 id="1-RDD-Cache-缓存"><a href="#1-RDD-Cache-缓存" class="headerlink" title="1) RDD Cache 缓存"></a>1) RDD Cache 缓存</h3><p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用。  </p><pre><code>// cache 操作会增加血缘关系，不改变原有的血缘关系  // 可以更改存储级别//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)  </code></pre><h4 id="RDD持久化可选存储级别"><a href="#RDD持久化可选存储级别" class="headerlink" title="RDD持久化可选存储级别"></a>RDD持久化可选存储级别</h4><pre><code>object StorageLevel &#123; val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(true, true, true, false, 1)  </code></pre><p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD 的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。  </p><p>Spark 会自动对一些 Shuffle 操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点 Shuffle 失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用 persist 或 cache。  </p><h3 id="2）RDD-CheckPoint-检查点"><a href="#2）RDD-CheckPoint-检查点" class="headerlink" title="2）RDD CheckPoint 检查点"></a>2）RDD CheckPoint 检查点</h3><p>所谓的检查点其实就是通过将 RDD 中间结果写入磁盘  </p><p>对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发。  </p><ol start="3"><li><p>缓存和检查点区别  </p><p> 1）Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖。<br> 2）Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高。<br> 3）建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD</p></li></ol><h2 id="RDD-分区器"><a href="#RDD-分区器" class="headerlink" title="RDD 分区器"></a>RDD 分区器</h2><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认分区。   </p><p>分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。   </p><pre><code>➢ 只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None  ➢ 每个 RDD 的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。  </code></pre><ol><li><p>Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余  </p></li><li><p>Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p></li></ol><h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge。  </p><h3 id="系统累加器"><a href="#系统累加器" class="headerlink" title="系统累加器"></a>系统累加器</h3><pre><code>val rdd = sc.makeRDD(List(1,2,3,4,5))// 声明累加器var sum = sc.longAccumulator(&quot;sum&quot;);rdd.foreach( num =&gt; &#123; // 使用累加器 sum.add(num) &#125;)// 获取累加器的值println(&quot;sum = &quot; + sum.value)</code></pre><h3 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h3><pre><code>// 自定义累加器// 1. 继承 AccumulatorV2，并设定泛型// 2. 重写累加器的抽象方法class WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, Long]]&#123;var map : mutable.Map[String, Long] = mutable.Map()// 累加器是否为初始状态override def isZero: Boolean = &#123; map.isEmpty&#125;// 复制累加器override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = &#123; new WordCountAccumulator&#125;// 重置累加器override def reset(): Unit = &#123; map.clear()&#125;// 向累加器中增加数据 (In)override def add(word: String): Unit = &#123; // 查询 map 中是否存在相同的单词 // 如果有相同的单词，那么单词的数量加 1 // 如果没有相同的单词，那么在 map 中增加这个单词 map(word) = map.getOrElse(word, 0L) + 1L&#125;// 合并累加器override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): Unit = &#123; val map1 = map val map2 = other.value // 两个 Map 的合并 map = map1.foldLeft(map2)( ( innerMap, kv ) =&gt; &#123; innerMap(kv._1) = innerMap.getOrElse(kv._1, 0L) + kv._2 innerMap &#125; )&#125;// 返回累加器的结果 （Out）override def value: mutable.Map[String, Long] = map&#125;</code></pre><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。  </p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E7%BA%B5%E6%B5%AA%E5%A4%A7%E5%8C%96%E4%B8%AD%EF%BC%8C%E4%B8%8D%E5%96%9C%E4%BA%A6%E4%B8%8D%E6%83%A7/">纵浪大化中，不喜亦不惧</category>
      
      
      <comments>http://example.com/2023/08/02/Spark-Core/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
