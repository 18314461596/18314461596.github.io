<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="纵浪大化中，不喜亦不惧 ~相关学习文档链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1hsV8GWRzW4Yx9VvdofjP_g提取码：mg5w    Spark概述Spark 是什么？Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎      Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎   Spark Core 中提供了 Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark学习笔记">
<meta property="og:url" content="http://example.com/2023/08/02/Spark-Core/index.html">
<meta property="og:site_name" content="第五门徒">
<meta property="og:description" content="纵浪大化中，不喜亦不惧 ~相关学习文档链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1hsV8GWRzW4Yx9VvdofjP_g提取码：mg5w    Spark概述Spark 是什么？Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎      Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎   Spark Core 中提供了 Spark">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/08/02/Spark-Core/1.png">
<meta property="og:image" content="http://example.com/2023/08/02/Spark-Core/2.png">
<meta property="og:image" content="http://example.com/2023/08/02/Spark-Core/3.png">
<meta property="og:image" content="http://example.com/2023/08/02/Spark-Core/5.png">
<meta property="og:image" content="http://example.com/2023/08/02/Spark-Core/6.png">
<meta property="og:image" content="http://example.com/2023/08/02/Spark-Core/7.png">
<meta property="og:image" content="http://example.com/2023/08/02/Spark-Core/8.png">
<meta property="og:image" content="http://example.com/2023/08/02/Spark-Core/9.png">
<meta property="article:published_time" content="2023-08-02T02:51:06.000Z">
<meta property="article:modified_time" content="2023-08-11T04:49:16.939Z">
<meta property="article:author" content="张宴银">
<meta property="article:tag" content="纵浪大化中，不喜亦不惧">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/08/02/Spark-Core/1.png">

<link rel="canonical" href="http://example.com/2023/08/02/Spark-Core/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Spark学习笔记 | 第五门徒</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="第五门徒" type="application/rss+xml">
</head>




<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">第五门徒</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="heartbeat fa-fw"></i>公益 404</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/02/Spark-Core/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张宴银">
      <meta itemprop="description" content="初级以内我无敌，中级以上我一换一">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="第五门徒">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark学习笔记
        </h1>

        <div class="post-meta">
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-02 10:51:06" itemprop="dateCreated datePublished" datetime="2023-08-02T10:51:06+08:00">2023-08-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-11 12:49:16" itemprop="dateModified" datetime="2023-08-11T12:49:16+08:00">2023-08-11</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="纵浪大化中，不喜亦不惧"><a href="#纵浪大化中，不喜亦不惧" class="headerlink" title="纵浪大化中，不喜亦不惧 ~"></a>纵浪大化中，不喜亦不惧 ~</h1><p>相关学习文档<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g">https://pan.baidu.com/s/1hsV8GWRzW4Yx9VvdofjP_g</a><br>提取码：mg5w   </p>
<h1 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h1><h2 id="Spark-是什么？"><a href="#Spark-是什么？" class="headerlink" title="Spark 是什么？"></a>Spark 是什么？</h2><p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎     </p>
<p>Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎  </p>
<p>Spark Core 中提供了 Spark 最基础与最核心的功能  </p>
<p>Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据   </p>
<p>Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API  </p>
<p>Spark 一直被认为是 Hadoop 框架的升级版。  </p>
<h2 id="Mapreduce是什么？"><a href="#Mapreduce是什么？" class="headerlink" title="Mapreduce是什么？"></a>Mapreduce是什么？</h2><p>MapReduce 是一种编程模型，作为 Hadoop 的分布式计算模型，是 Hadoop 的核心    </p>
<h2 id="HBase是什么？"><a href="#HBase是什么？" class="headerlink" title="HBase是什么？"></a>HBase是什么？</h2><p>HBase 是一个基于 HDFS 的分布式数据库，擅长实时地随机读&#x2F;写超大规模数据集  </p>
<h2 id="Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）"><a href="#Spark和Hadoop的差别是什么？-（Hadoop默认计算引擎为Mapreduce）" class="headerlink" title="Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）"></a>Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）</h2><p>Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘  </p>
<p>Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互  </p>
<p>Spark 的缓存机制比 HDFS 的缓存机制高效  </p>
<h2 id="什么时候选用Spark什么时候选用Mapreduce？"><a href="#什么时候选用Spark什么时候选用Mapreduce？" class="headerlink" title="什么时候选用Spark什么时候选用Mapreduce？"></a>什么时候选用Spark什么时候选用Mapreduce？</h2><p>Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark并不能完全替代 MR。  </p>
<h2 id="提交Spark应用的代码示例"><a href="#提交Spark应用的代码示例" class="headerlink" title="提交Spark应用的代码示例"></a>提交Spark应用的代码示例</h2><h3 id="local模式"><a href="#local模式" class="headerlink" title="local模式"></a>local模式</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ./examples/jars/spark-examples_2.12-3.0.0.jar 10

1) --class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序  
2) --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟 CPU 核数量  
3) spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包  
4) 数字 10 表示程序的入口参数，用于设定当前应用的任务数量  
</code></pre>
<h3 id="Yarn模式-（生产：Cluster模式）"><a href="#Yarn模式-（生产：Cluster模式）" class="headerlink" title="Yarn模式  （生产：Cluster模式）"></a>Yarn模式  （生产：Cluster模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2.12-3.0.0.jar 10  
</code></pre>
<h3 id="Yarn模式-（测试：Client模式）"><a href="#Yarn模式-（测试：Client模式）" class="headerlink" title="Yarn模式  （测试：Client模式）"></a>Yarn模式  （测试：Client模式）</h3><pre><code>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.12-3.0.0.jar 10  
</code></pre>
<h4 id="Spark的端口号"><a href="#Spark的端口号" class="headerlink" title="Spark的端口号"></a>Spark的端口号</h4><pre><code>➢ Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）  
➢ Spark Master 内部通信服务端口号：7077  
➢ Standalone 模式下，Spark Master Web 端口号：8080（资源）  
➢ Spark 历史服务器端口号：18080  
➢ Hadoop YARN 任务运行情况查看端口号：8088     
</code></pre>
<h2 id="Spark运行架构"><a href="#Spark运行架构" class="headerlink" title="Spark运行架构"></a>Spark运行架构</h2><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构  </p>
<p><img src="/2023/08/02/Spark-Core/1.png" alt="Spark运行架构">  </p>
<p>Driver表示Master，负责管理整个集群中的作业调度  </p>
<p>Executor表示slave，负责实际执行任务  </p>
<h3 id="Spark核心组件"><a href="#Spark核心组件" class="headerlink" title="Spark核心组件"></a>Spark核心组件</h3><p>Driver和Executor &amp; Master 和 Worker </p>
<h4 id="Driver的作用"><a href="#Driver的作用" class="headerlink" title="Driver的作用"></a>Driver的作用</h4><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。  </p>
<p>Driver 在 Spark 作业执行时主要负责：  </p>
<pre><code>➢ 将用户程序转化为作业（job）  
➢ 在 Executor 之间调度任务(task)  
➢ 跟踪 Executor 的执行情况  
➢ 通过 UI 展示查询运行情况      
</code></pre>
<p>所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。  </p>
<h4 id="Executor的作用"><a href="#Executor的作用" class="headerlink" title="Executor的作用"></a>Executor的作用</h4><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立  </p>
<p>Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。  </p>
<h5 id="Executor-有两个核心功能："><a href="#Executor-有两个核心功能：" class="headerlink" title="Executor 有两个核心功能："></a>Executor 有两个核心功能：</h5><pre><code>➢ 负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程
➢ 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。
  
</code></pre>
<h3 id="Master-和-Worker-Local模式时"><a href="#Master-和-Worker-Local模式时" class="headerlink" title="Master 和 Worker  (Local模式时)"></a>Master 和 Worker  (Local模式时)</h3><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker    </p>
<p>在Yarn模式时，Master 就是 RM ,Worker 就是 NM</p>
<h4 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h4><p>这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM  </p>
<h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>Worker也是进程，一个 Worker运行在集群中的一台服务器上，由 Master分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NM。    </p>
<h3 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h3><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。<br>说的简单点就是，ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是ApplicationMaster。    </p>
<h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="Executor-与-Core"><a href="#Executor-与-Core" class="headerlink" title="Executor 与 Core"></a>Executor 与 Core</h2><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。  </p>
<p>应用程序相关启动参数如下：<br>–num-executors 配置 Executor 的数量<br>–executor-memory 配置每个 Executor 的内存大小<br>–executor-cores 配置每个 Executor 的虚拟 CPU core 数量    </p>
<h2 id="并行度（Parallelism）"><a href="#并行度（Parallelism）" class="headerlink" title="并行度（Parallelism）"></a>并行度（Parallelism）</h2><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，是并行，而不是并发。  </p>
<p>将整个集群并行执行任务的数量称之为并行度。</p>
<p>一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。   </p>
<h2 id="有向无环图（DAG）"><a href="#有向无环图（DAG）" class="headerlink" title="有向无环图（DAG）"></a>有向无环图（DAG）</h2><p><img src="/2023/08/02/Spark-Core/2.png" alt="有向无环图">  </p>
<p>这里所谓的有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。    </p>
<p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。    </p>
<h2 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h2><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。  </p>
<p><img src="/2023/08/02/Spark-Core/3.png" alt="基于Yarn的Spark任务提交流程">   </p>
<p>Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client和 Cluster。两种模式主要区别在于：<strong>Driver 程序的运行节点位置</strong>。  </p>
<h3 id="Yarn-Client-模式"><a href="#Yarn-Client-模式" class="headerlink" title="Yarn Client 模式"></a>Yarn Client 模式</h3><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。  </p>
<pre><code>➢ Driver 在任务提交的本地机器上运行
➢ Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster
➢ ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存
➢ ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程  
➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数
➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。  
</code></pre>
<h3 id="Yarn-Cluster-模式"><a href="#Yarn-Cluster-模式" class="headerlink" title="Yarn Cluster 模式"></a>Yarn Cluster 模式</h3><p>Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。  </p>
<pre><code>➢ 在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster
➢ 随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。
➢ Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程  
➢ Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数
➢ 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行    
</code></pre>
<h1 id="Spark-核心编程"><a href="#Spark-核心编程" class="headerlink" title="Spark 核心编程"></a>Spark 核心编程</h1><p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。<strong>三大数据结构</strong>分别是：  </p>
<pre><code>➢ RDD : 弹性分布式数据集
➢ 累加器：分布式共享只写变量
➢ 广播变量：分布式共享只读变量
</code></pre>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合  </p>
<pre><code>➢ 弹性
    存储的弹性：内存与磁盘的自动切换；
    容错的弹性：数据丢失可以自动恢复；
    计算的弹性：计算出错重试机制；
    分片的弹性：可根据需要重新分片。
➢ 分布式：数据存储在大数据集群不同节点上
➢ 数据集：RDD 封装了计算逻辑，并不保存数据
➢ 数据抽象：RDD 是一个抽象类，需要子类具体实现
➢ 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑
➢ 可分区、并行计算  
</code></pre>
<h2 id="执行原理"><a href="#执行原理" class="headerlink" title="执行原理"></a>执行原理</h2><p>从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。  </p>
<p>执行时，需要将计算资源和计算模型进行协调和整合。  </p>
<p>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。  </p>
<p>在 Yarn 环境中，RDD的工作原理:  </p>
<p>1）启动Yarn集群环境  </p>
<p><img src="/2023/08/02/Spark-Core/5.png" alt="启动Yarn集群环境">   </p>
<p>2）Spark通过申请资源创建调度节点和计算节点   </p>
<p><img src="/2023/08/02/Spark-Core/6.png" alt="创建调度节点和计算节点">   </p>
<p>3）Spark框架根据需求将计算逻辑根据分区划分成不同的任务  </p>
<p><img src="/2023/08/02/Spark-Core/7.png" alt="根据分区划分成不同的任务">    </p>
<p>4）调度节点将任务根据计算节点状态发送到对应的计算节点进行计算  </p>
<p><img src="/2023/08/02/Spark-Core/8.png" alt="将任务分发给对应的计算节点进行计算"> </p>
<p>RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算  </p>
<h1 id="RDD算子总结"><a href="#RDD算子总结" class="headerlink" title="RDD算子总结"></a>RDD算子总结</h1><h2 id="Value类型总结"><a href="#Value类型总结" class="headerlink" title="Value类型总结"></a>Value类型总结</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><pre><code>｜ def map[U: ClassTag](f: T =&gt; U): RDD[U]
｜ 将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。
｜ val dataRDD1: RDD[Int] = dataRDD.map(
｜  num =&gt;
｜        &#123;   num * 2 &#125;
｜ )
｜ val dataRDD2: RDD[String] = dataRDD1.map(
｜  num =&gt; &#123;
｜  &quot;&quot; + num
｜  &#125;
｜ )
｜ ​
val mapRDD: RDD[Int] = rdd.map(_*2)
对传入的数据，一个一个的进行转换，再返回给结果集
</code></pre>
<h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><pre><code>｜ def mapPartitions[U: ClassTag](
｜  f: Iterator[T] =&gt; Iterator[U],
｜  preservesPartitioning: Boolean = false): RDD[U]
｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。
｜ val dataRDD1: RDD[Int] = dataRDD.mapPartitions(
｜  datas =&gt; &#123;
｜  datas.filter(_==2)
｜  &#125;
｜ )
｜ 
｜ 
val mpRDD: RDD[Int] = rdd.mapPartitions(
    iter =&gt; &#123;
println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;)
        iter.map(_ * 2) &#125;)
一个分区一个分区的数据进行转换，再返回给结果集
</code></pre>
<h3 id="map-和-mapPartitions-的区别："><a href="#map-和-mapPartitions-的区别：" class="headerlink" title="map 和 mapPartitions 的区别："></a>map 和 mapPartitions 的区别：</h3><pre><code>数据处理角度：
    Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作

功能的角度：
    Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。
    MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据

性能的角度：
    Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高
    但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作
</code></pre>
<h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h3><pre><code>｜ def mapPartitionsWithIndex[U: ClassTag](
｜  f: (Int, Iterator[T]) =&gt; Iterator[U],
｜  preservesPartitioning: Boolean = false): RDD[U]
｜ 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引
｜ val dataRDD1 = dataRDD.mapPartitionsWithIndex(
｜  (index, datas) =&gt; &#123;
｜  datas.map(index, _)
｜  &#125;
｜ )
｜ 
｜ 
mapPartitionsWithIndex在mapPartitions基础上加上了分区index
val mpiRDD = rdd.mapPartitionsWithIndex(
  (index,iter) =&gt; &#123;
// 1 ,     2 ,     3 ,    4    // (0,1)   (2,2)   (4,3)  (6,4)    iter.map(
      num =&gt; &#123; (index,num) &#125;  ）&#125; )
</code></pre>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>｜ def flatMap[U: ClassTag](f: T &#x3D;&gt; TraversableOnce[U]): RDD[U]<br>｜ 将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射<br>｜ val dataRDD1 &#x3D; dataRDD.flatMap( list &#x3D;&gt; list)<br>｜<br>    val flatRDD:RDD[String] &#x3D; rdd.flatMap( s &#x3D;&gt; { s.split(“ “)  })<br>        Hello<br>        Scala<br>        Hello<br>        Spark</p>
<pre><code>如果使用rdd.map( s =&gt; &#123; s.split(&quot; &quot;)  &#125;)，会发现打印的结果是
    [Ljava.lang.String;@f1a45f8
    [Ljava.lang.String;@5edf2821

所以切割等扁平映射操作，选用flatMap
</code></pre>
<h3 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h3><p>｜ def glom(): RDD[Array[T]]<br>｜ 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变<br>｜ val dataRDD1:RDD[Array[Int]] &#x3D; dataRDD.glom()<br>    将同一个分区的数据直接转换为相同类型的内存数组进行处理<br>    List[Int] &#x3D;&gt; Array[Int]</p>
<h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><pre><code>｜ def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]
｜ 将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。极限情况下，数据可能被分在同一个分区中
｜ ​
val groupRDD:RDD[(Int,Iterable[Int])] = rdd.groupBy(num % 2)
    (0,CompactBuffer(2, 4))
    (1,CompactBuffer(1, 3))
    会输出一个RDD[(K, Iterable[T])]
</code></pre>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><pre><code>｜ 将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。
val dataRDD1 = dataRDD.filter(_%2 == 0)
</code></pre>
<h3 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h3><p>｜ def sample(<br>｜ withReplacement: Boolean,<br>｜  fraction: Double,<br>｜  seed: Long &#x3D; Utils.random.nextLong): RDD[T]<br>｜ 根据指定的规则从数据集中抽取数据<br>｜<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4<br>    ｜ ),1)<br>｜ &#x2F;&#x2F; 抽取数据不放回（伯努利算法）<br>｜ &#x2F;&#x2F; 伯努利算法：又叫 0、1 分布。例如扔硬币，要么正面，要么反面。<br>｜ &#x2F;&#x2F; 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不<br>｜ 要<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD1 &#x3D; dataRDD.sample(false, 0.5)<br>｜ &#x2F;&#x2F; 抽取数据放回（泊松算法）<br>｜ &#x2F;&#x2F; 第一个参数：抽取的数据是否放回，true：放回；false：不放回<br>｜ &#x2F;&#x2F; 第二个参数：重复数据的几率，范围大于等于 0.表示每一个元素被期望抽取到的次数<br>｜ &#x2F;&#x2F; 第三个参数：随机数种子<br>    ｜ val dataRDD2 &#x3D; dataRDD.sample(true, 2)<br>｜ ​<br>｜<br>    相同的seed种子，多次运行依旧是相同的抽样结果,修改withReplacement也不会发生变化，&#x2F;&#x2F; 修改fraction后结果会发生改变<br>    rdd.sample(true,0.5,101)</p>
<h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>｜ def distinct()(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>｜ def distinct(numPartitions: Int)(implicit ord: Ordering[T] &#x3D; null): RDD[T]<br>    ｜ 将数据集中重复的数据去重<br>    ｜ val dataRDD &#x3D; sparkContext.makeRDD(List(<br>    ｜  1,2,3,4,1,2<br>    ｜ ),1)<br>    ｜ val dataRDD1 &#x3D; dataRDD.distinct()<br>    ｜ val dataRDD2 &#x3D; dataRDD.distinct(2)<br>｜ ​<br>｜ </p>
<h3 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h3><p>｜ def coalesce(numPartitions: Int, shuffle: Boolean &#x3D; false,<br>｜  partitionCoalescer: Option[PartitionCoalescer] &#x3D; Option.empty)<br>｜  (implicit ord: Ordering[T] &#x3D; null)<br>｜  : RDD[T]<br>｜ 根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率<br>｜ 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少<br>｜ 分区的个数，减小任务调度成本<br>｜ ​<br>｜<br>    &#x2F;&#x2F; coalesce 方法默认情况下不会将分区的数据打乱重新组合<br>    &#x2F;&#x2F; 这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜<br>    &#x2F;&#x2F; 如果想要让数据倾斜，可以进行shuffle处理<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2,shuffle &#x3D; false)<br>    &#x2F;&#x2F;val newRDD &#x3D; rdd.coalesce(2) 默认不进行shuffle<br>    &#x2F;&#x2F; 进行shuffle处理后会出现数据倾斜val newRDD &#x3D; rdd.coalesce(2, true)</p>
<h3 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h3><pre><code>｜ def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
repartition 操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true
</code></pre>
<h3 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h3><p>｜ def sortBy[K](<br>｜  f: (T) &#x3D;&gt; K,<br>｜ ascending: Boolean &#x3D; true,<br>｜  numPartitions: Int &#x3D; this.partitions.length)<br>｜  (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]<br>｜ 该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，之后按照 f 函数处理<br>｜ 的结果进行排序，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一<br>｜ 致。中间存在 shuffle 的过程<br>｜ ​<br>｜ ​<br>    默认升序排序<br>    val dataRDD1 &#x3D; dataRDD.sortBy(num&#x3D;&gt;num, false, 4)<br>    val newRDD &#x3D; rdd.sortBy(t &#x3D;&gt; t._1.toInt, true)</p>
<h2 id="双-Value-类型总结"><a href="#双-Value-类型总结" class="headerlink" title="双 Value 类型总结"></a>双 Value 类型总结</h2><h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><pre><code>def intersection(other: RDD[T]): RDD[T]
val dataRDD = dataRDD1.intersection(dataRDD2)
求交集
</code></pre>
<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><pre><code>def union(other: RDD[T]): RDD[T]
val dataRDD = dataRDD1.union(dataRDD2)
求并集
</code></pre>
<h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h3><pre><code>def subtract(other: RDD[T]): RDD[T]
val dataRDD = dataRDD1.subtract(dataRDD2)
求差集
</code></pre>
<h3 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h3><pre><code>def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]
val dataRDD = dataRDD1.zip(dataRDD2)
将两个 RDD 中的元素，以键值对的形式进行合并。其中，键值对中的 Key 为第 1 个 RDD中的元素，Value 为第 2 个 RDD 中的相同位置的元素
</code></pre>
<h2 id="Key-Value类型总结"><a href="#Key-Value类型总结" class="headerlink" title="Key-Value类型总结"></a>Key-Value类型总结</h2><h3 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h3><pre><code>def partitionBy(partitioner: Partitioner): RDD[(K, V)]
将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner
val rdd2: RDD[(Int, String)] = rdd.partitionBy(new HashPartitioner(2))
</code></pre>
<h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3><pre><code>def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]
def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]
可以将数据按照相同的 Key 对 Value 进行聚合
val dataRDD2 = dataRDD1.reduceByKey(_+_)
</code></pre>
<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><pre><code>def groupByKey(): RDD[(K, Iterable[V])]
def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
将数据源的数据根据 key 对 value 进行分组
val dataRDD2 = dataRDD1.groupByKey()
val dataRDD3 = dataRDD1.groupByKey(2)
val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2))
</code></pre>
<h3 id="reduceByKey-和-groupByKey-的区别："><a href="#reduceByKey-和-groupByKey-的区别：" class="headerlink" title="reduceByKey 和 groupByKey 的区别："></a>reduceByKey 和 groupByKey 的区别：</h3><pre><code>从 shuffle 的角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的
数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。
从功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey
</code></pre>
<h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h3><pre><code>def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): RDD[(K, U)]
将数据根据不同的规则进行分区内计算和分区间计算
dataRDD1.aggregateByKey(0)(_+_,_+_)
｜ // 1. 第一个参数列表中的参数表示初始值
｜ // 2. 第二个参数列表中含有两个参数
｜ // 2.1 第一个参数表示分区内的计算规则
｜ // 2.2 第二个参数表示分区间的计算规则
｜ 
</code></pre>
<h3 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h3><pre><code>def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]
当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey
</code></pre>
<h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h3><pre><code>def combineByKey[C](
createCombiner: V =&gt; C,
 mergeValue: (C, V) =&gt; C,
 mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]
最通用的对 key-value 型 rdd 进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致
val combineRdd: RDD[(String, (Int, Int))] = input.combineByKey(
 (_, 1),
 (acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1),
 (acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2))
</code></pre>
<h3 id="reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别："><a href="#reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别：" class="headerlink" title="reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别："></a>reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别：</h3><pre><code>｜ reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同
｜ FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同
｜ AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同
｜ CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同
｜ 
</code></pre>
<h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3><pre><code>def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length): RDD[(K, V)]
val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(true)
val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(false)
</code></pre>
<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><pre><code>def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的(K,(V,W))的 RDD
rdd.join(rdd1).collect().foreach(println)
</code></pre>
<h3 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h3><pre><code>def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]
类似于 SQL 语句的左外连接
val rdd: RDD[(String, (Int, Option[Int]))] = dataRDD1.leftOuterJoin(dataRDD2)
</code></pre>
<h3 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h3><pre><code>类似于 SQL 语句的右外连接
</code></pre>
<h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><pre><code>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))类型的 RDD
val value: RDD[(String, (Iterable[Int], Iterable[Int]))] = dataRDD1.cogroup(dataRDD2)
(a,(CompactBuffer(1),CompactBuffer(4)))
(b,(CompactBuffer(2),CompactBuffer(5)))
(c,(CompactBuffer(3),CompactBuffer(6, 7)))
</code></pre>
<h2 id="Spark行动算子"><a href="#Spark行动算子" class="headerlink" title="Spark行动算子"></a>Spark行动算子</h2><h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>在驱动程序中，以数组 Array 的形式返回数据集的所有元素  </p>
<pre><code>// 收集数据到 Driver
rdd.collect().foreach(println)  
</code></pre>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>返回 RDD 中元素的个数  </p>
<pre><code>// 返回 RDD 中元素的个数
val countResult: Long = rdd.count()  
</code></pre>
<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>返回 RDD 中的第一个元素  </p>
<pre><code>// 返回 RDD 中元素的第1个元素
val firstResult: Int = rdd.first()  
</code></pre>
<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>返回一个由 RDD 的前 n 个元素组成的数组  </p>
<pre><code>// 返回 RDD 中元素的个数
val takeResult: Array[Int] = rdd.take(2)
println(takeResult.mkString(&quot;,&quot;))  
</code></pre>
<h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>返回该 RDD 排序后的前 n 个元素组成的数组  </p>
<pre><code>// 返回 RDD 中元素的个数
val result: Array[Int] = rdd.takeOrdered(2)  
</code></pre>
<h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合  </p>
<pre><code>val result: Int = rdd.aggregate(10)(_ + _, _ + _)  
</code></pre>
<h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p>折叠操作，aggregate 的简化版操作  </p>
<pre><code>val foldResult: Int = rdd.fold(0)(_+_)  
</code></pre>
<h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>统计每种 key 的个数  </p>
<pre><code>// 统计每种 key 的个数
val result: collection.Map[Int, Long] = rdd.countByKey()  
</code></pre>
<h3 id="save-相关算子"><a href="#save-相关算子" class="headerlink" title="save 相关算子"></a>save 相关算子</h3><p>将数据保存到不同格式的文件中  </p>
<pre><code>// 保存成 Text 文件
rdd.saveAsTextFile(&quot;output&quot;)  
</code></pre>
<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><pre><code>// 收集后打印
rdd.map(num=&gt;num).collect().foreach(println)  
</code></pre>
<h2 id="RDD-序列化"><a href="#RDD-序列化" class="headerlink" title="RDD 序列化"></a>RDD 序列化</h2><h3 id="闭包检查"><a href="#闭包检查" class="headerlink" title="闭包检查"></a>闭包检查</h3><p>从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。</p>
<h1 id="RDD-依赖关系"><a href="#RDD-依赖关系" class="headerlink" title="RDD 依赖关系"></a>RDD 依赖关系</h1><h2 id="1）RDD血缘关系"><a href="#1）RDD血缘关系" class="headerlink" title="1）RDD血缘关系"></a>1）RDD血缘关系</h2><p>RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage（血统）记录下来，以便恢复丢失的分区。    </p>
<p>RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。   </p>
<pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)
println(fileRDD.toDebugString)
</code></pre>
<h2 id="2）RDD依赖关系"><a href="#2）RDD依赖关系" class="headerlink" title="2）RDD依赖关系"></a>2）RDD依赖关系</h2><p>所谓的依赖关系，其实就是两个相邻 RDD 之间的关系。  </p>
<pre><code>val fileRDD: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)
println(fileRDD.dependencies)   
</code></pre>
<h2 id="3）RDD-窄依赖（没有Shuffle）"><a href="#3）RDD-窄依赖（没有Shuffle）" class="headerlink" title="3）RDD 窄依赖（没有Shuffle）"></a>3）RDD 窄依赖（没有Shuffle）</h2><p>窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，窄依赖我们形象的比喻为独生子女。</p>
<h2 id="4）RDD宽依赖-（有Shuffle）"><a href="#4）RDD宽依赖-（有Shuffle）" class="headerlink" title="4）RDD宽依赖 （有Shuffle）"></a>4）RDD宽依赖 （有Shuffle）</h2><p>宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。    </p>
<h2 id="RDD-任务划分"><a href="#RDD-任务划分" class="headerlink" title="RDD 任务划分"></a>RDD 任务划分</h2><p>RDD 任务切分中间分为：Application、Job、Stage 和 Task  </p>
<pre><code>Application：初始化一个 SparkContext 即生成一个 Application；
Job：一个 Action 算子就会生成一个 Job；
Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；
Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数。  
</code></pre>
<p>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系</p>
<p><img src="/2023/08/02/Spark-Core/9.png" alt="Spark任务划分流程">     </p>
<h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><h3 id="1-RDD-Cache-缓存"><a href="#1-RDD-Cache-缓存" class="headerlink" title="1) RDD Cache 缓存"></a>1) RDD Cache 缓存</h3><p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用。  </p>
<pre><code>// cache 操作会增加血缘关系，不改变原有的血缘关系  

// 可以更改存储级别
//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)  
</code></pre>
<h4 id="RDD持久化可选存储级别"><a href="#RDD持久化可选存储级别" class="headerlink" title="RDD持久化可选存储级别"></a>RDD持久化可选存储级别</h4><pre><code>object StorageLevel &#123;
 val NONE = new StorageLevel(false, false, false, false)
 val DISK_ONLY = new StorageLevel(true, false, false, false)
 val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)
 val MEMORY_ONLY = new StorageLevel(false, true, false, true)
 val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)
 val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)
 val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)
 val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)
 val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)
 val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)
 val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)
 val OFF_HEAP = new StorageLevel(true, true, true, false, 1)  
</code></pre>
<p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD 的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。  </p>
<p>Spark 会自动对一些 Shuffle 操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点 Shuffle 失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用 persist 或 cache。  </p>
<h3 id="2）RDD-CheckPoint-检查点"><a href="#2）RDD-CheckPoint-检查点" class="headerlink" title="2）RDD CheckPoint 检查点"></a>2）RDD CheckPoint 检查点</h3><p>所谓的检查点其实就是通过将 RDD 中间结果写入磁盘  </p>
<p>对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发。  </p>
<ol start="3">
<li><p>缓存和检查点区别  </p>
<p> 1）Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖。<br> 2）Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高。<br> 3）建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD</p>
</li>
</ol>
<h2 id="RDD-分区器"><a href="#RDD-分区器" class="headerlink" title="RDD 分区器"></a>RDD 分区器</h2><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认分区。   </p>
<p>分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。   </p>
<pre><code>➢ 只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None  
➢ 每个 RDD 的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。  
</code></pre>
<ol>
<li><p>Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余  </p>
</li>
<li><p>Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p>
</li>
</ol>
<h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge。  </p>
<h3 id="系统累加器"><a href="#系统累加器" class="headerlink" title="系统累加器"></a>系统累加器</h3><pre><code>val rdd = sc.makeRDD(List(1,2,3,4,5))
// 声明累加器
var sum = sc.longAccumulator(&quot;sum&quot;);
rdd.foreach(
 num =&gt; &#123;
 // 使用累加器
 sum.add(num)
 &#125;
)
// 获取累加器的值
println(&quot;sum = &quot; + sum.value)
</code></pre>
<h3 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h3><pre><code>// 自定义累加器
// 1. 继承 AccumulatorV2，并设定泛型
// 2. 重写累加器的抽象方法
class WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, 
Long]]&#123;
var map : mutable.Map[String, Long] = mutable.Map()
// 累加器是否为初始状态
override def isZero: Boolean = &#123;
 map.isEmpty
&#125;
// 复制累加器
override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = &#123;
 new WordCountAccumulator
&#125;
// 重置累加器
override def reset(): Unit = &#123;
 map.clear()
&#125;
// 向累加器中增加数据 (In)
override def add(word: String): Unit = &#123;
 // 查询 map 中是否存在相同的单词
 // 如果有相同的单词，那么单词的数量加 1
 // 如果没有相同的单词，那么在 map 中增加这个单词
 map(word) = map.getOrElse(word, 0L) + 1L
&#125;
// 合并累加器
override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): 
Unit = &#123;
 val map1 = map
 val map2 = other.value
 // 两个 Map 的合并
 map = map1.foldLeft(map2)(
 ( innerMap, kv ) =&gt; &#123;
 innerMap(kv._1) = innerMap.getOrElse(kv._1, 0L) + kv._2
 innerMap
 &#125;
 )
&#125;
// 返回累加器的结果 （Out）
override def value: mutable.Map[String, Long] = map
&#125;
</code></pre>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。  </p>

    </div>

    
    
    
	
	  <div>
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
	  </div>
	
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>张宴银
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2023/08/02/Spark-Core/" title="Spark学习笔记">http://example.com/2023/08/02/Spark-Core/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>



      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%BA%B5%E6%B5%AA%E5%A4%A7%E5%8C%96%E4%B8%AD%EF%BC%8C%E4%B8%8D%E5%96%9C%E4%BA%A6%E4%B8%8D%E6%83%A7/" rel="tag"># 纵浪大化中，不喜亦不惧</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/08/01/SQLServer%E6%9F%A5%E8%AF%A2%E4%BD%93%E7%B3%BB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" rel="prev" title="SQLServer查询体系学习记录">
      <i class="fa fa-chevron-left"></i> SQLServer查询体系学习记录
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/08/03/kafka/" rel="next" title="kafka3.0.0学习记录">
      kafka3.0.0学习记录 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=2042878838&auto=1&height=66"></iframe>

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%B5%E6%B5%AA%E5%A4%A7%E5%8C%96%E4%B8%AD%EF%BC%8C%E4%B8%8D%E5%96%9C%E4%BA%A6%E4%B8%8D%E6%83%A7"><span class="nav-number">1.</span> <span class="nav-text">纵浪大化中，不喜亦不惧 ~</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark%E6%A6%82%E8%BF%B0"><span class="nav-number">2.</span> <span class="nav-text">Spark概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">2.1.</span> <span class="nav-text">Spark 是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mapreduce%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">2.2.</span> <span class="nav-text">Mapreduce是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HBase%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">2.3.</span> <span class="nav-text">HBase是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark%E5%92%8CHadoop%E7%9A%84%E5%B7%AE%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F-%EF%BC%88Hadoop%E9%BB%98%E8%AE%A4%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%E4%B8%BAMapreduce%EF%BC%89"><span class="nav-number">2.4.</span> <span class="nav-text">Spark和Hadoop的差别是什么？ （Hadoop默认计算引擎为Mapreduce）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E9%80%89%E7%94%A8Spark%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E9%80%89%E7%94%A8Mapreduce%EF%BC%9F"><span class="nav-number">2.5.</span> <span class="nav-text">什么时候选用Spark什么时候选用Mapreduce？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4Spark%E5%BA%94%E7%94%A8%E7%9A%84%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.6.</span> <span class="nav-text">提交Spark应用的代码示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#local%E6%A8%A1%E5%BC%8F"><span class="nav-number">2.6.1.</span> <span class="nav-text">local模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn%E6%A8%A1%E5%BC%8F-%EF%BC%88%E7%94%9F%E4%BA%A7%EF%BC%9ACluster%E6%A8%A1%E5%BC%8F%EF%BC%89"><span class="nav-number">2.6.2.</span> <span class="nav-text">Yarn模式  （生产：Cluster模式）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn%E6%A8%A1%E5%BC%8F-%EF%BC%88%E6%B5%8B%E8%AF%95%EF%BC%9AClient%E6%A8%A1%E5%BC%8F%EF%BC%89"><span class="nav-number">2.6.3.</span> <span class="nav-text">Yarn模式  （测试：Client模式）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark%E7%9A%84%E7%AB%AF%E5%8F%A3%E5%8F%B7"><span class="nav-number">2.6.3.1.</span> <span class="nav-text">Spark的端口号</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84"><span class="nav-number">2.7.</span> <span class="nav-text">Spark运行架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">2.7.1.</span> <span class="nav-text">Spark核心组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Driver%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-number">2.7.1.1.</span> <span class="nav-text">Driver的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Executor%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-number">2.7.1.2.</span> <span class="nav-text">Executor的作用</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Executor-%E6%9C%89%E4%B8%A4%E4%B8%AA%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%EF%BC%9A"><span class="nav-number">2.7.1.2.1.</span> <span class="nav-text">Executor 有两个核心功能：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Master-%E5%92%8C-Worker-Local%E6%A8%A1%E5%BC%8F%E6%97%B6"><span class="nav-number">2.7.2.</span> <span class="nav-text">Master 和 Worker  (Local模式时)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Master"><span class="nav-number">2.7.2.1.</span> <span class="nav-text">Master</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Worker"><span class="nav-number">2.7.2.2.</span> <span class="nav-text">Worker</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ApplicationMaster"><span class="nav-number">2.7.3.</span> <span class="nav-text">ApplicationMaster</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="nav-number">3.</span> <span class="nav-text">核心概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Executor-%E4%B8%8E-Core"><span class="nav-number">3.1.</span> <span class="nav-text">Executor 与 Core</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E5%BA%A6%EF%BC%88Parallelism%EF%BC%89"><span class="nav-number">3.2.</span> <span class="nav-text">并行度（Parallelism）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE%EF%BC%88DAG%EF%BC%89"><span class="nav-number">3.3.</span> <span class="nav-text">有向无环图（DAG）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B"><span class="nav-number">3.4.</span> <span class="nav-text">提交流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn-Client-%E6%A8%A1%E5%BC%8F"><span class="nav-number">3.4.1.</span> <span class="nav-text">Yarn Client 模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn-Cluster-%E6%A8%A1%E5%BC%8F"><span class="nav-number">3.4.2.</span> <span class="nav-text">Yarn Cluster 模式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B"><span class="nav-number">4.</span> <span class="nav-text">Spark 核心编程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD"><span class="nav-number">4.1.</span> <span class="nav-text">RDD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86"><span class="nav-number">4.2.</span> <span class="nav-text">执行原理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93"><span class="nav-number">5.</span> <span class="nav-text">RDD算子总结</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Value%E7%B1%BB%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="nav-number">5.1.</span> <span class="nav-text">Value类型总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#map"><span class="nav-number">5.1.1.</span> <span class="nav-text">map</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mapPartitions"><span class="nav-number">5.1.2.</span> <span class="nav-text">mapPartitions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#map-%E5%92%8C-mapPartitions-%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9A"><span class="nav-number">5.1.3.</span> <span class="nav-text">map 和 mapPartitions 的区别：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mapPartitionsWithIndex"><span class="nav-number">5.1.4.</span> <span class="nav-text">mapPartitionsWithIndex</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flatMap"><span class="nav-number">5.1.5.</span> <span class="nav-text">flatMap</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#glom"><span class="nav-number">5.1.6.</span> <span class="nav-text">glom</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#groupBy"><span class="nav-number">5.1.7.</span> <span class="nav-text">groupBy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#filter"><span class="nav-number">5.1.8.</span> <span class="nav-text">filter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sample"><span class="nav-number">5.1.9.</span> <span class="nav-text">sample</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#distinct"><span class="nav-number">5.1.10.</span> <span class="nav-text">distinct</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coalesce"><span class="nav-number">5.1.11.</span> <span class="nav-text">coalesce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#repartition"><span class="nav-number">5.1.12.</span> <span class="nav-text">repartition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sortBy"><span class="nav-number">5.1.13.</span> <span class="nav-text">sortBy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8C-Value-%E7%B1%BB%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="nav-number">5.2.</span> <span class="nav-text">双 Value 类型总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#intersection"><span class="nav-number">5.2.1.</span> <span class="nav-text">intersection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#union"><span class="nav-number">5.2.2.</span> <span class="nav-text">union</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#subtract"><span class="nav-number">5.2.3.</span> <span class="nav-text">subtract</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zip"><span class="nav-number">5.2.4.</span> <span class="nav-text">zip</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Key-Value%E7%B1%BB%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="nav-number">5.3.</span> <span class="nav-text">Key-Value类型总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#partitionBy"><span class="nav-number">5.3.1.</span> <span class="nav-text">partitionBy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduceByKey"><span class="nav-number">5.3.2.</span> <span class="nav-text">reduceByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#groupByKey"><span class="nav-number">5.3.3.</span> <span class="nav-text">groupByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduceByKey-%E5%92%8C-groupByKey-%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9A"><span class="nav-number">5.3.4.</span> <span class="nav-text">reduceByKey 和 groupByKey 的区别：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#aggregateByKey"><span class="nav-number">5.3.5.</span> <span class="nav-text">aggregateByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#foldByKey"><span class="nav-number">5.3.6.</span> <span class="nav-text">foldByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#combineByKey"><span class="nav-number">5.3.7.</span> <span class="nav-text">combineByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduceByKey%E3%80%81foldByKey%E3%80%81aggregateByKey%E3%80%81combineByKey-%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9A"><span class="nav-number">5.3.8.</span> <span class="nav-text">reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sortByKey"><span class="nav-number">5.3.9.</span> <span class="nav-text">sortByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#join"><span class="nav-number">5.3.10.</span> <span class="nav-text">join</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#leftOuterJoin"><span class="nav-number">5.3.11.</span> <span class="nav-text">leftOuterJoin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rightOuterJoin"><span class="nav-number">5.3.12.</span> <span class="nav-text">rightOuterJoin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cogroup"><span class="nav-number">5.3.13.</span> <span class="nav-text">cogroup</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="nav-number">5.4.</span> <span class="nav-text">Spark行动算子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#collect"><span class="nav-number">5.4.1.</span> <span class="nav-text">collect</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#count"><span class="nav-number">5.4.2.</span> <span class="nav-text">count</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#first"><span class="nav-number">5.4.3.</span> <span class="nav-text">first</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#take"><span class="nav-number">5.4.4.</span> <span class="nav-text">take</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#takeOrdered"><span class="nav-number">5.4.5.</span> <span class="nav-text">takeOrdered</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#aggregate"><span class="nav-number">5.4.6.</span> <span class="nav-text">aggregate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fold"><span class="nav-number">5.4.7.</span> <span class="nav-text">fold</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#countByKey"><span class="nav-number">5.4.8.</span> <span class="nav-text">countByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#save-%E7%9B%B8%E5%85%B3%E7%AE%97%E5%AD%90"><span class="nav-number">5.4.9.</span> <span class="nav-text">save 相关算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#foreach"><span class="nav-number">5.4.10.</span> <span class="nav-text">foreach</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-%E5%BA%8F%E5%88%97%E5%8C%96"><span class="nav-number">5.5.</span> <span class="nav-text">RDD 序列化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AD%E5%8C%85%E6%A3%80%E6%9F%A5"><span class="nav-number">5.5.1.</span> <span class="nav-text">闭包检查</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">6.</span> <span class="nav-text">RDD 依赖关系</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1%EF%BC%89RDD%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB"><span class="nav-number">6.1.</span> <span class="nav-text">1）RDD血缘关系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2%EF%BC%89RDD%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">6.2.</span> <span class="nav-text">2）RDD依赖关系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3%EF%BC%89RDD-%E7%AA%84%E4%BE%9D%E8%B5%96%EF%BC%88%E6%B2%A1%E6%9C%89Shuffle%EF%BC%89"><span class="nav-number">6.3.</span> <span class="nav-text">3）RDD 窄依赖（没有Shuffle）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4%EF%BC%89RDD%E5%AE%BD%E4%BE%9D%E8%B5%96-%EF%BC%88%E6%9C%89Shuffle%EF%BC%89"><span class="nav-number">6.4.</span> <span class="nav-text">4）RDD宽依赖 （有Shuffle）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-%E4%BB%BB%E5%8A%A1%E5%88%92%E5%88%86"><span class="nav-number">6.5.</span> <span class="nav-text">RDD 任务划分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">6.6.</span> <span class="nav-text">RDD 持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-RDD-Cache-%E7%BC%93%E5%AD%98"><span class="nav-number">6.6.1.</span> <span class="nav-text">1) RDD Cache 缓存</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD%E6%8C%81%E4%B9%85%E5%8C%96%E5%8F%AF%E9%80%89%E5%AD%98%E5%82%A8%E7%BA%A7%E5%88%AB"><span class="nav-number">6.6.1.1.</span> <span class="nav-text">RDD持久化可选存储级别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%EF%BC%89RDD-CheckPoint-%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="nav-number">6.6.2.</span> <span class="nav-text">2）RDD CheckPoint 检查点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-number">6.7.</span> <span class="nav-text">RDD 分区器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">6.8.</span> <span class="nav-text">累加器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B3%BB%E7%BB%9F%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">6.8.1.</span> <span class="nav-text">系统累加器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">6.8.2.</span> <span class="nav-text">自定义累加器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">6.9.</span> <span class="nav-text">广播变量</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张宴银"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">张宴银</p>
  <div class="site-description" itemprop="description">初级以内我无敌，中级以上我一换一</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; Sat Jul 29 2023 08:00:00 GMT+0800 (中国标准时间) – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张宴银</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>



    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共48.3k字</span>
</div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
